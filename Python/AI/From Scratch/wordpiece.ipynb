{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the WordPiece Tokenization Technique\n",
    "\n",
    "Improvment: Enhanced Suffix Array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be ableabab to understand how they are trained and generate tokens.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def cleaning(text):\n",
    "    for p in punctuation + ' ' + '\\n':\n",
    "        text = ' '.join([sub.strip() for sub in text.split(p) if len(sub) > 1])\n",
    "\n",
    "    return text\n",
    "\n",
    "# https://towardsdatascience.com/difference-between-nfd-nfc-nfkd-and-nfkc-explained-with-python-code-e2631f96ae6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the Hugging Face Course',\n",
       " 'This chapter is about tokenization',\n",
       " 'This section shows several tokenizer algorithms',\n",
       " 'Hopefully you will be ableabab to understand how they are trained and generate tokens']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [cleaning(text) for text in corpus]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Frequency of each word in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'ableabab': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "word_freqs = defaultdict(lambda : 0)\n",
    "for text in corpus:\n",
    "    for word in text.split():\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting those Words into Character Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', '##h', '##i', '##s'],\n",
       " 'is': ['i', '##s'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
       " 'Face': ['F', '##a', '##c', '##e'],\n",
       " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
       " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
       " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
       " 'tokenization': ['t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
       " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
       " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
       " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
       " 'algorithms': ['a',\n",
       "  '##l',\n",
       "  '##g',\n",
       "  '##o',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##h',\n",
       "  '##m',\n",
       "  '##s'],\n",
       " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
       " 'you': ['y', '##o', '##u'],\n",
       " 'will': ['w', '##i', '##l', '##l'],\n",
       " 'be': ['b', '##e'],\n",
       " 'ableabab': ['a', '##b', '##l', '##e', '##a', '##b', '##a', '##b'],\n",
       " 'to': ['t', '##o'],\n",
       " 'understand': ['u',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##s',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##n',\n",
       "  '##d'],\n",
       " 'how': ['h', '##o', '##w'],\n",
       " 'they': ['t', '##h', '##e', '##y'],\n",
       " 'are': ['a', '##r', '##e'],\n",
       " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
       " 'and': ['a', '##n', '##d'],\n",
       " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
       " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n",
    "\n",
    "splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Pair Scores: $\\cfrac{freq \\; of \\; pair}{freq \\; of \\; first \\; element \\times freq \\; of \\; second \\; element}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_score(splits):\n",
    "    token_freqs = defaultdict(lambda: 0) # Capturing the global corpus statistics for `freq of firsy element` and `freq of second element`\n",
    "    pair_freqs = defaultdict(lambda: 0)   # Capturing the statistics for `freq of pair`: How many times this pair appears in the corpus\n",
    "\n",
    "    # Iterate over all words of the corpus\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "\n",
    "        # If a word contains only 1 letter\n",
    "        if len(split) == 1:\n",
    "            token_freqs[split[0]] += freq\n",
    "            continue\n",
    "\n",
    "        for i in range(len(split) - 1):\n",
    "            token_freqs[split[i]] += freq\n",
    "            pair_freqs[(split[i], split[i+1])] += freq\n",
    "\n",
    "        # Adding the final token that the for-loop is not processing\n",
    "        token_freqs[split[-1]] += freq\n",
    "\n",
    "    # Returning the scores, calculated from the formula above\n",
    "    return {pair: pair_freq / (token_freqs[pair[0]] * token_freqs[pair[1]]) for pair, pair_freq in pair_freqs.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('T', '##h'): 0.125,\n",
       " ('##h', '##i'): 0.03409090909090909,\n",
       " ('##i', '##s'): 0.02727272727272727,\n",
       " ('i', '##s'): 0.1,\n",
       " ('t', '##h'): 0.03571428571428571,\n",
       " ('##h', '##e'): 0.011904761904761904,\n",
       " ('H', '##u'): 0.1,\n",
       " ('##u', '##g'): 0.05,\n",
       " ('##g', '##g'): 0.0625,\n",
       " ('##g', '##i'): 0.022727272727272728,\n",
       " ('##i', '##n'): 0.01652892561983471,\n",
       " ('##n', '##g'): 0.022727272727272728,\n",
       " ('F', '##a'): 0.1111111111111111,\n",
       " ('##a', '##c'): 0.05555555555555555,\n",
       " ('##c', '##e'): 0.023809523809523808,\n",
       " ('C', '##o'): 0.07692307692307693,\n",
       " ('##o', '##u'): 0.046153846153846156,\n",
       " ('##u', '##r'): 0.022222222222222223,\n",
       " ('##r', '##s'): 0.022222222222222223,\n",
       " ('##s', '##e'): 0.004761904761904762,\n",
       " ('c', '##h'): 0.125,\n",
       " ('##h', '##a'): 0.013888888888888888,\n",
       " ('##a', '##p'): 0.05555555555555555,\n",
       " ('##p', '##t'): 0.07142857142857142,\n",
       " ('##t', '##e'): 0.013605442176870748,\n",
       " ('##e', '##r'): 0.026455026455026454,\n",
       " ('a', '##b'): 0.1,\n",
       " ('##b', '##o'): 0.019230769230769232,\n",
       " ('##u', '##t'): 0.02857142857142857,\n",
       " ('t', '##o'): 0.04395604395604396,\n",
       " ('##o', '##k'): 0.07692307692307693,\n",
       " ('##k', '##e'): 0.047619047619047616,\n",
       " ('##e', '##n'): 0.017316017316017316,\n",
       " ('##n', '##i'): 0.01652892561983471,\n",
       " ('##i', '##z'): 0.09090909090909091,\n",
       " ('##z', '##a'): 0.05555555555555555,\n",
       " ('##a', '##t'): 0.031746031746031744,\n",
       " ('##t', '##i'): 0.025974025974025976,\n",
       " ('##i', '##o'): 0.013986013986013986,\n",
       " ('##o', '##n'): 0.013986013986013986,\n",
       " ('s', '##e'): 0.031746031746031744,\n",
       " ('##e', '##c'): 0.023809523809523808,\n",
       " ('##c', '##t'): 0.07142857142857142,\n",
       " ('s', '##h'): 0.041666666666666664,\n",
       " ('##h', '##o'): 0.009615384615384616,\n",
       " ('##o', '##w'): 0.07692307692307693,\n",
       " ('##w', '##s'): 0.05,\n",
       " ('##e', '##v'): 0.047619047619047616,\n",
       " ('##v', '##e'): 0.047619047619047616,\n",
       " ('##r', '##a'): 0.037037037037037035,\n",
       " ('##a', '##l'): 0.015873015873015872,\n",
       " ('##z', '##e'): 0.023809523809523808,\n",
       " ('a', '##l'): 0.02857142857142857,\n",
       " ('##l', '##g'): 0.03571428571428571,\n",
       " ('##g', '##o'): 0.019230769230769232,\n",
       " ('##o', '##r'): 0.008547008547008548,\n",
       " ('##r', '##i'): 0.010101010101010102,\n",
       " ('##i', '##t'): 0.012987012987012988,\n",
       " ('##t', '##h'): 0.017857142857142856,\n",
       " ('##h', '##m'): 0.125,\n",
       " ('##m', '##s'): 0.1,\n",
       " ('H', '##o'): 0.038461538461538464,\n",
       " ('##o', '##p'): 0.038461538461538464,\n",
       " ('##p', '##e'): 0.023809523809523808,\n",
       " ('##e', '##f'): 0.047619047619047616,\n",
       " ('##f', '##u'): 0.2,\n",
       " ('##u', '##l'): 0.02857142857142857,\n",
       " ('##l', '##l'): 0.04081632653061224,\n",
       " ('##l', '##y'): 0.07142857142857142,\n",
       " ('y', '##o'): 0.07692307692307693,\n",
       " ('w', '##i'): 0.09090909090909091,\n",
       " ('##i', '##l'): 0.012987012987012988,\n",
       " ('b', '##e'): 0.047619047619047616,\n",
       " ('##b', '##l'): 0.03571428571428571,\n",
       " ('##l', '##e'): 0.006802721088435374,\n",
       " ('##e', '##a'): 0.005291005291005291,\n",
       " ('##a', '##b'): 0.05555555555555555,\n",
       " ('##b', '##a'): 0.027777777777777776,\n",
       " ('u', '##n'): 0.09090909090909091,\n",
       " ('##n', '##d'): 0.06818181818181818,\n",
       " ('##d', '##e'): 0.011904761904761904,\n",
       " ('##s', '##t'): 0.014285714285714285,\n",
       " ('##t', '##a'): 0.015873015873015872,\n",
       " ('##a', '##n'): 0.010101010101010102,\n",
       " ('h', '##o'): 0.07692307692307693,\n",
       " ('##e', '##y'): 0.023809523809523808,\n",
       " ('a', '##r'): 0.022222222222222223,\n",
       " ('##r', '##e'): 0.005291005291005291,\n",
       " ('t', '##r'): 0.015873015873015872,\n",
       " ('##a', '##i'): 0.010101010101010102,\n",
       " ('##n', '##e'): 0.008658008658008658,\n",
       " ('##e', '##d'): 0.011904761904761904,\n",
       " ('a', '##n'): 0.01818181818181818,\n",
       " ('g', '##e'): 0.047619047619047616,\n",
       " ('##n', '##s'): 0.00909090909090909}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = pair_score(splits)\n",
    "\n",
    "ps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Function that returns the Pair with the higher Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score(pair_scores):\n",
    "    max_pair = ('', '')\n",
    "    max_freq = 0\n",
    "    for pair, freq in pair_scores.items():\n",
    "        if freq > max_freq:\n",
    "            max_freq = freq\n",
    "            max_pair = pair\n",
    "    \n",
    "    return max_pair, max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('##f', '##u'), 0.2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score(ps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Function that Merge Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair_t, splits):\n",
    "    # Iterating over the word-tokens defaultdict\n",
    "    for word in splits.keys():\n",
    "        split = splits[word] # contains the tokens of the word\n",
    "\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        \n",
    "        # Iterating until we find the pair in the tokenize representation of the word\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if (split[i] == pair_t[0]) and (split[i+1] == pair_t[1]):\n",
    "                split = split[:i] + [pair_t[0] + pair_t[1][2:]] + split[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', '##b', '##l', '##e', '##a', '##b', '##a', '##b']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[\"ableabab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '##b', '##l', '##e', '##ab', '##ab']\n"
     ]
    }
   ],
   "source": [
    "s = merge_pair((\"##a\", \"##b\"), splits)\n",
    "\n",
    "print(s[\"ableabab\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(desire_len):\n",
    "    # Original splits (character tokenization of each word in the corpus)\n",
    "    splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n",
    "\n",
    "    # Creating the basic vocabulary\n",
    "    vocab_ = set([token for tokens in splits.values() for token in tokens])\n",
    "    \n",
    "    i = len(vocab_)\n",
    "    new_tokens = []\n",
    "    while i < desire_len:\n",
    "        ps = pair_score(splits)          # Calculate the Score of each pair\n",
    "        bs, _ = best_score(ps)           # Get the pair with the highest score\n",
    "        splits = merge_pair(bs, splits)  # Merge those two tokens\n",
    "\n",
    "        new_tokens.append(bs[0] + bs[1][2:])\n",
    "        i += 1\n",
    "\n",
    "    # Adding the the basic vocabulary the new tokens\n",
    "    vocab_ = [\"[CLS]\", \"[PAD]\", \"[UNK]\"] + sorted(list((vocab_ | set(new_tokens))))\n",
    "    return {term: i for i, term in enumerate(vocab_)}, vocab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_ = create_vocab(70_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[PAD]', '[UNK]', '', '##a', '##ab', '##abab', '##at', '##b', '##c', '##ct', '##cti', '##ctio', '##ction', '##d', '##e', '##f', '##fu', '##ful', '##full', '##fully', '##g', '##h', '##hm', '##i', '##ithms', '##iz', '##izat', '##izati', '##k', '##l', '##lg', '##ll', '##m', '##n', '##niz', '##nizati', '##nizatio', '##nization', '##ns', '##o', '##p', '##r', '##ra', '##ral', '##rat', '##rithms', '##rsta', '##rstan', '##rstand', '##s', '##sta', '##t', '##ta', '##thm', '##thms', '##u', '##ur', '##urs', '##ut', '##v', '##w', '##ws', '##y', '##z', '##za', '##zat', 'C', 'Co', 'Cours', 'Course', 'F', 'Fa', 'Fac', 'Face', 'H', 'Ho', 'Hop', 'Hope', 'Hopefully', 'Hu', 'Hug', 'Hugg', 'Huggi', 'Huggin', 'Hugging', 'T', 'Th', 'Thi', 'This', 'a', 'ab', 'abl', 'able', 'ableabab', 'abo', 'about', 'alg', 'algo', 'algorithms', 'an', 'and', 'ar', 'are', 'b', 'be', 'c', 'ch', 'cha', 'chap', 'chapt', 'chapte', 'chapter', 'g', 'ge', 'gen', 'gene', 'generat', 'generate', 'h', 'ho', 'how', 'i', 'is', 's', 'se', 'section', 'sev', 'seve', 'several', 'sh', 'sho', 'shows', 't', 'th', 'the', 'they', 'to', 'tok', 'toke', 'tokeniz', 'tokenization', 'tokenize', 'tokenizer', 'tokens', 'tra', 'trai', 'train', 'traine', 'trained', 'u', 'un', 'und', 'unde', 'understand', 'w', 'wi', 'will', 'y', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "len(vocab_)\n",
    "print(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS]': 0, '[PAD]': 1, '[UNK]': 2, '': 3, '##a': 4, '##ab': 5, '##abab': 6, '##at': 7, '##b': 8, '##c': 9, '##ct': 10, '##cti': 11, '##ctio': 12, '##ction': 13, '##d': 14, '##e': 15, '##f': 16, '##fu': 17, '##ful': 18, '##full': 19, '##fully': 20, '##g': 21, '##h': 22, '##hm': 23, '##i': 24, '##ithms': 25, '##iz': 26, '##izat': 27, '##izati': 28, '##k': 29, '##l': 30, '##lg': 31, '##ll': 32, '##m': 33, '##n': 34, '##niz': 35, '##nizati': 36, '##nizatio': 37, '##nization': 38, '##ns': 39, '##o': 40, '##p': 41, '##r': 42, '##ra': 43, '##ral': 44, '##rat': 45, '##rithms': 46, '##rsta': 47, '##rstan': 48, '##rstand': 49, '##s': 50, '##sta': 51, '##t': 52, '##ta': 53, '##thm': 54, '##thms': 55, '##u': 56, '##ur': 57, '##urs': 58, '##ut': 59, '##v': 60, '##w': 61, '##ws': 62, '##y': 63, '##z': 64, '##za': 65, '##zat': 66, 'C': 67, 'Co': 68, 'Cours': 69, 'Course': 70, 'F': 71, 'Fa': 72, 'Fac': 73, 'Face': 74, 'H': 75, 'Ho': 76, 'Hop': 77, 'Hope': 78, 'Hopefully': 79, 'Hu': 80, 'Hug': 81, 'Hugg': 82, 'Huggi': 83, 'Huggin': 84, 'Hugging': 85, 'T': 86, 'Th': 87, 'Thi': 88, 'This': 89, 'a': 90, 'ab': 91, 'abl': 92, 'able': 93, 'ableabab': 94, 'abo': 95, 'about': 96, 'alg': 97, 'algo': 98, 'algorithms': 99, 'an': 100, 'and': 101, 'ar': 102, 'are': 103, 'b': 104, 'be': 105, 'c': 106, 'ch': 107, 'cha': 108, 'chap': 109, 'chapt': 110, 'chapte': 111, 'chapter': 112, 'g': 113, 'ge': 114, 'gen': 115, 'gene': 116, 'generat': 117, 'generate': 118, 'h': 119, 'ho': 120, 'how': 121, 'i': 122, 'is': 123, 's': 124, 'se': 125, 'section': 126, 'sev': 127, 'seve': 128, 'several': 129, 'sh': 130, 'sho': 131, 'shows': 132, 't': 133, 'th': 134, 'the': 135, 'they': 136, 'to': 137, 'tok': 138, 'toke': 139, 'tokeniz': 140, 'tokenization': 141, 'tokenize': 142, 'tokenizer': 143, 'tokens': 144, 'tra': 145, 'trai': 146, 'train': 147, 'traine': 148, 'trained': 149, 'u': 150, 'un': 151, 'und': 152, 'unde': 153, 'understand': 154, 'w': 155, 'wi': 156, 'will': 157, 'y': 158, 'yo': 159, 'you': 160}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Function to Tokenize Words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word):\n",
    "    tokens = []\n",
    "\n",
    "    # Iterating over the entire word starting from the end\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        # Trying to find the bigest sub-word that exists on our vocabulary\n",
    "        while (i > 0) and word[:i] not in vocab_:\n",
    "            i -= 1\n",
    "\n",
    "        # If a sub-word does not exist on the vocabulary\n",
    "        if i == 0:\n",
    "            # Keeping some information about the word\n",
    "            tokens.append(\"[UNK]\")\n",
    "            return tokens\n",
    "        \n",
    "        # The first sub-word is not going to contain `##`\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "\n",
    "        # All the other sub-words are going to contain `##`\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugging']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_word(\"Hugging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huggi', '[UNK]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_word(\"Huggi0ng\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Function to Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return [token for word in cleaning(text).split() for token in tokenize_word(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'Hugging', 'Face', 'c', '##o', '##urs', '##e']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text(\"This is the Hugging Face course!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Class that Contains everything we Discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self, corpus=None, ntokens=30_000, cleaning=lambda text: text):\n",
    "        if corpus is not None:\n",
    "            # Cleaning Corpus\n",
    "            corpus = [cleaning(text) for text in corpus]\n",
    "\n",
    "            # Calculating the frequencies of each word (global statistics)\n",
    "            self._word_freqs = defaultdict(lambda : 0)\n",
    "            for text in corpus:\n",
    "                for word in text.split():\n",
    "                    self._word_freqs[word] += 1\n",
    "\n",
    "        self._cleaning = cleaning\n",
    "        self._ntokens = ntokens\n",
    "        self.special_t = [\"[CLS]\", \"[UNK]\", \"[PAD]\", \"[SEP]\"]\n",
    "        self.vocab_l = []\n",
    "        self.vocab_d = {}\n",
    "        self.ivocab_d = {}\n",
    "        \n",
    "\n",
    "    def __calc_pair_scores(self, splits):\n",
    "        token_freqs = defaultdict(lambda: 0)  # Capturing the global corpus statistics for `freq of firsy element` and `freq of second element`\n",
    "        pair_freqs = defaultdict(lambda: 0)   # Capturing the statistics for `freq of pair`: How many times this pair appears in the corpus\n",
    "\n",
    "        # Iterate over all words of the corpus\n",
    "        for word, freq in self._word_freqs.items():\n",
    "            split = splits[word]\n",
    "\n",
    "            # If a word contains only 1 letter\n",
    "            if len(split) == 1:\n",
    "                token_freqs[split[0]] += freq\n",
    "                continue\n",
    "\n",
    "            for i in range(len(split) - 1):\n",
    "                token_freqs[split[i]] += freq\n",
    "                pair_freqs[(split[i], split[i+1])] += freq\n",
    "\n",
    "            # Adding the final token that the for-loop is not processing\n",
    "            token_freqs[split[-1]] += freq\n",
    "\n",
    "        # Returning the scores, calculated from the formula above\n",
    "        return {pair: pair_freq / (token_freqs[pair[0]] * token_freqs[pair[1]]) for pair, pair_freq in pair_freqs.items()}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __highest_score(pair_scores):\n",
    "        max_pair = ('', '')\n",
    "        max_freq = 0\n",
    "        for pair, freq in pair_scores.items():\n",
    "            if freq > max_freq:\n",
    "                max_freq = freq\n",
    "                max_pair = pair\n",
    "        \n",
    "        return max_pair, max_freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def __merge_pair(pair_tuple, splits):\n",
    "        # Iterating over the word-tokens defaultdict\n",
    "        for word in splits.keys():\n",
    "            split = splits[word] # contains the tokens of the word\n",
    "\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            \n",
    "            # Iterating until we find the pair in the tokenize representation of the word\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if (split[i] == pair_tuple[0]) and (split[i+1] == pair_tuple[1]):\n",
    "                    merge = pair_tuple[0] + pair_tuple[1][2:] if pair_tuple[1].startswith(\"##\") else pair_tuple[0] + pair_tuple[1]\n",
    "                    split = split[:i] + [merge] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            splits[word] = split\n",
    "        return splits\n",
    "    \n",
    "\n",
    "    def fit(self):\n",
    "        # Original splits (character tokenization of each word in the corpus)\n",
    "        splits = {word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in self._word_freqs.keys()}\n",
    "\n",
    "        # Creating the basic vocabulary\n",
    "        vocab_ = set([token for tokens in splits.values() for token in tokens])\n",
    "        \n",
    "        new_tokens = []\n",
    "        for _ in tqdm(range(len(vocab_), self._ntokens), desc=\"Creating Vocabulary: \"):\n",
    "            ps = self.__calc_pair_scores(splits)   # Calculate the Score of each pair\n",
    "            bs, _ = self.__highest_score(ps)       # Get the pair with the highest score\n",
    "            splits = self.__merge_pair(bs, splits) # Merge those two tokens\n",
    "\n",
    "            if bs[1].startswith(\"##\"):\n",
    "                new_tokens.append(bs[0] + bs[1][2:])\n",
    "                continue\n",
    "            new_tokens.append(bs[0] + bs[1])\n",
    "\n",
    "        # Adding to the the basic vocabulary the new tokens\n",
    "        self.vocab_l = self.special_t + sorted(list((vocab_ | set(new_tokens))))\n",
    "        self.vocab_d = {term: i for i, term in enumerate(self.vocab_l)}\n",
    "        self.ivocab_d = {i: term for i, term in enumerate(self.vocab_l)}\n",
    "\n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        # Saving the Vocabulary Dict into a JSON file\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.vocab_d, f)\n",
    "\n",
    "    def load_vocab(self, path):\n",
    "        # Updating the Vocabulary elements from the JSON file\n",
    "        with open(path, \"r\") as f:\n",
    "            self.vocab_d = json.loads(f.read())\n",
    "        self.vocab_l = list(self.vocab_d.keys())\n",
    "        self.ivocab_d = {i: token for i, token in enumerate(self.vocab_l)}\n",
    "\n",
    "\n",
    "    def __tokenize_word(self, word):\n",
    "        tokens = []\n",
    "\n",
    "        # Iterating over the entire word starting from the end\n",
    "        while len(word) > 0:\n",
    "            i = len(word)\n",
    "            # Trying to find the bigest sub-word that exists on our vocabulary\n",
    "            while (i > 0) and word[:i] not in self.vocab_l:\n",
    "                i -= 1\n",
    "\n",
    "            # If a sub-word does not exist on the vocabulary\n",
    "            if i == 0:\n",
    "                tokens.append(\"[UNK]\")\n",
    "                return tokens          # keeping some information about the word\n",
    "            \n",
    "            # The first sub-word is not going to contain `##`\n",
    "            tokens.append(word[:i])\n",
    "            word = word[i:]\n",
    "\n",
    "            # All the other sub-words are going to contain `##`\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def __decode_word(self, idx):\n",
    "        to_tokens = [self.ivocab_d[i] for i in idx]\n",
    "        return to_tokens[0] + ''.join([token[2:] if (token not in self.special_t) else token for token in to_tokens[1:]])\n",
    "    \n",
    "\n",
    "    def tokenize(self, text, _npad=None):\n",
    "        t_text = []\n",
    "        for word in self._cleaning(text).split():\n",
    "            for token in self.__tokenize_word(word):\n",
    "                t_text.append(token)\n",
    "            t_text.append(\"[SEP]\")\n",
    "\n",
    "        if _npad is not None:\n",
    "            for _ in range(_npad - len(t_text)):\n",
    "                t_text.append(\"[PAD]\")\n",
    "\n",
    "        return t_text\n",
    "\n",
    "    def encode(self, text, _npad=None):\n",
    "        return [self.vocab_d[token] for token in self.tokenize(text, _npad=_npad)]\n",
    "\n",
    "    def decode(self, idx):\n",
    "        text = \"\"\n",
    "        i, j = 0, 0\n",
    "        while i < len(idx) and idx[i] != self.vocab_d[\"[PAD]\"]:\n",
    "            if idx[i] == self.vocab_d[\"[SEP]\"]:\n",
    "                text += self.__decode_word(idx[j: i]) + \" \"\n",
    "                j = i + 1\n",
    "            i += 1\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be ableabab to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPiece(corpus=corpus, _ntokens=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 6314.19it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g', '[SEP]']\n",
      "[52, 22, 26, 19, 3]\n",
      "Hugging \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Hugging\"))\n",
    "print(tokenizer.encode(\"Hugging\"))\n",
    "print(tokenizer.decode([52, 22, 26, 19, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '[UNK]', '[SEP]']\n",
      "[52, 22, 1, 3]\n",
      "Huggi[UNK] \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"Huggi0ng\"))\n",
    "print(tokenizer.encode(\"Huggi0ng\"))\n",
    "print(tokenizer.decode([52, 22, 1, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', '##i', '##s', '[SEP]', 'is', '[SEP]', 'th', '##e', '[SEP]', 'Hugg', '##i', '##n', '##g', '[SEP]', 'Fac', '##e', '[SEP]', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]', '[SEP]']\n",
      "[54, 22, 30, 3, 66, 3, 70, 13, 3, 52, 22, 26, 19, 3, 48, 13, 3, 58, 27, 36, 29, 30, 13, 1, 3]\n",
      "This is the Hugging Face course[UNK] \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"This is the Hugging Face course!\"))\n",
    "print(tokenizer.encode(\"This is the Hugging Face course!\"))\n",
    "print(tokenizer.decode([54, 22, 30, 3, 66, 3, 70, 13, 3, 52, 22, 26, 19, 3, 48, 13, 3, 58, 27, 36, 29, 30, 13, 1, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', '##i', '##s', '[SEP]', 'is', '[SEP]', 'th', '##e', '[SEP]', 'Hugg', '##i', '##n', '##g', '[SEP]', 'Fac', '##e', '[SEP]', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[54, 22, 30, 3, 66, 3, 70, 13, 3, 52, 22, 26, 19, 3, 48, 13, 3, 58, 27, 36, 29, 30, 13, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "This is the Hugging Face course[UNK] \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"This is the Hugging Face course!\", _npad=100))\n",
    "print(tokenizer.encode(\"This is the Hugging Face course!\", _npad=100))\n",
    "print(tokenizer.decode([54, 22, 30, 3, 66, 3, 70, 13, 3, 52, 22, 26, 19, 3, 48, 13, 3, 58, 27, 36, 29, 30, 13, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
