{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Unigram Tokenization Technique\n",
    "\n",
    "Compared `BPE` to `WordPiece`, `Unigram` works in the other direction: it starts from a big vocabulary and `removes` tokens from it until it reaches the desired vocabulary size.\n",
    "\n",
    "There are several options to use to `build` that `base vocabulary`: we can take the `most common substrings` in pre-tokenized words, for instance, or apply `BPE` on the initial corpus with a large vocabulary size.\n",
    "\n",
    "At each step of the `training`, the `Unigram algorithm` computes a `loss` over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would `increase` if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a `lower effect` on the overall loss over the corpus, so in a sense they are “less needed” and are the best candidates for removal.\n",
    "\n",
    "This is all a very `costly operation`, so we don’t just remove the single symbol associated with the lowest loss increase, but the $p$ ($p$ being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the `desired size`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization algorithm\n",
    "\n",
    "A `Unigram model` is a type of language model that considers each token to be `independent` of the tokens before it.\n",
    "\n",
    "The `probability` of a given token is its frequency in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary.\n",
    "\n",
    "Consider the corpus: $(\"hug\", \\; 10), \\; (\"pug\", \\; 5), \\; (\"pun\", \\; 12), \\; (\"bun\", \\; 4), \\; (\"hugs\", \\; 5)$\n",
    "\n",
    "An example of a base vocabulary is $[\"h\",\\; \"u\",\\; \"g\",\\; \"hu\",\\; \"ug\",\\; \"p\",\\; \"pu\",\\; \"n\",\\; \"un\",\\; \"b\",\\; \"bu\",\\; \"s\",\\; \"hug\",\\; \"gs\",\\; \"ugs\"]$\n",
    "\n",
    "Then for example the probability of the word \"pug\" is: $P([\"p\", \"u\", \"g\"]) = P(\"p\")\\cdot P(\"u\")\\cdot P(\"g\") = \\cfrac{5}{210}\\cdot \\cfrac{36}{210} \\cdot \\cfrac{20}{210} = 0.000389$\n",
    "\n",
    "But also: $P([\"pu\", \"g\"]) = P(\"pu\")\\cdot P(\"g\") = \\cfrac{5}{210} \\cdot \\cfrac{20}{210} = 0.0022676$ and also $[\"p\", \"ug\"]$\n",
    "\n",
    "In general, tokenizations with the `least tokens` possible will have the `highest probability` (because of that division by 210 repeated for each token), which corresponds to what we want intuitively: to split a word into the least number of tokens possible.\n",
    "\n",
    "The tokenization of a word with the `Unigram model` is then the tokenization with the highest probability. In the example of \"pug\", here are the probabilities we would get for each possible segmentation:\n",
    "$[\"p\", \"u\", \"g\"] : 0.000389$\n",
    "\n",
    "$[\"p\", \"ug\"] : 0.0022676$\n",
    "\n",
    "$[\"pu\", \"g\"] : 0.0022676$\n",
    "\n",
    "So, \"pug\" would be tokenized as [\"p\", \"ug\"] or [\"pu\", \"g\"], depending on which of those segmentations is encountered first.\n",
    "\n",
    "In general it’s going to be a bit hard finding `all` possible segmentation for each word. Thankfully `Viterbi algorithm` does exactly that. Essentially, we can build a `graph` to detect the possible segmentations of a given word by saying there is a branch from character $a$ to character $b$ if the subword from $a$ to $b$ is in the vocabulary, and attribute to that branch the probability of the subword.\n",
    "\n",
    "To find the `path` in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by `looping` through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to `unroll` the path taken to arrive at the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss\n",
    "\n",
    "Now that we have seen how the tokenization works, we can dive a little more deeply into the `loss` used during training. At any given stage, this `loss` is computed by `tokenizing every word` in the corpus, using the current vocabulary and the `Unigram` model determined by the frequencies of each token in the corpus.\n",
    "\n",
    "Each word in the corpus has a `score`, and the loss is the negative log likelihood of those scores, i.e., $\\sum_{w \\in corpus}freq(w) \\cdot (-log(P(w)))$.\n",
    "\n",
    "Let’s go back to our example with the following corpus: $(\"hug\", \\;10), \\;(\"pug\", \\;5), \\;(\"pun\", \\;12), \\;(\"bun\", \\;4), \\;(\"hugs\", \\;5)$.\n",
    "\n",
    "The tokenization of each word with their respective scores is:\n",
    "$\"hug\": \\;[\"hug\"] \\;(score \\;0.071428)$\n",
    "\n",
    "$\"pug\":\\; [\"pu\",\\; \"g\"] \\;(score \\;0.007710)$\n",
    "\n",
    "$\"pun\": \\;[\"pu\", \\;\"n\"] \\;(score \\;0.006168)$\n",
    "\n",
    "$\"bun\": \\;[\"bu\",\\; \"n\"] \\;(score\\; 0.001451)$\n",
    "\n",
    "$\"hugs\": \\;[\"hug\",\\; \"s\"] \\;(score\\; 0.001701)$\n",
    "\n",
    "So the loss is: $10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8$.\n",
    "\n",
    "\n",
    "Now we need to compute how `removing` each token `affects` the loss. This is rather tedious, so we’ll just do it for two tokens here.  In this (very) particular case, we had two `equivalent tokenizations` of all the words: as we saw earlier, for example, \"pug\" could be tokenized [\"p\", \"ug\"] with the same score. Thus, removing the \"pu\" token from the vocabulary will give the exact same loss.\n",
    "\n",
    "On the other hand, removing \"hug\" will make the loss `worse`, because the tokenization of \"hug\" and \"hugs\" will become:\n",
    "$\"hug\": \\;[\"hu\",\\; \"g\"] \\;(score\\; 0.006802)$\n",
    "\n",
    "$\"hugs\": \\;[\"hu\", \\;\"gs\"] \\;(score \\;0.001701)$\n",
    "\n",
    "These changes will cause the loss to rise by: $- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5$.\n",
    "\n",
    "Therefore, the token \"pu\" will probably be removed from the vocabulary, but not \"hug\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Frequency of words in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is the hugging face course', 'this chapter is about tokenization', 'this section shows several tokenizer algorithms', 'hopefully you will be able to understand how they are trained and generate tokens']\n"
     ]
    }
   ],
   "source": [
    "## Removing punctuations\n",
    "from string import punctuation\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for p in punctuation:\n",
    "        corpus[i] = ' '.join([sub_s.strip().lower() for sub_s in corpus[i].split(p)])\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000002026762FDC0>, {'this': 3, 'is': 2, 'the': 1, 'hugging': 1, 'face': 1, 'course': 1, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'hopefully': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(lambda : 0)\n",
    "\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        word_freq[word] += 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Basic Vocabulary\n",
    "\n",
    "Will contain all character and most common substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Character and Sub-String Frequencies\n",
    "\n",
    "char_freq = defaultdict(lambda : 0)\n",
    "subs_freq = defaultdict(lambda : 0)\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freq[word[i]] += freq # updating chracter frequency\n",
    "\n",
    "        for j in range(i+2, len(word) + 1): # we need to contain `len(word)` because `slice` does not contain `end index`.\n",
    "            subs_freq[word[i:j]] += freq    # updating substring frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('th', 6),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('thi', 3),\n",
       " ('this', 3),\n",
       " ('hi', 3),\n",
       " ('his', 3),\n",
       " ('ou', 3)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sorting the Frequencies for the Sub-String, in order to keep the most frequent ones\n",
    "\n",
    "sorted_subs = sorted(subs_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_subs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate those two dictionaries and creating a Vocabulary of size: 300\n",
    "\n",
    "token_freq = list(char_freq.items()) + sorted_subs[: 300 - len(char_freq)]\n",
    "\n",
    "# Converting the List of Typles into a Dictionary\n",
    "token_freq = dict(token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Frequencies into Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum(token_freq.values())\n",
    "\n",
    "## Creating the Model: Going to store the `log`` of those probabilities\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freq.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm\n",
    "\n",
    "We will store one `dictionary` per position in the word (from 0 to its total length), with two `keys`: the *index of the start* of the *last* token in the best segmentation, and the *score* of the best segmentation.\n",
    "\n",
    "Populating the list is done with just two loops: the `main loop` goes over each start position, and the `second loop` tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in best_segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best Segmentations: each segmentation is a List of Dictionaries (start, score) that represent each character of the word.\n",
    "##  `start`: it's the best segmentation of the word that starts from `start` and ends in that character\n",
    "##  `score`: it's the score of the segmentation\n",
    "\n",
    "def _best_segmentations(word, model):\n",
    "    # Contains starting character index of the best word segmentations that ends in this particular index\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 0}] + [{\"start\": None, \"score\": None} for _ in range(len(word))] # adding one more element for efficiency\n",
    "\n",
    "    for start_idx in range(len(word)):\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"] # initially is 0, but then: `{\"start\": start_idx, \"score\": score}`\n",
    "\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx: end_idx]\n",
    "\n",
    "            if (token in model) and (best_score_at_start is not None):\n",
    "                score = model[token] + best_score_at_start\n",
    "\n",
    "                # If we have found a better segmentation ending at `end_idx``, we update\n",
    "                if (best_segmentations[end_idx][\"score\"] is None) or (best_segmentations[end_idx][\"score\"] > score):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "    \n",
    "    return best_segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0, 'score': 0},\n",
       " {'start': 0, 'score': 3.871201010907891},\n",
       " {'start': 0, 'score': 6.269096283706261},\n",
       " {'start': 0, 'score': 6.269096283706261},\n",
       " {'start': 0, 'score': 6.269096283706261},\n",
       " {'start': 4, 'score': 9.973243209950986}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_best_segmentations(\"huggs\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding a word base on the best segmentation we have found\n",
    "\n",
    "def segment_word(word, model):\n",
    "    best_segmentations = _best_segmentations(word, model) # contains the starting indexes of the best segmentation ending at each character position\n",
    "    segmentation = best_segmentations[-1]                 # contains the index and the score of the best segmentation ending at the last character\n",
    "\n",
    "    if segmentation[\"score\"] is None: return ([\"[UNK]\"], None) # If the mode cannot segment the word, due to OOV\n",
    "\n",
    "    score = segmentation[\"score\"] # the score of the best segmentation ending at the last `word` character\n",
    "    start = segmentation[\"start\"] # the starting index of the best segmentation ending at the last `word` character\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "\n",
    "    # Calculating the best segmentation\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start: end])\n",
    "        start, end = best_segmentations[start][\"start\"], start # start -> the index of the best segmentation ending at `end` | end -> start, we have segment the word from end to start\n",
    "    \n",
    "    tokens.insert(0, word[start: end]) # adding the last segment to the list\n",
    "\n",
    "    return (tokens, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hugg', 's'], 9.973243209950986)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word(\"huggs\", model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Function for Computing the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    l = 0\n",
    "    for word, freq in word_freq.items():\n",
    "        _, word_loss = segment_word(word, model)\n",
    "        l += (freq * word_loss)\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.6900959655888"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Scores after Deleting a Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "\n",
    "    for token in model.keys():\n",
    "        if len(token) == 1: # preventing the model for deleting character-level tokens\n",
    "            scores[token] = float(\"inf\")\n",
    "\n",
    "        else:\n",
    "            model_without_token = deepcopy(model)\n",
    "            model_without_token.pop(token) # removing the token from the model - Dictionary\n",
    "\n",
    "            scores[token] = compute_loss(model_without_token) - model_loss\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is very `inefficient`, so `SentencePiece` uses an approximation of the loss of the model without token X: instead of starting from scratch, it just `replaces` token X by its segmentation in the vocabulary that is left. This way, all the scores can be computed at once at the same time as the model loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0704231661555355\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary of `n` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, p_rem, size):\n",
    "    while len(model) > size:\n",
    "        # Calculating scores and sorting them from lower to higher\n",
    "        scores = sorted(compute_scores(model).items(), key=lambda x: x[1])\n",
    "\n",
    "        # Removing the percentage specified of low-scored tokens\n",
    "        for i in range(int(len(model) * p_rem)):\n",
    "            token_freq.pop(scores[i][0])\n",
    "\n",
    "        # Initializing the new model\n",
    "        total_sum = sum([freq for token, freq in token_freq.items()])\n",
    "        model = {token: -log(freq / total_sum) for token, freq in token_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, 0.1, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Function for Tokenizing a Sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model, pre_f):\n",
    "    # Preprocessing the Text\n",
    "    pre_text = pre_f(text)\n",
    "\n",
    "    tokenized_text = [segment_word(w, model)[0] for w in pre_text.split()] # 0: for only getting the segmentation\n",
    "\n",
    "    return sum(tokenized_text, []) # starting from [] and adding elements from the `tokenized_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(text):\n",
    "    for p in punctuation:\n",
    "        text = ' '.join([sub_s.strip().lower() for sub_s in text.split(p)])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'hugging',\n",
       " 'face',\n",
       " 'course',\n",
       " 'hugg',\n",
       " 's',\n",
       " 'to',\n",
       " 'ever',\n",
       " 'y',\n",
       " 'on',\n",
       " 'e']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course. Huggs to everyone!!!\", model, pre_f=pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.269096283706261"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freq[\"hugg\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
