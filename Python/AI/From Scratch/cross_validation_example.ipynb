{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NhVaa2BOnXlJ"
      },
      "outputs": [],
      "source": [
        "from Tokenizers import WordPiece\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import Accuracy, Recall, Precision, F1Score\n",
        "from torchinfo import summary\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "from math import ceil\n",
        "from random import randint"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KG0bSY-inXlO"
      },
      "source": [
        "## Setting Device Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyEplS8SnXlR",
        "outputId": "7e4c1c1e-a590-481a-a0b4-8cf7e9a754f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X3inl2TSnXlS"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3oeozOiWnXlT",
        "outputId": "75224d86-3005-4c25-9b38-e8343f13d660"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label                                               Text\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro...\n",
              "...     ...                                                ...\n",
              "5567      1  This is the 2nd time we have tried 2 contact u...\n",
              "5568      0              Will Ì_ b going to esplanade fr home?\n",
              "5569      0  Pity, * was in mood for that. So...any other s...\n",
              "5570      0  The guy did some bitching but I acted like i'd...\n",
              "5571      0                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"C:\\\\Users\\\\jacob\\\\Documents\\\\Programming\\\\Theory\\\\Python\\\\AI\\\\Datasets\\\\NLP\\\\sms_spam.csv\", encoding=\"latin-1\")[[\"v1\", \"v2\"]]\n",
        "df.rename(columns={\"v1\": \"Label\", \"v2\": \"Text\"}, inplace=True)\n",
        "\n",
        "df[\"Label\"] = df[\"Label\"].map({\n",
        "    \"ham\": 0,\n",
        "    \"spam\": 1\n",
        "})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX76lwoXnXlU",
        "outputId": "27e026de-2480-4dae-b394-f72d3e1ccc9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Label\n",
              "0    4825\n",
              "1     747\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"Label\"].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K_I9QVOZnXlU"
      },
      "source": [
        "## Converting Dataset to Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MA9vAS9InXlV"
      },
      "outputs": [],
      "source": [
        "df[\"Cl_Text\"] = df[\"Text\"].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vc4tBQT5nXlV",
        "outputId": "4827b331-65e9-4c72-b4a1-0b7f23a38db9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cl_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>will ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>pity, * was in mood for that. so...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>the guy did some bitching but i acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>rofl. its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label                                               Text  \\\n",
              "0         0  Go until jurong point, crazy.. Available only ...   \n",
              "1         0                      Ok lar... Joking wif u oni...   \n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3         0  U dun say so early hor... U c already then say...   \n",
              "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
              "...     ...                                                ...   \n",
              "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
              "5568      0              Will Ì_ b going to esplanade fr home?   \n",
              "5569      0  Pity, * was in mood for that. So...any other s...   \n",
              "5570      0  The guy did some bitching but I acted like i'd...   \n",
              "5571      0                         Rofl. Its true to its name   \n",
              "\n",
              "                                                Cl_Text  \n",
              "0     go until jurong point, crazy.. available only ...  \n",
              "1                         ok lar... joking wif u oni...  \n",
              "2     free entry in 2 a wkly comp to win fa cup fina...  \n",
              "3     u dun say so early hor... u c already then say...  \n",
              "4     nah i don't think he goes to usf, he lives aro...  \n",
              "...                                                 ...  \n",
              "5567  this is the 2nd time we have tried 2 contact u...  \n",
              "5568              will ì_ b going to esplanade fr home?  \n",
              "5569  pity, * was in mood for that. so...any other s...  \n",
              "5570  the guy did some bitching but i acted like i'd...  \n",
              "5571                         rofl. its true to its name  \n",
              "\n",
              "[5572 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WPr0BBoXnXlW"
      },
      "source": [
        "## Converting Dataset into List of Strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLJQWlQFnXlX",
        "outputId": "f32dbc38-6209-47cf-da21-2815c592b2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5572\n",
            "i'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? i've cried enough today.\n"
          ]
        }
      ],
      "source": [
        "corpus = list(df.Cl_Text)\n",
        "\n",
        "print(len(corpus))\n",
        "print(corpus[10])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ym1PvFhnXlY"
      },
      "source": [
        "## Creating Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-so2NVTDnXlZ"
      },
      "outputs": [],
      "source": [
        "w = WordPiece(corpus=corpus, ntokens=1_000, cleaning=lambda text: text.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "388RdaJMnXlZ",
        "outputId": "aa76ff70-fca3-4003-cb81-831dd83110ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating Vocabulary: 100%|██████████| 847/847 [00:54<00:00, 15.42it/s]\n"
          ]
        }
      ],
      "source": [
        "w.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYKv4gTLnXla",
        "outputId": "883c0410-500f-435f-b190-ec65b8aa5437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'[CLS]': 0, '[UNK]': 1, '[PAD]': 2, '[SEP]': 3, '!': 4, '#': 5, '##!': 6, '##!\\x8eö´\\x89ó_': 7, '##!\\x8eö´\\x89ó_?': 8, '##!\\x8eö´\\x89ó_??': 9, '##!\\x8eö´\\x89ó_??\\x8bû¬': 10, '##\"': 11, '###': 12, '###&': 13, '##$': 14, '##$7': 15, '##$70': 16, '##$700': 17, '##%': 18, '##&': 19, \"##'\": 20, '##(': 21, '##(å£': 22, '##)': 23, '##*': 24, '##**': 25, '##****': 26, '##******': 27, '##**********': 28, '##**************': 29, '##****7': 30, '##*å£1': 31, '##+': 32, '##+6*å£1': 33, '##+6+': 34, '##+å£1': 35, '##,': 36, '##-': 37, '##-$': 38, '##-$9': 39, '##-$90': 40, '##-$900': 41, '##-(': 42, '##-)': 43, '##-|': 44, '##-å£': 45, '##-å£5': 46, '##.': 47, '##/': 48, '##/~': 49, '##/ì¼': 50, '##/ì¼1': 51, '##0': 52, '##0-å£5': 53, '##01216+': 54, '##1': 55, '##1(å£': 56, '##11(å£': 57, '##1216+': 58, '##141701216+': 59, '##16+': 60, '##16+1': 61, '##16+å£1': 62, '##1701216+': 63, '##18+': 64, '##18+)': 65, '##2': 66, '##216+': 67, '##2228>>': 68, '##228>>': 69, '##2735=å£45': 70, '##28>>': 71, '##3': 72, '##35=å£4': 73, '##4': 74, '##41701216+': 75, '##498****7': 76, '##4q=': 77, '##5': 78, '##5=å£4': 79, '##6': 80, '##6*å£1': 81, '##6+': 82, '##6+å£1': 83, '##7': 84, '##701216+': 85, '##735=å£4': 86, '##735=å£45': 87, '##78498****7': 88, '##8': 89, '##8****7': 90, '##8+': 91, '##8+)': 92, '##8+6*å£1': 93, '##82228>>': 94, '##8498****7': 95, '##8>>': 96, '##9': 97, '##911(å£': 98, '##98****7': 99, '##9911(å£': 100, '##:': 101, '##:(': 102, '##:)': 103, '##:):-):-):-)': 104, '##:-(': 105, '##:-)': 106, '##:-)\"': 107, '##:-):-)': 108, '##:-):-):)': 109, '##:-):-):):-)': 110, '##:-):-):-)': 111, '##:-);-)': 112, '##:-);-)b-)': 113, '##:-|': 114, '##:\\\\': 115, '##;': 116, '##;#&': 117, '##;-(': 118, '##;-)': 119, '##;:(': 120, '##=': 121, '##=q': 122, '##=qj': 123, '##=å£': 124, '##=å£4': 125, '##>': 126, '##>>': 127, '##>>>': 128, '##?': 129, '##?:-|': 130, '##?ì_': 131, '##@': 132, '##@å£': 133, '##@å£1': 134, '##[': 135, '##[/': 136, '##\\\\': 137, '##\\\\\"': 138, '##\\\\\"\"': 139, '##\\\\\":-)\"': 140, \"##\\\\'\": 141, '##]': 142, '##_': 143, '##a': 144, '##b': 145, '##b-)': 146, '##c': 147, '##d': 148, '##e': 149, '##f': 150, '##g': 151, '##h': 152, '##i': 153, '##j': 154, '##k': 155, '##l': 156, '##m': 157, '##n': 158, '##o': 159, '##p': 160, '##q': 161, '##q=': 162, '##r': 163, '##s': 164, '##t': 165, '##u': 166, '##v': 167, '##w': 168, '##x': 169, '##xå£7': 170, '##y': 171, '##z': 172, '##|': 173, '##~': 174, '##~j': 175, '##\\x89': 176, '##\\x89û': 177, '##\\x89û_': 178, '##\\x89ûª': 179, '##\\x89ûó': 180, '##\\x89û÷': 181, '##\\x8b': 182, '##\\x8bû': 183, '##\\x8bû¬': 184, '##\\x8e': 185, '##\\x8eö': 186, '##\\x8eö´': 187, '##\\x8eö´\\x89': 188, '##\\x8eö´\\x89ó': 189, '##\\x8eö´\\x89ó_': 190, '##£': 191, '##©': 192, '##ª': 193, '##¬': 194, '##´': 195, '##¼': 196, '##á': 197, '##â': 198, '##ä': 199, '##å': 200, '##å£': 201, '##å£1': 202, '##å£1/': 203, '##å£2': 204, '##å£20': 205, '##å£200': 206, '##å£2000': 207, '##å£3': 208, '##å£38': 209, '##å£7': 210, '##åá': 211, '##åó': 212, '##åó-': 213, '##åõ': 214, '##è': 215, '##ì': 216, '##ì_': 217, '##ì¬': 218, '##ì¼': 219, '##ìâ': 220, '##ìä': 221, '##ï': 222, '##ð': 223, '##ò': 224, '##ó': 225, '##ô': 226, '##õ': 227, '##ö': 228, '##÷': 229, '##û': 230, '#5': 231, '#50': 232, '#500': 233, '#5000': 234, '$': 235, '%': 236, '&': 237, \"'\": 238, '(': 239, '($': 240, '($9': 241, '($90': 242, '($900': 243, '($900)': 244, '(18+)': 245, '(\\x89û': 246, '(\\x89û_': 247, '(\\x89û_)': 248, '(å£': 249, '(å£4': 250, ')': 251, '*': 252, '*****': 253, '***********': 254, '***************': 255, '+': 256, '+å£': 257, '+å£4': 258, '+å£40': 259, '+å£400': 260, ',': 261, '-': 262, '.': 263, '/': 264, '/-': 265, '/7': 266, '0': 267, '02': 268, '02/': 269, '02/0': 270, '02/06': 271, '02/06/': 272, '02/06/0': 273, '02/06/03': 274, '02/06/03!': 275, '02/09': 276, '02/09/': 277, '02/09/0': 278, '02/09/03': 279, '02/09/03!': 280, '07': 281, '07/': 282, '07/1': 283, '07/11': 284, '07/11/': 285, '07/11/0': 286, '07/11/04': 287, '077': 288, '0773': 289, '07732': 290, '077325': 291, '0773258': 292, '07732584': 293, '077325843': 294, '0773258435': 295, '07732584351': 296, '07734': 297, '077343': 298, '0773439': 299, '07734396': 300, '077343968': 301, '0773439683': 302, '07734396839': 303, '0774': 304, '07742': 305, '077426': 306, '0774267': 307, '07742676': 308, '077426769': 309, '0774267696': 310, '07742676969': 311, '0775': 312, '07753': 313, '077537': 314, '0775374': 315, '07753741': 316, '077537412': 317, '0775374122': 318, '07753741225': 319, '0776': 320, '0776x': 321, '0776xx': 322, '0776xxx': 323, '0776xxxx': 324, '0776xxxxx': 325, '0776xxxxxx': 326, '0776xxxxxxx': 327, '0778': 328, '07786': 329, '077862': 330, '0778620': 331, '07786200': 332, '077862001': 333, '0778620011': 334, '07786200117': 335, '077x': 336, '077xx': 337, '077xxx': 338, '078': 339, '078498****7': 340, '079': 341, '0794': 342, '07946': 343, '079467': 344, '0794674': 345, '07946746': 346, '079467462': 347, '0794674629': 348, '07946746291': 349, '07946746291/': 350, '07946746291/0': 351, '07946746291/07': 352, '07946746291/078': 353, '07946746291/0788': 354, '07946746291/07880': 355, '07946746291/078808': 356, '07946746291/0788086': 357, '07946746291/07880867': 358, '07946746291/078808678': 359, '07946746291/0788086786': 360, '07946746291/07880867867': 361, '0796': 362, '0796x': 363, '0796xx': 364, '0796xxx': 365, '0796xxxx': 366, '0796xxxxx': 367, '0796xxxxxx': 368, '0797': 369, '07973': 370, '079737': 371, '0797378': 372, '07973788': 373, '079737882': 374, '0797378824': 375, '07973788240': 376, '08': 377, '087': 378, '0870': 379, '0870141701216+': 380, '08707': 381, '087073': 382, '0870737': 383, '08707379': 384, '087073791': 385, '0870737910': 386, '08707379102': 387, '087073791021': 388, '0870737910216': 389, '087075': 390, '087078': 391, '0870780': 392, '08707808': 393, '087078082': 394, '0870780822': 395, '08707808226': 396, '0871': 397, '0871-': 398, '0871-4': 399, '0871-47': 400, '0871-471': 401, '0871-4719': 402, '0871-4719-': 403, '0871-4719-5': 404, '0871-4719-52': 405, '0871-4719-523': 406, '0871-8': 407, '0871-87': 408, '0871-872': 409, '0871-872-': 410, '0871-872-9': 411, '0871-872-97': 412, '0871-872-975': 413, '0871-872-9755': 414, '0871-872-9758': 415, '08710': 416, '087104': 417, '0871047': 418, '08710471': 419, '087104711': 420, '0871047111': 421, '08710471114': 422, '087104711148': 423, '08712': 424, '087121': 425, '087123': 426, '087124': 427, '0871240': 428, '08712400': 429, '087124002': 430, '0871240020': 431, '08712400200': 432, '087124006': 433, '0871240060': 434, '08712400603': 435, '08712402': 436, '087124020': 437, '0871240205': 438, '08712402050': 439, '087124025': 440, '0871240257': 441, '08712402578': 442, '087124027': 443, '0871240277': 444, '08712402779': 445, '087124029': 446, '0871240290': 447, '08712402902': 448, '0871240297': 449, '08712402972': 450, '08712404': 451, '087124040': 452, '0871240400': 453, '08712404000': 454, '08712405': 455, '087124050': 456, '0871240502': 457, '08712405020': 458, '08712405022': 459, '08712405022,': 460, '0871246': 461, '087127': 462, '0871277': 463, '08712778': 464, '087127781': 465, '0871277810': 466, '08712778107': 467, '087127781071': 468, '0871277810710': 469, '08712778108': 470, '087127781081': 471, '0871277810810': 472, '08712778109': 473, '087127781091': 474, '0871277810910': 475, '08714': 476, '087143': 477, '0871434': 478, '08714342': 479, '087143423': 480, '0871434239': 481, '08714342399': 482, '087147': 483, '0871471': 484, '08714712': 485, '087147123': 486, '0871471237': 487, '08714712377': 488, '087147123779': 489, '08714712379': 490, '0871471238': 491, '08714712388': 492, '0871471239': 493, '08714712394': 494, '087147124': 495, '0871471241': 496, '08714712412': 497, '08714714': 498, '087147140': 499, '0871471401': 500, '08714714011': 501, '08715': 502, '087152': 503, '0871520': 504, '08715203': 505, '087152030': 506, '0871520302': 507, '08715203028': 508, '087152036': 509, '0871520364': 510, '08715203649': 511, '0871520365': 512, '08715203652': 513, '08715203656': 514, '0871520367': 515, '08715203677': 516, '0871520368': 517, '08715203685': 518, '0871520369': 519, '08715203694': 520, '08715205': 521, '087152052': 522, '0871520527': 523, '08715205273': 524, '087155': 525, '0871550': 526, '08715500': 527, '087155000': 528, '0871550002': 529, '08715500022': 530, '087157': 531, '0871570': 532, '08715705': 533, '087157050': 534, '0871570502': 535, '08715705022': 536, '08715705022,': 537, '08717': 538, '087171': 539, '0871711': 540, '08717111': 541, '087171118': 542, '0871711182': 543, '08717111821': 544, '0871716': 545, '08717168': 546, '087171685': 547, '0871716852': 548, '08717168528': 549, '087172': 550, '0871720': 551, '08717205': 552, '087172055': 553, '0871720554': 554, '08717205546': 555, '087175': 556, '0871750': 557, '08717507': 558, '087175073': 559, '0871750738': 560, '08717507382': 561, '08717509': 562, '087175099': 563, '0871750999': 564, '08717509990': 565, '087178': 566, '0871789': 567, '08717890': 568, '087178908': 569, '0871789089': 570, '08717890890': 571, '08717890890å£1': 572, '08717895': 573, '087178956': 574, '0871789569': 575, '08717895698': 576, '08717898': 577, '087178980': 578, '0871789803': 579, '08717898035': 580, '08718': 581, '087187': 582, '0871871': 583, '08718711': 584, '087187111': 585, '0871871110': 586, '08718711108': 587, '0871872': 588, '08718726': 589, '087187262': 590, '0871872627': 591, '08718726270': 592, '087187262701': 593, '087187269': 594, '0871872697': 595, '08718726970': 596, '08718726971': 597, '08718726978': 598, '08718727': 599, '087187272': 600, '0871872720': 601, '08718727200': 602, '087187272008': 603, '087187278': 604, '0871872786': 605, '08718727868': 606, '0871872787': 607, '08718727870': 608, '0871873': 609, '08718730': 610, '087187305': 611, '0871873055': 612, '08718730555': 613, '087187306': 614, '0871873066': 615, '08718730666': 616, '08718738': 617, '087187380': 618, '0871873800': 619, '08718738001': 620, '08718738002': 621, '0871873803': 622, '08718738034': 623, '08719': 624, '087191': 625, '0871918': 626, '08719180': 627, '087191802': 628, '0871918021': 629, '08719180219': 630, '0871918024': 631, '08719180248': 632, '08719181': 633, '087191812': 634, '0871918125': 635, '08719181259': 636, '087191815': 637, '0871918150': 638, '08719181503': 639, '0871918151': 640, '08719181513': 641, '087198': 642, '0871983': 643, '08719839': 644, '087198398': 645, '0871983983': 646, '08719839835': 647, '0871989': 648, '08719899': 649, '087198992': 650, '0871989921': 651, '08719899217': 652, '0871989922': 653, '08719899229': 654, '0871989923': 655, '08719899230': 656, '09': 657, '090': 658, '0904': 659, '09041': 660, '090419': 661, '0904194': 662, '0905': 663, '09058': 664, '0906': 665, '09061': 666, '090611': 667, '090612': 668, '0906122': 669, '09061221': 670, '090617': 671, '0906174': 672, '09061743': 673, '090617433': 674, '0906174338': 675, '09061743386': 676, '090617438': 677, '09061744': 678, '090617445': 679, '0906174455': 680, '09061744553': 681, '09061749': 682, '090617496': 683, '0906179': 684, '09063': 685, '090634': 686, '0906344': 687, '09063442': 688, '090634421': 689, '0906344215': 690, '09063442151': 691, '0906345': 692, '09063458': 693, '090634581': 694, '0906345813': 695, '0906346': 696, '09063463': 697, '090634633': 698, '09064': 699, '09065': 700, '090651': 701, '0906517': 702, '090653': 703, '0906539': 704, '09065394': 705, '090653949': 706, '0906539497': 707, '090659': 708, '0906598': 709, '09065989': 710, '090659891': 711, '0906598918': 712, '09065989182': 713, '09066': 714, '090663': 715, '0906635': 716, '09066358': 717, '090663581': 718, '0906635815': 719, '09066358152': 720, '090663583': 721, '0906635836': 722, '09066358361': 723, '0906636': 724, '09066361': 725, '090663619': 726, '0906636192': 727, '09066361921': 728, '09066362': 729, '090663622': 730, '0906636222': 731, '0906636223': 732, '09066362231': 733, '09066364': 734, '090663643': 735, '0906636431': 736, '09066364311': 737, '0906636434': 738, '09066364349': 739, '090663645': 740, '0906636458': 741, '09066364589': 742, '09066368': 743, '090663683': 744, '0906636832': 745, '09066368327': 746, '090663684': 747, '0906636847': 748, '090663687': 749, '0906636875': 750, '09066368753': 751, '0906638': 752, '09066382': 753, '090663824': 754, '0906638242': 755, '09066382422': 756, '090666': 757, '0906661': 758, '09066612': 759, '090666126': 760, '0906661266': 761, '09066612661': 762, '0906664': 763, '09066649': 764, '090666497': 765, '0906664973': 766, '09066649731': 767, '0906666': 768, '0907': 769, '09071': 770, '090715': 771, '0907151': 772, '09071512': 773, '090715124': 774, '0907151243': 775, '09071512432': 776, '09071512433': 777, '09071517': 778, '090715178': 779, '0907151786': 780, '09071517866': 781, '09077': 782, '090778': 783, '0907781': 784, '09077818': 785, '090778181': 786, '0907781815': 787, '09077818151': 788, '0909': 789, '09094': 790, '090941': 791, '090946': 792, '0909464': 793, '09094646': 794, '090946466': 795, '0909464663': 796, '09094646631': 797, '090946468': 798, '0909464689': 799, '09094646899': 800, '09099': 801, '090997': 802, '0909972': 803, '09099725': 804, '090997258': 805, '0909972582': 806, '09099725823': 807, '09099726': 808, '090997263': 809, '0909972639': 810, '09099726395': 811, '090997264': 812, '0909972642': 813, '09099726429': 814, '0909972648': 815, '09099726481': 816, '090997265': 817, '0909972655': 818, '09099726553': 819, '091': 820, '0911': 821, '09111': 822, '097': 823, '1': 824, '16+': 825, '18+': 826, '18+6*å£1': 827, '2': 828, '3': 829, '4': 830, '4*': 831, '5': 832, '5+': 833, '5+-': 834, '5+3': 835, '5+3+': 836, '5+3+2': 837, '5+3+2=': 838, '6': 839, '62735=å£45': 840, '62735=å£450': 841, '69': 842, '691': 843, '6910': 844, '69101': 845, '692': 846, '6920': 847, '69200': 848, '696': 849, '6966': 850, '69669': 851, '6969': 852, '69696': 853, '69698': 854, '698': 855, '6985': 856, '69855': 857, '69855,': 858, '6986': 859, '69866': 860, '6987': 861, '69876': 862, '6988': 863, '69888': 864, '699': 865, '69911(å£': 866, '69911(å£1': 867, '6996': 868, '69969': 869, '6998': 870, '69988': 871, '7': 872, '7+': 873, '7+2': 874, '7+2+': 875, '7+2+5': 876, '7+2+5=': 877, '7+2+5=?': 878, '7+2+5=??': 879, '7+2+5=???': 880, '7+2+5=????': 881, '7+2+5=?????': 882, '8': 883, '8+6+': 884, '8+6+3': 885, '8+6+3=': 886, '9': 887, '9+': 888, '9+2': 889, '9+2+': 890, '9+2+4': 891, '9+2+4=': 892, ':': 893, ':(': 894, ':)': 895, ':-': 896, ':-(': 897, ':-)': 898, ':-):):-):-):-)': 899, ':-):-)': 900, ':-/': 901, ':/': 902, ';': 903, ';)': 904, ';-(': 905, ';-)': 906, ';_': 907, ';_;': 908, '<': 909, '=': 910, '=)': 911, '>': 912, '>>': 913, '>>>': 914, '?': 915, '@': 916, '[': 917, '[\\x89û': 918, '[\\x89û_': 919, '[\\x89û_]': 920, '\\\\': 921, '\\\\\"': 922, '\\\\\"\"': 923, '^': 924, '_': 925, '__': 926, '___': 927, '____': 928, 'a': 929, 'b': 930, 'c': 931, 'd': 932, 'e': 933, 'eå£': 934, 'f': 935, 'g': 936, 'h': 937, 'i': 938, 'j': 939, 'k': 940, 'l': 941, 'm': 942, 'n': 943, 'o': 944, 'p': 945, 'q': 946, 'r': 947, 's': 948, 't': 949, 'u': 950, 'v': 951, 'w': 952, 'x': 953, 'x\\\\\"\"': 954, 'y': 955, 'z': 956, '|': 957, '||': 958, '~': 959, '\\x89': 960, '\\x89û': 961, '\\x89û_': 962, '\\x89ûï': 963, '\\x89ûò': 964, 'å': 965, 'å£': 966, 'å£1': 967, 'å£2': 968, 'å£3': 969, 'å£3/': 970, 'å£35': 971, 'å£350': 972, 'å£4': 973, 'å£5': 974, 'å£50-å£5': 975, 'å£50-å£50': 976, 'å£50-å£500': 977, 'å£6': 978, 'å£7': 979, 'å£71': 980, 'å£75': 981, 'å£75,': 982, 'å£75,0': 983, 'å£75,00': 984, 'å£75,000': 985, 'å£750': 986, 'å£79': 987, 'å£8': 988, 'å£80': 989, 'å£800': 990, 'å£9': 991, 'å£90': 992, 'å£900': 993, 'åè': 994, 'åè1': 995, 'åè10': 996, 'åð': 997, 'åò': 998, 'åô': 999, 'ì': 1000, 'ì_': 1001, 'ì©': 1002, 'ìï': 1003}\n"
          ]
        }
      ],
      "source": [
        "print(w.vocab_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nqZ82DenXla",
        "outputId": "46c0de24-a1dd-4300-c3b2-1853127ccd74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1004"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(w.vocab_l)\n",
        "\n",
        "vocab_size"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vXgfzz-lnXlb"
      },
      "source": [
        "## Encoding the Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMdWTG1qnXlc",
        "outputId": "f4e0089b-f059-4e15-fd07-6b28790606c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting max text length for padding\n",
        "max_len = df[\"Cl_Text\"].str.len().max()\n",
        "\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CAgqGzvJnXlc"
      },
      "outputs": [],
      "source": [
        "max_len += 1 # due to [SEP] at the end of each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DldxCaGYnXld"
      },
      "outputs": [],
      "source": [
        "df[\"En_Text\"] = df[\"Cl_Text\"].apply(lambda x: [w.vocab_d[\"[CLS]\"]] + w.encode(text=x, npad=max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Nn2Mm1PfnXld"
      },
      "outputs": [],
      "source": [
        "max_len += 1 # due to the CLS token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "B5UrqHy5nXle",
        "outputId": "824bfbd6-f93c-43a2-8f95-1afcd22eee8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cl_Text</th>\n",
              "      <th>En_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "      <td>[0, 936, 159, 3, 950, 158, 165, 153, 156, 3, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "      <td>[0, 944, 155, 3, 941, 144, 163, 47, 47, 47, 3,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "      <td>[0, 935, 163, 149, 149, 3, 933, 158, 165, 163,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "      <td>[0, 950, 3, 932, 166, 158, 3, 948, 144, 171, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "      <td>[0, 943, 144, 152, 3, 938, 3, 932, 159, 158, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>[0, 949, 152, 153, 164, 3, 938, 164, 3, 949, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>0</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>will ì_ b going to esplanade fr home?</td>\n",
              "      <td>[0, 952, 153, 156, 156, 3, 1001, 3, 930, 3, 93...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>0</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>pity, * was in mood for that. so...any other s...</td>\n",
              "      <td>[0, 945, 153, 165, 171, 36, 3, 252, 3, 952, 14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>0</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>the guy did some bitching but i acted like i'd...</td>\n",
              "      <td>[0, 949, 152, 149, 3, 936, 166, 171, 3, 932, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>0</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>rofl. its true to its name</td>\n",
              "      <td>[0, 947, 159, 150, 156, 47, 3, 938, 165, 164, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label                                               Text  \\\n",
              "0         0  Go until jurong point, crazy.. Available only ...   \n",
              "1         0                      Ok lar... Joking wif u oni...   \n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3         0  U dun say so early hor... U c already then say...   \n",
              "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
              "...     ...                                                ...   \n",
              "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
              "5568      0              Will Ì_ b going to esplanade fr home?   \n",
              "5569      0  Pity, * was in mood for that. So...any other s...   \n",
              "5570      0  The guy did some bitching but I acted like i'd...   \n",
              "5571      0                         Rofl. Its true to its name   \n",
              "\n",
              "                                                Cl_Text  \\\n",
              "0     go until jurong point, crazy.. available only ...   \n",
              "1                         ok lar... joking wif u oni...   \n",
              "2     free entry in 2 a wkly comp to win fa cup fina...   \n",
              "3     u dun say so early hor... u c already then say...   \n",
              "4     nah i don't think he goes to usf, he lives aro...   \n",
              "...                                                 ...   \n",
              "5567  this is the 2nd time we have tried 2 contact u...   \n",
              "5568              will ì_ b going to esplanade fr home?   \n",
              "5569  pity, * was in mood for that. so...any other s...   \n",
              "5570  the guy did some bitching but i acted like i'd...   \n",
              "5571                         rofl. its true to its name   \n",
              "\n",
              "                                                En_Text  \n",
              "0     [0, 936, 159, 3, 950, 158, 165, 153, 156, 3, 9...  \n",
              "1     [0, 944, 155, 3, 941, 144, 163, 47, 47, 47, 3,...  \n",
              "2     [0, 935, 163, 149, 149, 3, 933, 158, 165, 163,...  \n",
              "3     [0, 950, 3, 932, 166, 158, 3, 948, 144, 171, 3...  \n",
              "4     [0, 943, 144, 152, 3, 938, 3, 932, 159, 158, 2...  \n",
              "...                                                 ...  \n",
              "5567  [0, 949, 152, 153, 164, 3, 938, 164, 3, 949, 1...  \n",
              "5568  [0, 952, 153, 156, 156, 3, 1001, 3, 930, 3, 93...  \n",
              "5569  [0, 945, 153, 165, 171, 36, 3, 252, 3, 952, 14...  \n",
              "5570  [0, 949, 152, 149, 3, 936, 166, 171, 3, 932, 1...  \n",
              "5571  [0, 947, 159, 150, 156, 47, 3, 938, 165, 164, ...  \n",
              "\n",
              "[5572 rows x 4 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXFyuE54nXle",
        "outputId": "d51c4e89-6871-4869-cb41-0d491ed42f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "912\n",
            "912\n"
          ]
        }
      ],
      "source": [
        "print(df[\"En_Text\"].str.len().max())\n",
        "print(max_len)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0DMqxfzSnXlf"
      },
      "source": [
        "## Converting DataFrame to Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "egw8W7dbnXlg"
      },
      "outputs": [],
      "source": [
        "class FinancialNewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.samples = [(dataframe[\"En_Text\"][i], dataframe[\"Label\"][i]) for i in range(len(dataframe))]\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "        self.idx_to_classes = {i: c for i, c in enumerate(self.classes)}\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        if isinstance(index, slice):\n",
        "            return [(torch.tensor(sample[0], dtype=torch.long), sample[1]) for sample in self.samples[index]] # List (Tuple (Tensor, Int) )\n",
        "        return (torch.tensor(self.samples[index][0], dtype=torch.long), self.samples[index][1])               # Tuple (Tensor, Int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZNlDHzvnXlg",
        "outputId": "49ca2dc7-0f35-4f14-e3bd-82c4dc37a231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5572\n",
            "[(tensor([  0, 944, 155,   3, 941, 144, 163,  47,  47,  47,   3, 939, 159, 155,\n",
            "        153, 158, 151,   3, 952, 153, 150,   3, 950,   3, 944, 158, 153,  47,\n",
            "         47,  47,   3,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2]), 0), (tensor([  0, 935, 163, 149, 149,   3, 933, 158, 165, 163, 171,   3, 938, 158,\n",
            "          3, 828,   3, 929,   3, 952, 155, 156, 171,   3, 931, 159, 157, 160,\n",
            "          3, 949, 159,   3, 952, 153, 158,   3, 935, 144,   3, 931, 166, 160,\n",
            "          3, 935, 153, 158, 144, 156,   3, 949, 155, 165, 164,   3, 828,  55,\n",
            "        164, 165,   3, 942, 144, 171,   3, 828,  52,  52,  78,  47,   3, 949,\n",
            "        149, 169, 165,   3, 935, 144,   3, 949, 159,   3, 883,  84,  55,  66,\n",
            "         55,   3, 949, 159,   3, 947, 149, 147, 149, 153, 167, 149,   3, 933,\n",
            "        158, 165, 163, 171,   3, 946, 166, 149, 164, 165, 153, 159, 158,  21,\n",
            "        164, 165, 148,   3, 949, 169, 165,   3, 947, 144, 165, 149,  23, 165,\n",
            "         19, 147,  20, 164,   3, 929, 160, 160, 156, 171,   3, 377,  74,  78,\n",
            "         66,  89,  55,  52,  52,  84,  78, 159, 167, 149, 163,  55,  89,  20,\n",
            "        164,   3,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2]), 1)]\n",
            "(tensor([  0, 937, 144, 148,   3, 955, 159, 166, 163,   3, 942, 159, 145, 153,\n",
            "        156, 149,   3, 824,  55,   3, 942, 159, 158, 165, 152, 164,   3, 944,\n",
            "        163,   3, 942, 159, 163, 149, 129,   3, 950,   3, 947,   3, 933, 158,\n",
            "        165, 153, 165, 156, 149, 148,   3, 949, 159,   3, 950, 160, 148, 144,\n",
            "        165, 149,   3, 949, 159,   3, 949, 152, 149,   3, 941, 144, 165, 149,\n",
            "        164, 165,   3, 931, 159, 156, 159, 166, 163,   3, 942, 159, 145, 153,\n",
            "        156, 149, 164,   3, 952, 153, 165, 152,   3, 931, 144, 157, 149, 163,\n",
            "        144,   3, 935, 159, 163,   3, 935, 163, 149, 149,   6,   3, 931, 144,\n",
            "        156, 156,   3, 949, 152, 149,   3, 942, 159, 145, 153, 156, 149,   3,\n",
            "        950, 160, 148, 144, 165, 149,   3, 931, 159,   3, 935, 163, 149, 149,\n",
            "          3, 944, 158,   3, 377,  52,  52,  66,  97,  89,  80,  52,  72,  52,\n",
            "          3,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2]), 1)\n"
          ]
        }
      ],
      "source": [
        "ds = FinancialNewsDataset(df, [\"ham\", \"spam\"])\n",
        "\n",
        "print(len(ds))\n",
        "print(ds[1:3])\n",
        "print(ds[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De7B9ZTLnXlg",
        "outputId": "f5dac302-d3be-4323-ffed-7b9901bbe625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ham': 0, 'spam': 1}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes = ds.classes\n",
        "class_to_idx = ds.class_to_idx\n",
        "idx_to_classes = ds.idx_to_classes\n",
        "\n",
        "class_to_idx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "35eZw080nXlh"
      },
      "source": [
        "## Splitting the Dataset into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZrrm0IrnXlh",
        "outputId": "bee70abc-cf46-4a35-ec83-d234fdcecda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5293 279\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(ds) * 0.95)\n",
        " \n",
        "train_ds = ds[:train_size]\n",
        "test_ds = ds[train_size:]\n",
        "\n",
        "print(len(train_ds), len(test_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3pX1L_klnXlh"
      },
      "source": [
        "## Creating DataLoader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tOccY9panXlh"
      },
      "outputs": [],
      "source": [
        "class Loader:\n",
        "    def __init__(self, ds, batch_size, shuffle):\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self._dsx = [s[0].tolist() for s in ds] # contains the x-values (inputs) of the dataset | `tolist()` converts the x-tensor into Python List | List (List (Int) )\n",
        "        self._dsy = [s[1] for s in ds]          # contains the y-values (targets) of the dataset | List (Int)\n",
        "\n",
        "        if shuffle:\n",
        "            self._temp_dsx = self._dsx.copy() \n",
        "            self._temp_dsy = self._dsy.copy()\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            # Iterating over the number of batches that the dataset is going to bet split\n",
        "            for _ in range(len(self._dsx) // self.batch_size):\n",
        "                \n",
        "                # This random index gives the index of the first sample for the batch\n",
        "                ridx = randint(0, len(self._temp_dsx) - self.batch_size)\n",
        "\n",
        "                yield (torch.tensor(self._temp_dsx[ridx: ridx + self.batch_size], dtype=torch.long), torch.tensor(self._temp_dsy[ridx: ridx + self.batch_size], dtype=torch.long))\n",
        "\n",
        "                # Removing the already `yield`ed batch from the dataset\n",
        "                self._temp_dsx = self._temp_dsx[:ridx] + self._temp_dsx[ridx + self.batch_size:]\n",
        "                self._temp_dsy = self._temp_dsy[:ridx] + self._temp_dsy[ridx + self.batch_size:]\n",
        "\n",
        "            # Returning the last batch, which is not going to contain `batch_size` samples\n",
        "            if len(self._temp_dsx) > 0:\n",
        "                yield (torch.tensor(self._temp_dsx, dtype=torch.long), torch.tensor(self._temp_dsy, dtype=torch.long))\n",
        "\n",
        "            # If we try to iterate again over the loader without those two lines, no samples are going to be returned\n",
        "            self._temp_dsx = self._dsx.copy()\n",
        "            self._temp_dsy = self._dsy.copy()\n",
        "\n",
        "        else:\n",
        "            j = 0\n",
        "            for _ in range(ceil(len(self._dsx) / self.batch_size)):\n",
        "                yield (torch.tensor(self._dsx[j: j + self.batch_size], dtype=torch.long), torch.tensor(self._dsy[j: j + self.batch_size], dtype=torch.long))\n",
        "                j += self.batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return ceil(len(self._dsx) / self.batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "75D8OJKinXli"
      },
      "source": [
        "## Creating the Cross Validation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NAcmygMinXli"
      },
      "outputs": [],
      "source": [
        "def cross_validation(ds, valid_prop, batch_size):\n",
        "    valid_size = int(len(ds) * valid_prop)\n",
        "    ridx = randint(0, len(ds) - valid_size)\n",
        "\n",
        "    return (Loader(ds[ridx: ridx + valid_size], batch_size=batch_size, shuffle=False), Loader(ds[:ridx] + ds[ridx + valid_size:], batch_size=batch_size, shuffle=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt6w5ksvnXlj"
      },
      "source": [
        "## Creating Model's Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Vqs3Y04unXlj"
      },
      "outputs": [],
      "source": [
        "class ModelUtils(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "\n",
        "    def __training_step(self, train_dl, opt, device):\n",
        "        losses = torch.zeros(len(train_dl), device=device)\n",
        "        for i, (x_train, y_train) in enumerate(train_dl):\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "\n",
        "            _, loss = self(x_train, y_train)\n",
        "            losses[i] = loss.item()\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        return losses.mean().item()\n",
        "\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def __validation_step(self, valid_dl, device):\n",
        "        self.eval()\n",
        "        losses = torch.zeros(len(valid_dl), device=device)\n",
        "        for i, (x_train, y_train) in enumerate(valid_dl):\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "\n",
        "            _, loss = self(x_train, y_train)\n",
        "            losses[i] = loss.item()\n",
        "\n",
        "        self.train()\n",
        "        return losses.mean().item()\n",
        "\n",
        "\n",
        "    def fit(self, epochs, train_ds, opt):\n",
        "        start_time = timer()\n",
        "        device = next(self.parameters()).device\n",
        "        train_losses, valid_losses = [], []\n",
        "\n",
        "        t = tqdm(range(1, epochs + 1), desc=\"Training Model: \")\n",
        "        t.set_postfix({\"train_loss\": \"inf\", \"valid_loss\": \"inf\"})\n",
        "        for _ in t:\n",
        "            valid_dl, train_dl = cross_validation(train_ds, valid_prop=0.2, batch_size=32)\n",
        "        \n",
        "            train_loss = self.__training_step(train_dl, opt, device)\n",
        "            valid_loss = self.__validation_step(valid_dl, device)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "            t.set_postfix({\"train_loss\": train_loss, \"valid_loss\": valid_loss})\n",
        "            t.refresh()\n",
        "\n",
        "        return {\"model_train_loss\": train_losses,\n",
        "            \"model_valid_loss\": valid_losses,\n",
        "            \"model_name\": self.__class__.__name__,\n",
        "            \"model_optimizer\": opt.__class__.__name__,\n",
        "            \"model_device\": device.type,\n",
        "            \"model_epochs\": epochs,\n",
        "            \"model_time\": timer() - start_time}\n",
        "\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def evaluate(self, dl):\n",
        "        self.eval()\n",
        "\n",
        "        device = next(self.parameters()).device\n",
        "        metric_collection = MetricCollection([\n",
        "            Accuracy(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            Precision(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            Recall(task=\"multiclass\", num_classes=2, average=\"macro\"),\n",
        "            F1Score(task=\"multiclass\", num_classes=2, average=\"macro\")\n",
        "        ]).to(device)\n",
        "        losses = torch.zeros(len(dl))\n",
        "\n",
        "        for i, (xb, yb) in enumerate(dl):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            logits, loss = self(xb, yb)\n",
        "            preds = F.softmax(logits, dim=-1)\n",
        "\n",
        "            metric_collection.update(preds[:, 0, :], yb)\n",
        "            losses[i] = loss.item()\n",
        "        \n",
        "        res = metric_collection.compute()\n",
        "        \n",
        "        self.train()\n",
        "        return losses.mean().item(), res[\"MulticlassAccuracy\"].item(), res[\"MulticlassPrecision\"].item(), res[\"MulticlassRecall\"].item(), res[\"MulticlassF1Score\"].item()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kX3CTEvpnXlk"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IAYpTIOSnXlk"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, C = x.shape\n",
        "\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * (C**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, embed_size, head_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([SelfAttention(embed_size, head_size, dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(embed_size, embed_size, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Contatenate the outputs of each Masked Self-Attention\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1) # (B, T, EMBED_SIZE)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, scale_embeds, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, scale_embeds * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(scale_embeds * embed_size, embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) # (B, T, EMBED_SIZE)\n",
        "    \n",
        "class Block(nn.Module): # combining Masked Multi-Head Attention and one Feed-Forward layer\n",
        "    def __init__(self, embed_size, scale_embeds, num_heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = embed_size // num_heads # because the result of the Masked Multi-Head layer we want to have shape: (B, T, EMBED_SIZE)\n",
        "        self.multi_att_m = MultiHeadAttention(num_heads, embed_size, head_size, dropout)\n",
        "        self.ffwd = FeedForward(embed_size, scale_embeds, dropout)\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = x + self.multi_att_m(self.ln1(x)) # (B, T, EMBED_SIZE)\n",
        "        x = x + self.ffwd(self.ln2(x))        # (B, T, EMBED_SIZE)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class TransformerEncoder(ModelUtils):\n",
        "    def __init__(self, embed_size, num_layers, scale_embeds, num_heads, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # Self-Attention doesn't take into consideration the position of tokens when computing the attetnion matrix, so we have to\n",
        "        self.position_embedding_table = nn.Embedding(max_len, embed_size)\n",
        "        self.block = nn.Sequential(*[Block(embed_size, scale_embeds, num_heads, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embed_size)\n",
        "        self.linear_head = nn.Linear(embed_size, 2)        \n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        token_embeddings = self.embedding_table(idx)                                              # (B, T, EMBED_SIZE)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(max_len, device=device)) # (T, EMBED_SIZE)\n",
        "\n",
        "        x = token_embeddings + position_embeddings # (B, T, EMBED_SIZE)\n",
        "\n",
        "        x = self.block(x) # (B, T, EMBED_SIZE)\n",
        "        x = self.ln_f(x)  # (B, T, EMBED_SIZE)\n",
        "\n",
        "        logits = self.linear_head(x) # (B, T, 2)\n",
        "\n",
        "        # Condition to seperate training and generating phase\n",
        "        loss = F.cross_entropy(logits[:, 0, :], targets) if targets is not None else None\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecZffMumnXlk",
        "outputId": "7be6de09-a72d-4304-b324-9372102afa4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Trainable\n",
              "=================================================================================================================================================\n",
              "TransformerEncoder                            [1, 912]                  [1, 912, 2]               --                        True\n",
              "├─Embedding: 1-1                              [1, 912]                  [1, 912, 100]             100,400                   True\n",
              "├─Embedding: 1-2                              [912]                     [912, 100]                91,200                    True\n",
              "├─Sequential: 1-3                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    └─Block: 2-1                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    │    └─LayerNorm: 3-1                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─MultiHeadAttention: 3-2           [1, 912, 100]             [1, 912, 100]             40,100                    True\n",
              "│    │    └─LayerNorm: 3-3                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─FeedForward: 3-4                  [1, 912, 100]             [1, 912, 100]             20,200                    True\n",
              "│    └─Block: 2-2                             [1, 912, 100]             [1, 912, 100]             --                        True\n",
              "│    │    └─LayerNorm: 3-5                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─MultiHeadAttention: 3-6           [1, 912, 100]             [1, 912, 100]             40,100                    True\n",
              "│    │    └─LayerNorm: 3-7                    [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "│    │    └─FeedForward: 3-8                  [1, 912, 100]             [1, 912, 100]             20,200                    True\n",
              "├─LayerNorm: 1-4                              [1, 912, 100]             [1, 912, 100]             200                       True\n",
              "├─Linear: 1-5                                 [1, 912, 100]             [1, 912, 2]               202                       True\n",
              "=================================================================================================================================================\n",
              "Total params: 313,402\n",
              "Trainable params: 313,402\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 83.40\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 13.88\n",
              "Params size (MB): 1.25\n",
              "Estimated Total Size (MB): 15.14\n",
              "================================================================================================================================================="
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EMBED_SIZE = 100\n",
        "NUM_LAYERS = 2\n",
        "SCALE_EMBEDS = 1\n",
        "NUM_HEADS = 2\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = TransformerEncoder(EMBED_SIZE, NUM_LAYERS, SCALE_EMBEDS, NUM_HEADS, DROPOUT).to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(1, max_len),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        dtypes=[torch.int64])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWYIOlpnXll"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VyjVEbcnXll",
        "outputId": "cc975ade-745b-4341-9629-1a78aebea2f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Model: 100%|██████████| 5/5 [13:24<00:00, 160.94s/it, train_loss=0.0714, valid_loss=0.0703]\n"
          ]
        }
      ],
      "source": [
        "opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "res = model.fit(EPOCHS, train_ds, opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmMBMs9mnXll",
        "outputId": "973af522-1c75-495e-faef-783c4298f8dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.07389478385448456,\n",
              " 0.9178264141082764,\n",
              " 0.9555172920227051,\n",
              " 0.9178264141082764,\n",
              " 0.9355807304382324)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(Loader(test_ds, batch_size=32, shuffle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNe7QoxoqEJv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
