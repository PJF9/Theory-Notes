{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings with Gensim\n",
        "\n",
        "Spacy, only computes word embeddigns using the GloVe technique. Gensim from the other hand gives us the ability to use any algorithm for creating vector representations.\n",
        "\n",
        "For all the available model gensim provide us look: https://github.com/RaRe-Technologies/gensim-data"
      ],
      "metadata": {
        "id": "uPYM_MpYDNk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5La1W7SsSd4F",
        "outputId": "a433906a-78bc-46cc-95ac-327c6b0eb50d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-09 09:20:49.359063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-09 09:20:51.846698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vsLBSJCC9hx-"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import gensim.downloader as api             # For downloading an existing model to compute word embeddings\n",
        "from gensim.utils import simple_preprocess  # For tokenize a document into words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the NLP object"
      ],
      "metadata": {
        "id": "JJqv91mDSjPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2ioIBLiSutL",
        "outputId": "df586ced-531a-4af4-d19a-a49382439689"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Embedding object"
      ],
      "metadata": {
        "id": "C31lJfrxSz7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "V-UB0ERzTAxo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a `Word2Vec` Object"
      ],
      "metadata": {
        "id": "JCd63jf-EsBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "LbCqMlCDEqMH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is being trained over google news and has an embedding size of 300"
      ],
      "metadata": {
        "id": "rqDsZZLyE3VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity of Vector Representation\n",
        "\n",
        "By `smimilarity` we don't mean how relevant is their semantinc information, rather how often, the frequency of which those vectors appear in a similar context context, at least for Word2Vec and GloVe, because they ae window based approaches.\n",
        "\n",
        "An example of two words that appear is a similar context is `good` and `bad`. Consider the following example:\n",
        "* I was feeling _good_ as it was holiday.\n",
        "* I was feeling _bad_ as it was Monday.\n",
        "Another example is `cat` and `dog`."
      ],
      "metadata": {
        "id": "i5YQ1PxcGAfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can get the similarity of two elements using:\n",
        "wv.similarity(w1=\"good\", w2=\"bad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4sdQub_GY5R",
        "outputId": "1020287d-aa7a-4e4e-92be-60399e8f433a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7190051"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also ge a list of the `closest` words of a given word\n",
        "wv.most_similar(\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG5ttFScHCmU",
        "outputId": "03103331-6eb2-4eb4-cdbe-01896d9311ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great', 0.7291510105133057),\n",
              " ('bad', 0.7190051078796387),\n",
              " ('terrific', 0.6889115571975708),\n",
              " ('decent', 0.6837348341941833),\n",
              " ('nice', 0.6836092472076416),\n",
              " ('excellent', 0.644292950630188),\n",
              " ('fantastic', 0.6407778263092041),\n",
              " ('better', 0.6120728850364685),\n",
              " ('solid', 0.5806034803390503),\n",
              " ('lousy', 0.576420247554779)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arithmetic using Vector Embeddings"
      ],
      "metadata": {
        "id": "lnjbLFMdIp3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# France - Paris + Berlin = Germany\n",
        "\n",
        "wv.most_similar(positive=[\"France\", \"Berlin\"], negative=[\"Paris\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWn2osPYHM7c",
        "outputId": "2949fb9d-b3bb-4523-9b62-f4bd826808da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Germany', 0.7901254892349243),\n",
              " ('Austria', 0.6026812195777893),\n",
              " ('German', 0.6004959940910339),\n",
              " ('Germans', 0.5851002931594849),\n",
              " ('Poland', 0.5847075581550598),\n",
              " ('Hungary', 0.5271855592727661),\n",
              " ('BBC_Tristana_Moore', 0.5249711275100708),\n",
              " ('symbol_RSTI', 0.5245768427848816),\n",
              " ('Belgium', 0.5221248269081116),\n",
              " ('Germnay', 0.5199405550956726)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# King - man + woman = Queen\n",
        "\n",
        "wv.most_similar(positive=[\"King\", \"woman\"], negative=[\"man\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfsrFp19JKBM",
        "outputId": "afd69923-11a2-4cbf-acbe-525e4978ca31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Queen', 0.5515626668930054),\n",
              " ('Oprah_BFF_Gayle', 0.47597548365592957),\n",
              " ('Geoffrey_Rush_Exit', 0.46460166573524475),\n",
              " ('Princess', 0.4533674716949463),\n",
              " ('Yvonne_Stickney', 0.4507041573524475),\n",
              " ('L._Bonauto', 0.4422135353088379),\n",
              " ('gal_pal_Gayle', 0.4408389925956726),\n",
              " ('Alveda_C.', 0.4402790665626526),\n",
              " ('Tupou_V.', 0.4373864233493805),\n",
              " ('K._Letourneau', 0.4351031482219696)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching with the Context\n",
        "\n",
        "Another functionality of gensim is that given a list of words it can tell us which one doesn't match with the other.\n",
        "\n",
        "Actually it returns the word that is farthest from the others using cosine similarity."
      ],
      "metadata": {
        "id": "_Lj8P0R0Jlyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv.doesnt_match([\"google\", \"apple\", \"dog\", \"twitter\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gSbXXS9YJd5b",
        "outputId": "9695e170-3a3c-4e91-a8fa-a3ccfbea06a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.doesnt_match([\"good\", \"great\", \"bad\", \"terrific\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j-4feRI8KiQg",
        "outputId": "200004d0-c056-4409-af3b-999e71defefa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Embedding"
      ],
      "metadata": {
        "id": "ic2bhaaoMoK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv[\"great\"].shape)\n",
        "\n",
        "wv[\"great\"][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs5QQOeoMp-r",
        "outputId": "fb911bdd-0a32-4004-ba51-7e67bd7287ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07177734,  0.20800781, -0.02844238,  0.17871094,  0.1328125 ,\n",
              "       -0.09960938,  0.09619141, -0.11669922, -0.00854492,  0.1484375 ,\n",
              "       -0.03344727, -0.18554688,  0.04101562, -0.08984375,  0.02172852,\n",
              "        0.06933594,  0.18066406,  0.22265625, -0.10058594, -0.06933594],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Representation of Sentences\n",
        "\n",
        "By default `gensim` has not created a way for automaticly get the embeddings of a sentence just like `spacy`.\n",
        "\n",
        "To create those embeddings we are taking the `mean` of all the tokens of the sentence."
      ],
      "metadata": {
        "id": "ArCCApebOFli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = nlp(\"Thor's finding a new job as we got fired from his previous one\")\n",
        "\n",
        "sentence_v = [wv.get_mean_vector(token.lemma_) for token in sentence if (not token.is_stop) and (not token.is_punct)]"
      ],
      "metadata": {
        "id": "2qR7ZleGOJaF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentence_v))\n",
        "print(len(sentence_v[0]))\n",
        "print(sentence_v[0][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSeoMmkNUEOd",
        "outputId": "56f51b53-af13-4766-edb4-98bce86b8560"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "300\n",
            "[-0.1020783   0.07230248 -0.00247384  0.02924935 -0.03211807 -0.00690309\n",
            " -0.06591261 -0.04352893 -0.03119576  0.01347279 -0.02085569 -0.02568291\n",
            " -0.08093277 -0.0019637  -0.07012518  0.04586982 -0.00617665  0.07006936\n",
            " -0.01070635 -0.02028628]\n"
          ]
        }
      ]
    }
  ]
}