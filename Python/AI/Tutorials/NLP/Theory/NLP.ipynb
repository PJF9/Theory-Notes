{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Goal\n",
        "\n",
        "The main goal of `Natural Language Processing` (NLP) is to **teach** an AI Model the human language (linguistics), meaning be able to understanding the human language (either via text or speech). \n",
        "\n"
      ],
      "metadata": {
        "id": "p5luxkL0ZTNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phases of NLP\n",
        "\n",
        "There are two main phases to NLP (text-approach)\n",
        "1. `Data Preprocessing`:  Involves _preparing_ and _cleaning_ the data for the Model to be able to analyze it. There are several ways of doing it, including:\n",
        "    * **Tokenization**: Text is broken down into smaller units to work with.\n",
        "    * **Stop word removal**: Common words are removed from text so unique words that offer the most information about the text remain.\n",
        "    * **Lemmatization and stemming**: Words are reduced to their root forms to process.\n",
        "    * **Part-of-speech tagging**: Words are marked based on the part-of speech they are -- such as nouns, verbs and adjectives.\n",
        "\n",
        "2. `Processing Algorithm`: The algorithm that is used so the Model can process and understand the data. There are a lot of approaches, but all lies in the following categories:\n",
        "    * **Rules-based system**: This system uses carefully designed linguistic rules to process the data.\n",
        "    * **Machine learning-based system**: They learn to perform tasks based on training data they are fed, and adjust their methods as more data is processed. Using Deep Learning those NLP algorithms create their own lingustic rules to process the data."
      ],
      "metadata": {
        "id": "rGUEqjSNbCHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Techniques of NLP\n",
        "\n",
        "`Syntax` and `Semantic` analysis are two main techniques used in Natural Language Processing.\n",
        "\n",
        "-\n",
        "\n",
        "`Syntax` is the arrangement of words in a sentence to make grammatical sense. NLP uses syntax to assess meaning from a language based on grammatical rules. Some `Syntax` techniques are:\n",
        "\n",
        "* **Parsing**: The grammatical analysis of a sentence. (Example: A natural language processing algorithm is fed the sentence, \"The dog barked.\" Parsing involves breaking this sentence into parts of speech -- i.e., dog = noun, barked = verb)\n",
        "\n",
        "* **Word Segmentation**: This is the act of taking a string of text and deriving word forms from it. (Example: A person scans a handwritten document into a computer. The algorithm would be able to analyze the page and recognize that the words are divided by white spaces.)\n",
        "\n",
        "* **Sentence Breaking**: This places sentence boundaries in large texts. (Example: A natural language processing algorithm is fed the text, \"The dog barked. I woke up.\" The algorithm can recognize the period that splits up the sentences using sentence breaking.)\n",
        "\n",
        "* **Morphological Segmentation**: This divides words into smaller parts called morphemes. (Example: The word untestably would be broken into [[un[[test]able]]ly], where the algorithm recognizes \"un,\" \"test,\" \"able\" and \"ly\" as morphemes.)This is especially useful in machine translation and speech recognition.\n",
        "\n",
        "* **Stemming**: This divides words with inflection in them to root forms. (Example: In the sentence, \"The dog barked,\" the algorithm would be able to recognize the root of the word \"barked\" is \"bark.)\n",
        "\n",
        "-\n",
        "\n",
        "`Semantics` involves understanding the meaning behind words and the structure a of sentence. Some `Semantic` techniques are:\n",
        "\n",
        "* **Word Sense Disambiguation**: This derives the meaning of a word based on context. (Example: Consider the sentence, \"The pig is in the pen.\" The word pen has different meanings.)\n",
        "\n",
        "* **Named Entity Recognition**: This determines words that can be categorized into groups. (Example: An algorithm using this method could analyze a news article and identify all mentions of a certain company or product. Using the semantics of the text, it would be able to differentiate between entities that are visually the same. For instance, in the sentence, \"Daniel McDonald's son went to McDonald's and ordered a Happy Meal,\" the algorithm could recognize the two instances of \"McDonald's\" as two separate entities -- one a restaurant and one a person.)\n",
        "\n",
        "* **Natural Language Generation**: This uses a database to determine semantics behind words and generate new text. (Example: An algorithm could automatically write a summary of findings from a business intelligence platform, mapping certain words and phrases to features of the data in the BI platform.)\n"
      ],
      "metadata": {
        "id": "vGLCLJGogASm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP used for?\n",
        "\n",
        "1. `Text Classification`: This involves assigning tags to texts to put them in categories. (This can be useful for **sentiment** analysis, which helps the natural language processing algorithm determine the sentiment, or emotion behind a text.)\n",
        "\n",
        "2. `Word Classifying`: This involves identifying the **grammatical components** of a sentence (noun, verb, adjective), or the **named entities** (person, location, organization).\n",
        "\n",
        "3. `Text Extraction`: This involves automatically summarizing text and finding important pieces of data. (One example of this is **keyword extraction**, which pulls the most important words from the text, which can be useful for search engine optimization.)\n",
        "\n",
        "4. `Machine Translation`: This is the process by which a computer translates text from one language, to another language, without human intervention.\n",
        "\n",
        "5. `Natural Language Generation`: This involves using NLP algorithms to analyze unstructured data and automatically produce content based on that data."
      ],
      "metadata": {
        "id": "hY0ibwURuP-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges of NLP\n",
        "\n",
        "There are a number of challenges most of them boil down to the fact that natural language is ambiguous and changing every day. Some challenges are:\n",
        "\n",
        "* `Precision`: Human speech, is not always precise; it is often ambiguous and the linguistic structure can depend on many complex variables, including slang, regional dialects and social context.\n",
        "\n",
        "* `Tone of Voice and Inflection`: A sentence can change meaning depending on which word or syllable the speaker puts stress on. NLP algorithms may miss the subtle, but important, tone changes in a person's voice when performing speech recognition. The tone and inflection of speech may also vary between different accents, which can be challenging for an algorithm to parse.\n",
        "\n",
        "* `Evolving use of Language`: NLP is also challenged by the fact that language -- and the way people use it -- is continually changing.\n",
        "\n",
        "* `Amount of Data`: NLP Models to be trained with high accuracy and precision need a huge amound of labeled data, that is some situations is a time-consuming process."
      ],
      "metadata": {
        "id": "y5ugjX_4xEFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Pipeline\n",
        "\n",
        "`NLP Pipeline` are the steps we need to follow in order to create a model and solve a downstream task. Those steps are:\n",
        "1. Data Acquisition.\n",
        "2. Text Extraction and Cleanup.\n",
        "3. Preprocessing.\n",
        "4. Feature Engineering.\n",
        "5. Model Building.\n",
        "6. Evaluate.\n",
        "7. Deployment.\n",
        "8. Monitor and Update.\n",
        "\n",
        "-\n",
        "\n",
        "Let's see an example to better showcase the Pipeline: Let's say we want to solve the task of classifying if a problem (error that occur in a platform) has high, medium or even low priority, base on its title.\n",
        "* To start we need to understand how this repors are stored in memory. Most times, at least for bigger websites/services, those problems/reports are saved in a `database`. The **json** representation of those entries might look like:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"title\": \" ... \",\n",
        "    \"description\": \" ... \",\n",
        "    \"creator\": \" ... \",\n",
        "    \"create_time\": \"../../.... ..:.. AM/PM TZ,\n",
        "    \"severity\": \"Hight\" / \"Medium\" / \"Low\"\n",
        "}\n",
        "```\n",
        "\n",
        "* In order to not affect the database, all those records are moved to the cloud (like Amzon S3), so the AI team can take the data and create the model. This process is called `Data Acquisition`. There are other ways to also acquire data, such us use public datasets, scrapying or even using data augmentation.\n",
        "\n",
        "* The next step is to Discard Irrelevant Information. In our case we could discard the \"creator\" and \"create_time\" fields and also concatinate the \"title\" and the \"descritpin\". Then we can also perform some spell-correcting and also remove escape characters, like \\n, etc. Those processes form the second step of the **Pipeline**, `Text Extraction and Cleanup`. So now each record might look like:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"text\": \" ... \",\n",
        "    \"severity\": \"Hight\" / \"Medium\" / \"Low\"\n",
        "}\n",
        "```\n",
        "\n",
        "* After that is time to modify the \"text\". First we could split it in sentences (Sentence Segmentation / `Sentence Tokenization`). This task might look easy at first by consider this text: \"Dr. Strange ordered samosas, ravioli, etc. for his lauch.\". Next its time to split those sentences into words (`Word Tokenization`).\n",
        "\n",
        "* Now for those words we want to convert each word to its root. For example the word \"loves\" would be converted into \"love\" and the word \"eating\" to \"eat\". This process is called `Stemming`. By only performing stemming words like \"ate\" and \"written\" would not be converted into \"eat\" and \"write\" respectively. We need `Lemmatization` for that.\n",
        "\n",
        "* Tokenization, stemming and lematization are all combined into the category of `Preprocessing`.\n",
        "\n",
        "* Then we need a way to convert those words into vector representation in order to pass them to the model. This numerical representation can't be random, we need similar words, like \"good\" and \"great\" to have similar (or even the same) representation. This process is called `Feature Engineering`.\n",
        "\n",
        "* After that we could start to build the model to slove the specific task (`Model Building`). Then we `Evaluate` the model and if necessarily return back to preprocessing, if the results we are getting are not the ones we anticipate.\n",
        "\n",
        "* Finally it's time to deploy the model in the cloud in order to build an API to use it from code with a request.get. (`Deployment`)\n",
        "\n",
        "* In order to contantly increasing the evaluation of the model we need to `Monitor and Update` the model by generating more example, training for more epochs. \n"
      ],
      "metadata": {
        "id": "nw_zeut1RV0T"
      }
    }
  ]
}