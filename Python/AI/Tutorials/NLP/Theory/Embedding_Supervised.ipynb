{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer (Pytorch)\n",
        "\n",
        "The `nn.Embedding` layer is a simple lookup table that maps an index value to a weight matrix of a certain dimension. This dimension is `(vocab_size, vector_size)`, where **vacab_size** is the size of the vocabulary and **vector_size** the size of the embedding.\n",
        "\n",
        "Each row represents a single _word embedding_ that is initialized randomly drawn from a standard normal distribution.\n",
        "\n",
        "-\n",
        "\n",
        "If we don't want to train our embeddings during model training (when we are using pre-trained embeddings), we can use:\n",
        "\n",
        "    `emb_layer.weight.requires_grad = False`\n",
        "\n",
        "-\n",
        "\n",
        "\n",
        "Lastly to load weight to our layer form a pre-trained embedding model we can use\n",
        "\n",
        "    `weight = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])`\n",
        "\n",
        "    `embedding = nn.Embedding.from_pretrained(weight)`\n",
        "\n",
        "For more information:\n",
        "https://towardsdatascience.com/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16"
      ],
      "metadata": {
        "id": "NLS_WK0TUdQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Example"
      ],
      "metadata": {
        "id": "x8OH_rhbjgF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyEU8fsZwsnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af1ec45-3cf8-4477-e878-41abdabfadcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.2067,  1.3697,  1.0623, -2.3392])\n",
            "tensor([0.9531, 0.0397, 0.3057, 1.3423])\n",
            "tensor([-0.1981,  0.1677, -0.4753, -1.6036])\n",
            "tensor([-1.1397, -0.1794,  0.3362,  0.5088])\n",
            "tensor([-0.5705,  1.4132, -0.0266,  0.6365])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "vocab_size = 30\n",
        "embed_size = 4\n",
        "\n",
        "# Creating the embedding layer\n",
        "embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "# Printing the first 5 word embeddings\n",
        "with torch.inference_mode():\n",
        "    for i in range(5):\n",
        "        print(embedding(torch.LongTensor([i])).squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning the Embedding using a Sentiment task"
      ],
      "metadata": {
        "id": "OeedPtpFkAtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn.functional import one_hot, pad"
      ],
      "metadata": {
        "id": "DCfUUvDoj2yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting the Data"
      ],
      "metadata": {
        "id": "bFUblsFUkNrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\"nice food\", \n",
        "           \"amazing restaurant\",\n",
        "           \"too good\",\n",
        "           \"just loved it\",\n",
        "           \"will go again\",\n",
        "           \"nice service\",\n",
        "           \"great chef\",\n",
        "           \"value for money\",\n",
        "           \"horrible food\",\n",
        "           \"never go again\",\n",
        "           \"terrible menu\",\n",
        "           \"poor quality\",\n",
        "           \"awfull service\",\n",
        "           \"needs imporvment\",\n",
        "           \"poor service\",\n",
        "           \"too expensive\"]\n",
        "\n",
        "sentiment = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "7IZpJ9grj8D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the vocabualry\n",
        "vocab = list(set(word for rev in reviews for word in rev.split()))\n",
        "vocab_idx = {vocab[i-1]: i for i in range(1, len(vocab) + 1)}\n",
        "\n",
        "print(len(vocab))\n",
        "print(vocab_idx)"
      ],
      "metadata": {
        "id": "8jRzqX4cluPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd377f7b-51ef-457f-fa48-76a31443eba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n",
            "{'loved': 1, 'restaurant': 2, 'great': 3, 'it': 4, 'poor': 5, 'never': 6, 'service': 7, 'amazing': 8, 'money': 9, 'terrible': 10, 'too': 11, 'good': 12, 'awfull': 13, 'menu': 14, 'quality': 15, 'expensive': 16, 'nice': 17, 'imporvment': 18, 'for': 19, 'go': 20, 'food': 21, 'again': 22, 'value': 23, 'horrible': 24, 'needs': 25, 'just': 26, 'will': 27, 'chef': 28}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the one-hot encodings of the vocabulary\n",
        "one_hot_encodings = one_hot(torch.arange(0, len(vocab)))\n",
        "\n",
        "print(one_hot_encodings[:3])"
      ],
      "metadata": {
        "id": "Ol73XLumrj0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d97b0c-1ea7-4e83-efbe-c2913d97e21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the one-hot encoding of the word \"chef\"\n",
        "print(one_hot_encodings[vocab_idx[\"chef\"] - 1])"
      ],
      "metadata": {
        "id": "LYDI5oxWsec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7309ceb5-73eb-4e4a-92ff-72f360e7e286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have encoded the vocabulary, now we will encode the `reviews`\n",
        "encoded_reviews = [[vocab_idx[word] for word in rev.split()] for rev in reviews]\n",
        "\n",
        "# Adding padding to the encoded reviews\n",
        "max_rev = max(len(rev.split()) for rev in reviews)\n",
        "padded_encoded_reviews = [enc_rev + [0 for _ in range(3 - len(enc_rev))] for enc_rev in encoded_reviews]\n",
        "\n",
        "padded_encoded_reviews"
      ],
      "metadata": {
        "id": "kwkoluiSs5Ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d6a88a-c163-4975-9e08-fb8125189141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[17, 21, 0],\n",
              " [8, 2, 0],\n",
              " [11, 12, 0],\n",
              " [26, 1, 4],\n",
              " [27, 20, 22],\n",
              " [17, 7, 0],\n",
              " [3, 28, 0],\n",
              " [23, 19, 9],\n",
              " [24, 21, 0],\n",
              " [6, 20, 22],\n",
              " [10, 14, 0],\n",
              " [5, 15, 0],\n",
              " [13, 7, 0],\n",
              " [25, 18, 0],\n",
              " [5, 7, 0],\n",
              " [11, 16, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pytorch's function `pad`\n",
        "max_rev = max(len(rev.split()) for rev in reviews)\n",
        "for i in range(len(encoded_reviews)):\n",
        "    if len(encoded_reviews[i]) < 3:\n",
        "        encoded_reviews[i] = pad(input=torch.tensor(encoded_reviews[i]), pad=(0, max_rev-len(encoded_reviews[i])), mode=\"constant\", value=0).tolist()\n",
        "\n",
        "encoded_reviews"
      ],
      "metadata": {
        "id": "SJeIypQXx20E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63012e82-0ae3-4be3-85de-0cd9def8e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[17, 21, 0],\n",
              " [8, 2, 0],\n",
              " [11, 12, 0],\n",
              " [26, 1, 4],\n",
              " [27, 20, 22],\n",
              " [17, 7, 0],\n",
              " [3, 28, 0],\n",
              " [23, 19, 9],\n",
              " [24, 21, 0],\n",
              " [6, 20, 22],\n",
              " [10, 14, 0],\n",
              " [5, 15, 0],\n",
              " [13, 7, 0],\n",
              " [25, 18, 0],\n",
              " [5, 7, 0],\n",
              " [11, 16, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Model"
      ],
      "metadata": {
        "id": "lQrQsOAG1z6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_rev_size=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(start_dim=0),\n",
        "            nn.Linear(in_features=embed_size*max_rev_size, out_features=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Where x are the `padded_encoding_reviews`\n",
        "        return self.classifier(self.embedding_layer(torch.LongTensor(x)))"
      ],
      "metadata": {
        "id": "MnAWq_lO02Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Model"
      ],
      "metadata": {
        "id": "dSTlQmMr3GPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Embedding_Model(vocab_size, embed_size)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "BZnHnfHz3DRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151c5c2d-3d60-4a26-cae2-239698a2800d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding_Model(\n",
            "  (embedding_layer): Embedding(30, 4)\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=0, end_dim=-1)\n",
            "    (1): Linear(in_features=12, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(encoded_reviews[0])\n",
        "logits"
      ],
      "metadata": {
        "id": "QwvkdXDK4a3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfe5130-bdcd-4861-fc85-1bdf81ce0fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1841], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Training Loop"
      ],
      "metadata": {
        "id": "Zpe-xi4f3Peh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(epochs, model, encoded_reviews, sentiment, loss_fn, accuracy_fn, opt):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        set_loss, set_acc = 0, 0\n",
        "\n",
        "        for i in range(len(encoded_reviews)):\n",
        "            logit = model(encoded_reviews[i])\n",
        "            loss = loss_fn(logit.squeeze(), sentiment[i])\n",
        "\n",
        "            set_loss += loss.item()\n",
        "            set_acc += accuracy_fn(logit.squeeze(), sentiment[i])\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        set_loss /= len(encoded_reviews)\n",
        "        set_acc /= len(encoded_reviews)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch} | Loss: {set_loss} | Acc(%): {set_acc}\")"
      ],
      "metadata": {
        "id": "iRcgmS1N3OeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Accuracy Metric"
      ],
      "metadata": {
        "id": "CM3Ouzw24JYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(logits, sentiment):\n",
        "    pred = logits.sigmoid().round()\n",
        "\n",
        "    return (pred == sentiment).item()"
      ],
      "metadata": {
        "id": "CmXo6ckj4IxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "CwhG8CFh9aiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "opt = optim.Adam(params=model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "FH5yMeXG9aD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(100, model, encoded_reviews, torch.tensor(sentiment, dtype=torch.float32), loss_fn, accuracy_fn, opt)"
      ],
      "metadata": {
        "id": "vUi2SBVY9unQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995dfc6e-600c-4315-ecd6-54da562b71af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Loss: 0.6899639777839184 | Acc(%): 0.5625\n",
            "Epoch: 20 | Loss: 0.6037122216075659 | Acc(%): 0.6875\n",
            "Epoch: 30 | Loss: 0.5316259227693081 | Acc(%): 0.875\n",
            "Epoch: 40 | Loss: 0.46818832680583 | Acc(%): 1.0\n",
            "Epoch: 50 | Loss: 0.41084972862154245 | Acc(%): 1.0\n",
            "Epoch: 60 | Loss: 0.35870243329554796 | Acc(%): 1.0\n",
            "Epoch: 70 | Loss: 0.3116523493081331 | Acc(%): 1.0\n",
            "Epoch: 80 | Loss: 0.2697701081633568 | Acc(%): 1.0\n",
            "Epoch: 90 | Loss: 0.23296113777905703 | Acc(%): 1.0\n",
            "Epoch: 100 | Loss: 0.20091756968759 | Acc(%): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Word Embeddings"
      ],
      "metadata": {
        "id": "d9a9rcFSCcYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = model.embedding_layer.weight\n",
        "word_embeddings.requires_grad = False\n",
        "\n",
        "print(word_embeddings[:5])"
      ],
      "metadata": {
        "id": "vi2AOnaU_3bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2368280a-9bec-4cf0-cc8c-81ceb63453d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.7385,  0.3555,  0.9445,  0.5888],\n",
            "        [-1.9104,  0.9121, -1.6049,  0.9643],\n",
            "        [-0.8481,  0.3568, -1.6062, -0.7131],\n",
            "        [ 0.6051, -0.3211, -1.4134, -0.4751],\n",
            "        [ 0.6058, -0.3648,  1.6964,  0.5499]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the word embedding of 2 similar words like \"nice\" and \"amazing\"\n",
        "\n",
        "print(word_embeddings[vocab_idx[\"nice\"] - 1])\n",
        "print(word_embeddings[vocab_idx[\"amazing\"] - 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1hxNidGDg1i",
        "outputId": "54fdacf6-4724-4ac8-901b-9ab8df0cc1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4986, -0.8899,  1.6748,  0.2998])\n",
            "tensor([ 0.4989,  0.8725,  2.4576, -0.6257])\n"
          ]
        }
      ]
    }
  ]
}