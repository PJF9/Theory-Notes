{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Word Vectors with Spacy\n",
        "\n",
        "Spacy provide us with packages that have pre-computed some `word embeddings` that we can use. For example one such package is **en_core_web_lg**.\n",
        "\n",
        "To compute those embeddings Space is using `GloVe` embedding technique.\n",
        "\n",
        "**Note**: Spacy is not only providing us with word embeddings over a document, but also computes the `embedding` for the entire `document` and also for a `sub-document` originate from the original document. It can provide that functionality by simple taking the average vector from all words vectors appear in the (sub)document."
      ],
      "metadata": {
        "id": "dWDqZDYDokqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS1MhDSAYQvF",
        "outputId": "3fb2bbad-ce84-4223-9453-547f59969b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-09 08:27:12.064513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-09 08:27:15.769038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "DkvYSxU4nzXh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initalizing the NLP Object"
      ],
      "metadata": {
        "id": "YHDmdpReoK8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "xz_7ItUGoGim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can see If a token has word embeddings pre-computed in Spacy or If it is OOV"
      ],
      "metadata": {
        "id": "qpJviNpQoc8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"orange, apple, dog cat and banana with asdasfag\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \" | \", token.has_vector, \" | \", token.is_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY1MrdV_oNRO",
        "outputId": "49cb5bf1-dce4-43cf-8ca1-aea8dcd9811d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orange  |  True  |  False\n",
            ",  |  True  |  False\n",
            "apple  |  True  |  False\n",
            ",  |  True  |  False\n",
            "dog  |  True  |  False\n",
            "cat  |  True  |  False\n",
            "and  |  True  |  False\n",
            "banana  |  True  |  False\n",
            "with  |  True  |  False\n",
            "asdasfag  |  False  |  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Word Embedding Vectors"
      ],
      "metadata": {
        "id": "c9hHC-lapb1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we want to get the word vector of the token banana\n",
        "banana = doc[3]\n",
        "\n",
        "print(banana.vector.shape)\n",
        "banana.vector[:36]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hejZRkpBpAcl",
        "outputId": "8c3c34d4-335c-4861-b8d6-d83fe8ee2097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.3899 , -4.7034 , -0.56101,  1.2291 ,  4.3298 , -1.0775 ,\n",
              "       -1.3006 ,  8.7939 , -0.16669, -4.3738 ,  2.3697 ,  2.6438 ,\n",
              "       -5.4589 ,  3.3491 ,  4.0331 ,  5.1368 , -3.0016 ,  4.3627 ,\n",
              "       -3.1921 , -4.6624 ,  6.065  ,  1.0278 , -2.302  ,  2.6546 ,\n",
              "       -1.9866 , -0.21586, -4.6756 , -4.2126 ,  4.552  ,  0.77829,\n",
              "       -2.3145 , -5.2688 , -0.83724,  1.5414 , -3.5657 , -2.157  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[:3].vector.shape)\n",
        "doc[:3].vector[:36]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwE3fsB-rmP4",
        "outputId": "d789a0df-fd91-43cb-e310-70e8ca03050b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.4707    , -2.3475866 , -1.41162   ,  2.8259335 ,  1.18327   ,\n",
              "       -3.123     , -2.2702668 ,  5.885967  , -2.48483   , -0.41277325,\n",
              "        4.2376    , -0.5593333 , -3.8496    ,  2.0623567 ,  1.6014701 ,\n",
              "        0.80606   , -0.5474667 ,  0.34885335,  0.56983334, -3.9805    ,\n",
              "        1.1141567 ,  2.6033332 , -2.3343668 , -0.87796664, -1.6004766 ,\n",
              "       -2.0258865 , -3.4727333 , -1.1013669 ,  1.8469567 ,  1.14383   ,\n",
              "       -1.56239   , -2.29303   , -1.4446467 , -1.1601    , -0.01396668,\n",
              "       -1.768     ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing those Words Vectors"
      ],
      "metadata": {
        "id": "w4EEORgTqeKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a token embedding for the word `apple`\n",
        "base_token = nlp(\"apple\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.has_vector:\n",
        "        print(f\"{token.text} <-> {base_token.text}: {token.similarity(base_token)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwBLuAbaqh2Q",
        "outputId": "f1db0627-7668-46c6-9af2-01b4850b7ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orange <-> apple: 0.6135187212109956\n",
            ", <-> apple: 0.12710916405242928\n",
            "apple <-> apple: 1.0\n",
            ", <-> apple: 0.12710916405242928\n",
            "dog <-> apple: 0.22881005140483499\n",
            "cat <-> apple: 0.20368060357742446\n",
            "and <-> apple: 0.13160242794047908\n",
            "banana <-> apple: 0.6646700599790674\n",
            "with <-> apple: 0.12418893150611857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because those embeddings are being claculated suing GloVe, this similarity has to do with how many times those two words are beaing captured close to each other in a large corpus."
      ],
      "metadata": {
        "id": "acf7VWT2swru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arithmetic using those Vectors"
      ],
      "metadata": {
        "id": "rt8BkLQMtaNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "king_emb = nlp(\"king\").vector\n",
        "queen_emb = nlp(\"queen\").vector\n",
        "man_emb = nlp(\"man\").vector\n",
        "woman_emb = nlp(\"woman\").vector\n",
        "\n",
        "result = king_emb - man_emb + woman_emb\n",
        "\n",
        "print(f\"Similarity of result and queen:{cosine_similarity([result], [queen_emb])[0][0] * 100: .2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa0mSEkitPm-",
        "outputId": "d5ab9907-84e5-4391-b13c-128bc474b95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity of result and queen: 61.78%\n"
          ]
        }
      ]
    }
  ]
}