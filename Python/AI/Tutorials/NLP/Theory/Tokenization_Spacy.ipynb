{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ4jKXJ4mrIs"
      },
      "source": [
        "## What is Tokenization\n",
        "\n",
        "`Tokenization` is the process of splitting the text into meaningful segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4HEI1Tkl7T5",
        "outputId": "d003bb96-75d0-4390-e612-424e333913a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-04-05 08:51:46.668745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-05 08:51:48.469379: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "axlFYnP9nNuz"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-sq8kLnCneMS"
      },
      "source": [
        "## Initalizing the NLP Object\n",
        "\n",
        "\n",
        "As we've discussed Space is an `object-oriented` library, so in order to have access to its methods and classes we need to initialize an object. This object is called **nlp object**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOcFoLP7nPfg",
        "outputId": "bcd09b74-c78e-44fe-94be-f213b6d5d75f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creates an empty Pipline for the english language (another option is `load()` which created a pre-trained pipline)\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "type(nlp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jZQfcZOQoTdH"
      },
      "source": [
        "To see all the language models: https://spacy.io/usage/models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGmGRTi9otaH"
      },
      "source": [
        "## Creating a Document Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hmE43I5ouc3",
        "outputId": "380cb73d-70c1-4ba6-804b-5940c18e8639"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
        "\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFVi_O5RpFCt",
        "outputId": "dd337b16-25c6-4bf6-cafc-7a3a85c8e672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr.\n",
            "Strange\n",
            "loves\n",
            "pav\n",
            "bhaji\n",
            "of\n",
            "mumbai\n",
            "as\n",
            "it\n",
            "costs\n",
            "only\n",
            "2\n",
            "$\n",
            "per\n",
            "plate\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# By default, the above line of code also performs tokenization\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDPBWJ4Ipjrq",
        "outputId": "52e0c77c-ff6b-4ac6-a6be-901e124ad42c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dr."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We have also access to those tokens using indexing\n",
        "doc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4vBeHCxr71p",
        "outputId": "37b6d6b4-8ac0-4fb9-c088-f77915ad6d0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THrjbdzisATw",
        "outputId": "f0bacca0-9873-4330-9471-208566781816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr. Strange loves\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also use slicing\n",
        "span = doc[0:3]\n",
        "\n",
        "print(span)\n",
        "type(span)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N9hsB8oRqVmW"
      },
      "source": [
        "## How Spacy Tokenizes the Document?\n",
        "\n",
        "Imaging the sentence: `\"Let's go to N.Y.!\"`\n",
        "1. Splitting the sentence on the spaces:\n",
        "```\n",
        "[\"let's, go, to, N.Y.!\"]\n",
        "```\n",
        "2. Splitting the words on the prefix (where it can be \", (, etc.): \n",
        "```\n",
        "[\", Let's, go, to, N.Y.!\"]\n",
        "```\n",
        "3. Splitting on the excetpions:\n",
        "```\n",
        "[\", Let, 's, go, to, N.Y.!\"]\n",
        "```\n",
        "4. Splitting on the suffix:\n",
        "```\n",
        "[\", Let, 's, go, to, N.Y.!, \"]\n",
        "```\n",
        "5. Again on exceptions:\n",
        "```\n",
        "[\", Let, 's, go, to, N.Y., !, \"]\n",
        "```\n",
        "\n",
        "And we are done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtVWmokjsVbY",
        "outputId": "b483e858-b355-4823-e5e9-a6cc3a3d76e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can see all the methods that are available of the tokens\n",
        "dir(doc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1jHqOJusiy2",
        "outputId": "0eafdb42-fc51-4fbd-d36b-24e69019be22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Dr.', str)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting a token into text\n",
        "text = doc[0].text\n",
        "\n",
        "text, type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhCYQO68stqW",
        "outputId": "c382e5ea-3688-48ff-8272-379714257c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "two\n",
            "True\n",
            "$\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Two powerfull attributes are `like_num`, `is_currency`\n",
        "doc = nlp(\"Dr. Strange borrows from Tony two $\")\n",
        "\n",
        "print(doc[-2])\n",
        "print(doc[-2].like_num)\n",
        "\n",
        "print(doc[-1])\n",
        "print(doc[-1].is_currency)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IDW5v2FdwJk3"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymIUFyRUt0n-",
        "outputId": "be3b0024-6e9d-471d-e241-0a1e8cee6807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dayton high school, 8th grade students information\n",
            "==================================================\n",
            "\n",
            "Name    Birth Day       Email\n",
            "----    --------        -----\n",
            "Virat   5 June, 1882    virat@kholi.com\n",
            "Maria   12 April, 2001  maria@sharapova.com\n",
            "Serena  24 May, 1998    serena@gmail.com\n",
            "Joe     4 July, 2004    joe@yahoo.de\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imaging the following document\n",
        "d = \"\"\"\n",
        "Dayton high school, 8th grade students information\n",
        "==================================================\n",
        "\n",
        "Name    Birth Day       Email\n",
        "----    --------        -----\n",
        "Virat   5 June, 1882    virat@kholi.com\n",
        "Maria   12 April, 2001  maria@sharapova.com\n",
        "Serena  24 May, 1998    serena@gmail.com\n",
        "Joe     4 July, 2004    joe@yahoo.de\n",
        "\n",
        "\"\"\".lstrip()\n",
        "\n",
        "print(d)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zuA0lhMivHf1"
      },
      "source": [
        "How we can extract the emails using Spacy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sAmLBr-MvFyB",
        "outputId": "a6dad0a0-f5bf-44be-b48e-528071ea6e4a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dayton high school, 8th grade students information ==================================================  Name    Birth Day       Email ----    --------        ----- Virat   5 June, 1882    virat@kholi.com Maria   12 April, 2001  maria@sharapova.com Serena  24 May, 1998    serena@gmail.com Joe     4 July, 2004    joe@yahoo.de  '"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the document into a single sentence\n",
        "text = \" \".join(d.split(\"\\n\"))\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9GwrlCCMvv7-"
      },
      "outputs": [],
      "source": [
        "# Creating the Document object\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtAM1JEPv1_r",
        "outputId": "1d18ce41-72b1-4379-d939-ca76e8015f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[virat@kholi.com, maria@sharapova.com, serena@gmail.com, joe@yahoo.de]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting the email tokens\n",
        "emails = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.like_email:\n",
        "        emails.append(token)\n",
        "\n",
        "emails"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlXbabUwxx8"
      },
      "source": [
        "## Customizing the Tokenization Rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItChAZ32xGHH",
        "outputId": "85e0c0a6-d7e1-4806-fbad-baed8efdd6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gim\n",
            "me\n",
            "double\n",
            "cheese\n",
            "pizza\n",
            "please\n"
          ]
        }
      ],
      "source": [
        "# Let's say we want to replace the word `gimme` into `give` and `me`\n",
        "doc = nlp(\"gimme double cheese pizza please\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SppJmDRzxeRC"
      },
      "outputs": [],
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "    {ORTH: \"gim\"},\n",
        "    {ORTH: \"me\"}\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Ha1GtyxsS1"
      },
      "outputs": [],
      "source": [
        "# After aplying the spacial case:\n",
        "doc = nlp(\"gimme double cheese pizza please\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9pui-k0HyLdL"
      },
      "source": [
        "## Sentece Tokenization\n",
        "\n",
        "Remember that the Pipeline we have created is a blank Pipeline. So we cannot use `doc.sents` to get the sentence tokenization.\n",
        "\n",
        "To solve this we need to add a Pipe to our Pipeline, which we can do that using `add_pipe()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_oCeYTxy-Bg",
        "outputId": "cb9a50b6-a0f6-4041-dee8-16be1fbc63d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['sentencizer']"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Printing the Pipeline\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_Pp9Chfyek1",
        "outputId": "3c100023-7856-4ee7-dd3b-0d764fc57b25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f9c9a99d640>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBLzpAEByM2V",
        "outputId": "898a82f5-00ce-493f-8cd2-82ab4bbff3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr. Strange loves pav bhaji of mubai.\n",
            "Hulk loves chaat of delhi\n"
          ]
        }
      ],
      "source": [
        "# Now the object knows how to split any sentence into tokens\n",
        "doc = nlp(\"Dr. Strange loves pav bhaji of mubai. Hulk loves chaat of delhi\")\n",
        "\n",
        "for snetence_token in doc.sents:\n",
        "    print(snetence_token)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XyUwr4jqz9cB"
      },
      "source": [
        "## Example 2\n",
        "\n",
        "You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from a book and you want to grab all urls from this paragraph using spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0cNqf9Xzl93",
        "outputId": "90da4ab1-0d5e-4f83-df22-8620a0532ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Look for data to help you address the question. Governments are good\n",
            "sources because data from public research is often freely available. Good\n",
            "places to start include http://www.data.gov/, and http://www.science.\n",
            "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
            "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
            "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "\"\"\".lstrip()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFe3GowB0I9Q",
        "outputId": "4ce634f2-b2a7-41fb-cdbb-46d372379262"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[http://www.data.gov/,\n",
              " http://www.science,\n",
              " http://data.gov.uk/.,\n",
              " http://www3.norc.org/gss+website/,\n",
              " http://www.europeansocialsurvey.org/.]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "urls = []\n",
        "for token in nlp(text):\n",
        "    if token.like_url:\n",
        "        urls.append(token)\n",
        "\n",
        "urls"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x1MwKVqx0lLT"
      },
      "source": [
        "## Example 3\n",
        "\n",
        "Extract all money transaction from below sentence along with currency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKj4dbSG0mHg",
        "outputId": "2854d137-b8e8-4b07-bd48-7d299c261810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['two $', '500 €']"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "\n",
        "results = []\n",
        "num = ''\n",
        "for token in nlp(transactions):\n",
        "\n",
        "    if token.like_num:\n",
        "        num = token\n",
        "\n",
        "    if token.is_currency:\n",
        "        results.append(num.text + \" \" + token.text)\n",
        "        num = ''\n",
        "\n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "On93z6cu2eFR"
      },
      "source": [
        "For more information about the linguistuc features of Spacy: https://spacy.io/usage/linguistic-features#tokenization"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
