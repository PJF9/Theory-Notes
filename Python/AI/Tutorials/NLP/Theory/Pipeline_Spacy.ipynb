{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What is Spacy Pipeline\n",
        "\n",
        "`Spacy Pipeline` are the extra capabilities we add (along side with word-tokenizer) in order to make our nlp object more powerfull.\n",
        "\n",
        "Luckily Spacy has created some powerfull language models: https://spacy.io/usage/models"
      ],
      "metadata": {
        "id": "dD0o9xMD38_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "szM6W70r2wID"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the NlP Object"
      ],
      "metadata": {
        "id": "fWzOOKqV4jss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "4iBdLe_14g9s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've seen the spacy pipeline of a `blank` object is empty:"
      ],
      "metadata": {
        "id": "s_BijKAT4r1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ca6Lg4h4qRV",
        "outputId": "27babf47-cf0b-4564-9cf8-327d15f8a755"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Language Model\n"
      ],
      "metadata": {
        "id": "7n0LOgbl5z26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BkBl-HH54fn",
        "outputId": "f7785b11-b6e2-4698-9699-593d8f102b16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-05 17:21:35.711361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pre = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp_pre.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDrRmygH56BV",
        "outputId": "674b6dca-51bd-4da1-c5f3-e0b9c3c0dc29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this model has a lot pipes, that gives it extra capabilities."
      ],
      "metadata": {
        "id": "rEZR3huz6QUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What those Components do?\n",
        "\n",
        "\n",
        "* `tagger`: We know the part of speech for any word (noun, verb, number).\n",
        "* `lemmatizer`: We know the base word of any word (ate -> eat).\n",
        "* `ner`: We know what objects (and the names) exists in the document (Name Entity Recognization)\n",
        "* `attribute_ruler`: Provides some rules for lemmatization."
      ],
      "metadata": {
        "id": "qxfwF9kf6gUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Captain America ate 100$ of samosa. Then he said I can do it all day\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_)\n",
        "\n",
        "# `pos_`: part of speech\n",
        "# `lemma_`: the base of the word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGz4aUO86fwp",
        "outputId": "da6932b3-7480-4f7a-8ea8-9df7557c064c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |    |  \n",
            "America  |    |  \n",
            "ate  |    |  \n",
            "100  |    |  \n",
            "$  |    |  \n",
            "of  |    |  \n",
            "samosa  |    |  \n",
            ".  |    |  \n",
            "Then  |    |  \n",
            "he  |    |  \n",
            "said  |    |  \n",
            "I  |    |  \n",
            "can  |    |  \n",
            "do  |    |  \n",
            "it  |    |  \n",
            "all  |    |  \n",
            "day  |    |  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech and Lemmatization"
      ],
      "metadata": {
        "id": "bX2UJCangy3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# But by going the ssame task with the build-in model\n",
        "doc = nlp_pre(\"Captain America ate 100$ of samosa. Then he said I can do it all day\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5acSBoh7E8-",
        "outputId": "45d774c8-e08a-46f2-ee04-7b55f77e06ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  Captain\n",
            "America  |  PROPN  |  America\n",
            "ate  |  VERB  |  eat\n",
            "100  |  NUM  |  100\n",
            "$  |  NUM  |  $\n",
            "of  |  ADP  |  of\n",
            "samosa  |  PROPN  |  samosa\n",
            ".  |  PUNCT  |  .\n",
            "Then  |  ADV  |  then\n",
            "he  |  PRON  |  he\n",
            "said  |  VERB  |  say\n",
            "I  |  PRON  |  I\n",
            "can  |  AUX  |  can\n",
            "do  |  VERB  |  do\n",
            "it  |  PRON  |  it\n",
            "all  |  DET  |  all\n",
            "day  |  NOUN  |  day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are those part of speech?\n",
        "* Noun: A person, an event, an idea, etc. (for a specific person, like \"Petros\" then a noun is called a Proper Noun)\n",
        "* Verb: An action\n",
        "* Pronoun: A replacement of a noun, like \"I\", \"He\", etc.\n",
        "* Adjective: A discription of the noun\n",
        "* Adverb: A discription of the verb.\n",
        "* Interjection: Represents a strong emotion, like \"Hey!\", \"Wow!\", etc.\n",
        "* Conjection: A connection of two sentences, like \"and\", \"or\", \"but\", etc.\n",
        "* Preposition: A link, connection of the noun to another word, like \"in\", \"on\", etc.\n",
        "\n",
        "With `spacy.explain(token.pos_)` and `token.tag_` we can see more details about the part of speech of a token."
      ],
      "metadata": {
        "id": "6dky7PEmZSBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqIDOx2WcuVn",
        "outputId": "95106a1d-9a78-4e1e-b1dd-e51822e5ab27"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "America  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "ate  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
            "100  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "of  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
            "samosa  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            ".  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
            "Then  |  ADV  |  adverb  |  RB  |  adverb\n",
            "he  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
            "said  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
            "I  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
            "can  |  AUX  |  auxiliary  |  MD  |  verb, modal auxiliary\n",
            "do  |  VERB  |  verb  |  VB  |  verb, base form\n",
            "it  |  PRON  |  pronoun  |  PRP  |  pronoun, personal\n",
            "all  |  DET  |  determiner  |  DT  |  determiner\n",
            "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another think we could do is to count the different part of speeches (we can also use lemma, etc.)\n",
        "count = doc.count_by(spacy.attrs.POS)\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crVBhkvHeIGI",
        "outputId": "f5740d17-8dc6-4187-9780-8980885386e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{96: 3, 100: 3, 93: 2, 85: 1, 97: 1, 86: 1, 95: 3, 87: 1, 90: 1, 92: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Because the results are in numerical representation we need to convert those into words using the vocabulary of the document\n",
        "print(doc.vocab[96].text)\n",
        "\n",
        "for k, v in count.items():\n",
        "    print(doc.vocab[k].text, \": \", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K40JlhL0eg79",
        "outputId": "6b8376f6-92e2-4338-fef8-07f1112d3111"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPN\n",
            "PROPN :  3\n",
            "VERB :  3\n",
            "NUM :  2\n",
            "ADP :  1\n",
            "PUNCT :  1\n",
            "ADV :  1\n",
            "PRON :  3\n",
            "AUX :  1\n",
            "DET :  1\n",
            "NOUN :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name Entity Recognization"
      ],
      "metadata": {
        "id": "zclg9jX2g-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_pre = nlp_pre(\"Tesla Inc is going to acquire Twitter for $45 billion\")\n",
        "\n",
        "for ent in doc_pre.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))\n",
        "\n",
        "# We can also tell Spacy to explain how it has defined each label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ubfB-F8Zt7",
        "outputId": "a58b2ce3-462c-4a03-d288-068d4579991a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "Twitter  |  PRODUCT  |  Objects, vehicles, foods, etc. (not services)\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also display the sentence in a way that emphasize different entities and objects\n",
        "from spacy import displacy\n",
        "\n",
        "print(displacy.render(doc_pre, style=\"ent\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7psfd6x9d2i",
        "outputId": "ac63e482-4dac-4903-877b-a11816f0bb1d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
            "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
            "    Tesla Inc\n",
            "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
            "</mark>\n",
            " is going to acquire \n",
            "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
            "    Twitter\n",
            "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
            "</mark>\n",
            " for \n",
            "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
            "    $45 billion\n",
            "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
            "</mark>\n",
            "</div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see all the named entities spacy supports\n",
        "print(nlp_pre.pipe_labels[\"ner\"])\n",
        "\n",
        "for ne in nlp_pre.pipe_labels[\"ner\"]:\n",
        "    print(ne, \": \", spacy.explain(ne))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE8KcI8dh2Jy",
        "outputId": "d650e515-e350-4cba-d6ba-46cc9f2c9766"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
            "CARDINAL :  Numerals that do not fall under another type\n",
            "DATE :  Absolute or relative dates or periods\n",
            "EVENT :  Named hurricanes, battles, wars, sports events, etc.\n",
            "FAC :  Buildings, airports, highways, bridges, etc.\n",
            "GPE :  Countries, cities, states\n",
            "LANGUAGE :  Any named language\n",
            "LAW :  Named documents made into laws.\n",
            "LOC :  Non-GPE locations, mountain ranges, bodies of water\n",
            "MONEY :  Monetary values, including unit\n",
            "NORP :  Nationalities or religious or political groups\n",
            "ORDINAL :  \"first\", \"second\", etc.\n",
            "ORG :  Companies, agencies, institutions, etc.\n",
            "PERCENT :  Percentage, including \"%\"\n",
            "PERSON :  People, including fictional\n",
            "PRODUCT :  Objects, vehicles, foods, etc. (not services)\n",
            "QUANTITY :  Measurements, as of weight or distance\n",
            "TIME :  Times smaller than a day\n",
            "WORK_OF_ART :  Titles of books, songs, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also add names to some entities\n",
        "doc_pre = nlp_pre(\"Tesla is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc_pre.ents:\n",
        "    print(ent, \" | \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUY8qXxhjB0I",
        "outputId": "31507ca1-f5c6-45e5-a6c2-0505609a732b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla  |  ORG\n",
            "$45 billion  |  MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "s1 = Span(doc_pre, 5, 6, label=\"ORG\") # Where [5, 6) are the index positions of the word twitter\n",
        "\n",
        "# Updating the set of entries\n",
        "doc_pre.set_ents([s1], default=\"unmodified\") # Where `default` represents all the entries that already exists in the document"
      ],
      "metadata": {
        "id": "WggMZ0_yjRkh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc_pre.ents:\n",
        "    print(ent, \" | \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoLMRqhQj42y",
        "outputId": "b15f6f3b-b369-46bb-9991-d151f9549776"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla  |  ORG\n",
            "twitter  |  ORG\n",
            "$45 billion  |  MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Pipes to the Pipeline"
      ],
      "metadata": {
        "id": "IH6AvDjI-96P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"lemmatizer\")\n",
        "nlp.add_pipe(\"tagger\")\n",
        "\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIHgv2_b-88Y",
        "outputId": "f22d2586-4d16-4c22-9c60-92f806dd8f4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lemmatizer', 'tagger']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp_pre(\"Captain America ate 100$ of samosa. Then he said I can do it all day\")\n",
        "\n",
        "# Now we can look for the part-of-speech and the lemma of the word tokens\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.pos_, \" | \", token.lemma_, \" | \", token.lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBbCf7sy_H_x",
        "outputId": "1fdc2f3e-a5d1-4141-f102-c721f906a96b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain  |  PROPN  |  Captain  |  7274276235148819399\n",
            "America  |  PROPN  |  America  |  13134984502707718284\n",
            "ate  |  VERB  |  eat  |  9837207709914848172\n",
            "100  |  NUM  |  100  |  12136287192145005166\n",
            "$  |  NUM  |  $  |  11283501755624150392\n",
            "of  |  ADP  |  of  |  886050111519832510\n",
            "samosa  |  PROPN  |  samosa  |  3808861230714717819\n",
            ".  |  PUNCT  |  .  |  12646065887601541794\n",
            "Then  |  ADV  |  then  |  2630753287402592467\n",
            "he  |  PRON  |  he  |  1655312771067108281\n",
            "said  |  VERB  |  say  |  8685289367999165211\n",
            "I  |  PRON  |  I  |  4690420944186131903\n",
            "can  |  AUX  |  can  |  6635067063807956629\n",
            "do  |  VERB  |  do  |  2158845516055552166\n",
            "it  |  PRON  |  it  |  10239237003504588839\n",
            "all  |  DET  |  all  |  13409319323822384369\n",
            "day  |  NOUN  |  day  |  1608482186128794349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where `token.lemma` gives us the hash value of the lemma of the token in the mapping table."
      ],
      "metadata": {
        "id": "Oyp5bpYLBW9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding some Lemmatization Rules"
      ],
      "metadata": {
        "id": "D55oGmDsDiHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp_pre.get_pipe(\"attribute_ruler\")\n",
        "\n",
        "ar.add([[{\"TEXT\": \"Bro\"}], [{\"TEXT\": \"brah\"}]], {\"LEMMA\": \"Brother\"})"
      ],
      "metadata": {
        "id": "gbCzhB2SCMq1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in nlp_pre(\"Bro, wanna gp out? Nah, brah I am exhausted!\"):\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br0QO13LCynP",
        "outputId": "f1b9c915-33d5-412c-bc48-e7e2258971eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro  |  Brother\n",
            ",  |  ,\n",
            "wanna  |  wanna\n",
            "gp  |  gp\n",
            "out  |  out\n",
            "?  |  ?\n",
            "Nah  |  Nah\n",
            ",  |  ,\n",
            "brah  |  Brother\n",
            "I  |  I\n",
            "am  |  be\n",
            "exhausted  |  exhaust\n",
            "!  |  !\n"
          ]
        }
      ]
    }
  ]
}