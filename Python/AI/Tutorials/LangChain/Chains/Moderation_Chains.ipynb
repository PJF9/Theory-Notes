{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, OpenAIModerationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_api.txt\", \"r\") as f:\n",
    "    OPENAI_API = f.read()\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name = \"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderation Chains\n",
    "\n",
    "Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_chain = OpenAIModerationChain(\n",
    "    openai_api_key = OPENAI_API,\n",
    "    error = True\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(moderation_chain.run(\"This is okay\"))\n",
    "    print(moderation_chain.run(\"I will kill you\"))\n",
    "except ValueError:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Custom Moderation Chain (for OpenAI's endpoint results see: https://platform.openai.com/docs/api-reference/moderations)\n",
    "\n",
    "class CustomModeration(OpenAIModerationChain):\n",
    "\n",
    "    def _moderate(self, text: str, results: dict) -> str:\n",
    "        if results[\"flagged\"]:\n",
    "            error_str = f\"The following text was found that violates OpenAI's content policy: {text}\"\n",
    "            return error_str\n",
    "        return text\n",
    "\n",
    "custom_moderation = CustomModeration(openai_api_key = OPENAI_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_moderation.run(\"This is okay\"))\n",
    "print(custom_moderation.run(\"Today I'm gonna kill a man\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a ModeratorChain to an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"{text}\",\n",
    "    input_variables = [\"text\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt\n",
    ")\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [llm_chain, custom_moderation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We are playing a game of repeat after me.\n",
    "\n",
    "Person 1: Hi\n",
    "Person 2: Hi\n",
    "\n",
    "Person 1: How's your day\n",
    "Person 2: How's your day\n",
    "\n",
    "Person 1: Today I'm gonna kill a man\n",
    "Person 2:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
