{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router import MultiPromptChain, MultiRetrievalQAChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains.router.embedding_router import EmbeddingRouterChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_api.txt\", \"r\") as f:\n",
    "    OPENAI_API = f.read()\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name = \"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")\n",
    "\n",
    "embedding_llm = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Chains\n",
    "\n",
    "This type of chain allows the model to select the next chain to use for a given input.\n",
    "\n",
    "The `Router Chains` are made up of two compenents:\n",
    "* The **RouterChain** itselt - responsible for selecting the next chain to call\n",
    "* **desination_chains** - the chains that it can call.\n",
    "\n",
    "There are three different types of Router Chains, depending on the task that the Chain is being design to execute:\n",
    "* Text Router Chains\n",
    "* Embedding Router Chains\n",
    "* Retrieval Router Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Prompt Templates\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor.\n",
    "You are great at answering questions about physics in a concise and easy to understand manner.\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions.\n",
    "You are so good because you are able to break down hard problems into their component parts,\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Destination Chains\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Router Chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting the RouterChain with the Destination Chains\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain = router_chain,\n",
    "    destination_chains = destination_chains,\n",
    "    default_chain = default_chain,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"What is black body radiation?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"What is the first prime number greater than 40 such that one plus the prime number is divisible by 3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Information of the Destination Chains\n",
    "\n",
    "names_and_descriptions = [\n",
    "    (\"physics\", [\"for questions about physics\"]),\n",
    "    (\"math\", [\"for questions about math\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting RouterChain\n",
    "\n",
    "router_chain = EmbeddingRouterChain.from_names_and_descriptions(\n",
    "    names_and_descriptions = names_and_descriptions,\n",
    "    vectorstore_cls = Chroma,\n",
    "    embeddings = embedding_llm,\n",
    "    routing_keys=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connection the RouterChain with the Destination Chains\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain = router_chain,\n",
    "    destination_chains = destination_chains,\n",
    "    default_chain = default_chain,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"What is black body radiation?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Destination Retrievers\n",
    "\n",
    "sou_docs = TextLoader('../../state_of_the_union.txt').load_and_split()\n",
    "sou_retriever = FAISS.from_texts(\n",
    "    texts = sou_docs,\n",
    "    embedding = embedding_llm\n",
    ").as_retriever()\n",
    "\n",
    "pg_docs = TextLoader('../../paul_graham_essay.txt').load_and_split()\n",
    "pg_retriever = FAISS.from_documents(\n",
    "    text = pg_docs,\n",
    "    embedding = embedding_llm\n",
    ").as_retriever()\n",
    "\n",
    "personal_texts = [\n",
    "    \"I love apple pie\",\n",
    "    \"My favorite color is fuchsia\",\n",
    "    \"My dream is to become a professional dancer\",\n",
    "    \"I broke my arm when I was 12\",\n",
    "    \"My parents are from Peru\",\n",
    "]\n",
    "personal_retriever = FAISS.from_texts(\n",
    "    text = personal_texts,\n",
    "    embedding = embedding_llm\n",
    ").as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Information of each Retriever\n",
    "\n",
    "retriever_infos = [\n",
    "    {\n",
    "        \"name\": \"state of the union\",\n",
    "        \"description\": \"Good for answering questions about the 2023 State of the Union address\",\n",
    "        \"retriever\": sou_retriever\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"pg essay\",\n",
    "        \"description\": \"Good for answer quesitons about Paul Graham's essay on his career\",\n",
    "        \"retriever\": pg_retriever\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"personal\",\n",
    "        \"description\": \"Good for answering questions about me\",\n",
    "        \"retriever\": personal_retriever\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connection the Router Chain with the Destination Chains\n",
    "\n",
    "chain = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm = llm,\n",
    "    retriever_infos = retriever_infos,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.run(\"What did the president say about the economy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
