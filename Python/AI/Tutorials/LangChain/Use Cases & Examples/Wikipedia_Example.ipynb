{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "import pinecone\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from datasets import load_dataset\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Knowledge Base\n",
    "\n",
    "We have two primary types of knowledge for LLMs:\n",
    "1. `parametric knowledge`: efers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM.\n",
    "\n",
    "2. `source knowledge`: This knowledge covers any information fed into the LLM via the input prompt.\n",
    "\n",
    "When we talk about retrieval augmentation, we’re talking about giving the LLM valuable source knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we will use a subset of Wikipedia. To get that data, we will use Hugging Face datasets like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Chunks\n",
    "\n",
    "Most datasets will contain records that include a lot of text. Because of this, our first task is usually to build a preprocessing pipeline that chunks those long bits of text into more concise chunks. Splitting our text into smaller chunks is essential for several reasons:\n",
    "* `Improve embedding accuracy` — this will improve the relevance of results later.\n",
    "\n",
    "* `Reduce the amount of text` fed into our LLM as source knowledge. Limiting input improves the LLM's ability to follow instructions, reduces generation costs, and helps us get faster responses.\n",
    "\n",
    "* Provide users with more `precise information sources` as we can narrow down the information source to a smaller chunk of text.\n",
    "\n",
    "* In the case of very long chunks of text, we will exceed the maximum context window of our embedding or completion models. Splitting these chunks makes it possible to `add these longer documents` to our knowledge base.\n",
    "\n",
    "To create these chunks, we first need a way of measuring the length of our text. LLMs don't measure text by word or character — they measure it by `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As we are going to use `gpt-3.5-turbo` as LLM we are going to use `pk50k_base` tokenize\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function that calculates the amount of tokens\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function we can find the length of this chunk of text in tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the Text into Chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400,   # splitting to no longer than that\n",
    "    chunk_overlap = 20, # manimum that amount of chunks can overlap\n",
    "    length_function = tiktoken_len,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(data[0][\"text\"])[:3]\n",
    "\n",
    "print(tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2]))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "\n",
    "We take the chunks of text we'd like to store in our knowledge base and encode each chunk into a `vector embedding`.\n",
    "\n",
    "We then create the embeddings with `another AI` language model that has learned to translate human-readable text into AI-readable embeddings.\n",
    "\n",
    "Finally, we store these embeddings in our `vector database` (more on this soon) and can find text chunks with similar meanings by calculating the distance between embeddings in vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are going to use `ada` for our embeddings (default)\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    openai_api_key = open(\"api_key.txt\", \"r\").read()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"this is the first chunk of text\",\n",
    "    \"then another second chunk of text is here\"\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we get two embeddings as we passed in two chunks of text. Each embedding is a 1536-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "A vector database is a type of knowledge base that allows us to scale the search of similar embeddings to billions of records, manage our knowledge base by adding, updating, or removing records, and even do things like filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Database Index\n",
    "\n",
    "index_name = \"langchain-retrieval-augmentation\"\n",
    "\n",
    "pinecone.init(api_key = open(\"pinecone_api.txt\", \"r\").read(), environment=\"asia-northeast1-gcp\")\n",
    "\n",
    "pinecone.create_index(\n",
    "    name = index_name,\n",
    "    metric = \"dotproduct\",\n",
    "    dimension = len(res[0]) # 1536 in our case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connecting to the index\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can clearly see that the index is empty.\n",
    "\n",
    "The indexing process consists of us iterating through the data we'd like to add to our knowledge base, creating IDs, embeddings, and metadata — then adding these to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # Adding Record's Metadata\n",
    "    metadata = {\n",
    "        \"wiki-id\": str(record[\"id\"]),\n",
    "        \"source\": record[\"url\"],\n",
    "        \"title\": record[\"title\"]\n",
    "    }\n",
    "\n",
    "    # Splitting `text` into Chunks\n",
    "    record_texts = text_splitter.split_text(record[\"text\"])\n",
    "\n",
    "    # Creating Individual Metadata Dictionaries for each Chunk\n",
    "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata} for j, text in enumerate(record_texts)]\n",
    "\n",
    "    # Appending to Current Batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "\n",
    "    # If we have reached `batch_limit` adding Texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Vectorstore and Querying\n",
    "\n",
    "We construct our index independently of LangChain. That's because it's a straightforward process, and it is faster to do this with the Pinecone client directly. However, we're about to jump back into LangChain, so we should reconnect to our index via the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = \"text\"\n",
    "\n",
    "# Connecting to our Index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Creating the LangChain Pinecone's Object\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Index\n",
    "\n",
    "query = \"who was Benito Mussolini?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query, # our search query\n",
    "    k=3    # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these are relevant results, telling us that the retrieval component of our systems is `functioning`. The next step is adding our LLM to generatively answer our question using the information provided in these retrieved contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Question Answering\n",
    "\n",
    "In generative question-answering (`GQA`), we pass our question to the LLM but instruct it to base the answer on the information returned from our `knowledge base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the LLM\n",
    "\n",
    "chatGPT = ChatOpenAI(\n",
    "    openai_api_key = open(\"openai_api.txt\").read(),\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the GQA Object\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = chatGPT,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on out query\n",
    "\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also create a Chain that link the source of every claim\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = chatGPT,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
