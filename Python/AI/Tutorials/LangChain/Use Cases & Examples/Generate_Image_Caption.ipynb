{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model_name = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature = 0,\n",
    "    openai_api_key = open(\"openai_api.txt\", \"r\").read()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take an existing open-source model that has been trained for a specific task that out LLM can't do. That model is going to be the `Salesforce/blip-image-captioning-large` from HuggingFace. That model is an expert for describing images in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process goes as follows:\n",
    "1. `Download` Image\n",
    "2. `Open` it as a PIL object\n",
    "3. Resize and Normalize it using the `processor`\n",
    "4. Create a caption using the `model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Model\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 and 2\n",
    "\n",
    "img_url = \"https://images.unsplash.com/photo-1616128417859-3a984dd35f02?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2372&q=80\"\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 and 4\n",
    "\n",
    "inputs = processor (image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Tool Class\n",
    "\n",
    "class ImageCaptioningTool(BaseTool):\n",
    "    name = \"Image Captioner\"\n",
    "    description = \"Use this tool when given a URL of an image that you'd like to descibe. It will return a simple caption describing the image.\"\n",
    "\n",
    "    def _run(self, url: str):\n",
    "        # Loading the Opening the Imahe\n",
    "        image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "        # Processing the Image\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Generate Caption Indexes\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        # Returning the Caption as a String\n",
    "        return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"[ERROR] This tool does not support async.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [ImageCaptioningTool()]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(f\"What does this image show?\\n{img_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://images.unsplash.com/photo-1502680390469-be75c86b636f?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2370&q=80\"\n",
    "agent(f\"what is in this image?\\n{img_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://images.unsplash.com/photo-1680382948929-2d092cd01263?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2365&q=80\"\n",
    "agent(f\"what is in this image?\\n{img_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
