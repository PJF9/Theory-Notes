{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrN1_jw2KmSB",
        "outputId": "259b91f0-4098-4296-dd16-8c2b3c3f2144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install langchain -q\n",
        "# !pip install openai -q\n",
        "# !pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2nhFKNCKPZsU"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.cache import InMemoryCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNaBdrpWPdZ6"
      },
      "source": [
        "## Models Overvie\n",
        "\n",
        "One of the core value props of LangChain is that it provides a standard `interface` to models. This allows you to swap easily between models. At a high level, there are two main types of models:\n",
        "* `Language Models` -- good for text generation\n",
        "    * LLMs: wrap APIs which take text input and output.\n",
        "    * ChatModels: wrap models whick take chat messages input and output.\n",
        "\n",
        "* `Text Embedding Models` -- good for turning text into numerical representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDItB6AwQIX3"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(openai_api_key=open(\"openai_api.txt\", \"r\").read())      # default is `text-davinci-003`\n",
        "chat = ChatOpenAI(openai_api_key=open(\"openai_api.txt\", \"r\").read()) # default is `gpt-3.5-turbo`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhqzuK42QYXr",
        "outputId": "6ce91210-2b72-4b9c-e1eb-f39bedc4671f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi there!\n",
            "Hello!\n"
          ]
        }
      ],
      "source": [
        "## Text -> Text\n",
        "\n",
        "print(llm.predict(\"Say Hi!\").strip())  # works also by `__call__()`\n",
        "print(chat.predict(\"Say Hi!\").strip()) # `__call__()` only works if we input a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ic2o9dQ90X",
        "outputId": "5335d82b-8025-460d-e4e6-c4d2c1df6a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='\\n\\nRobot: Hi there! How can I help you?' additional_kwargs={} example=False\n",
            "content='Hi!' additional_kwargs={} example=False\n"
          ]
        }
      ],
      "source": [
        "## Messages -> Message\n",
        "\n",
        "print(llm.predict_messages([HumanMessage(content=\"say hi!\")]))  # doesn't work with `__call__()`\n",
        "print(chat.predict_messages([HumanMessage(content=\"say hi!\")])) # also works with `__call__()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZQWRNltRP_k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHNgiglJWI3Q"
      },
      "source": [
        "## Chat Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQSDAEZ5WSis"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJyTechzWj86"
      },
      "source": [
        "### Messages\n",
        "\n",
        "We get chat completions by `passing` one or more messages to the model and the result will also be a `message`.\n",
        "\n",
        "LangChain supports:\n",
        "* **AIMessage** -- the respone of the AI in one of our question | is being used for few-shot-examples.\n",
        "\n",
        "* **HumanMessage** -- the human input | the query that we pass to the model.\n",
        "\n",
        "* **SystemMessage** -- the context of the model that we are passing to guide its behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-3kHuuvWezT",
        "outputId": "c1baaa8c-cd0e-4296-c44e-8d250dbf814c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CxlINLfXDSX",
        "outputId": "3b81d6ac-8ec3-4821-be98-afba6c8fd902"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7fqAJHXMRA",
        "outputId": "6176d47a-7898-4d56-eb28-5979c352703d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tWWFXQVegfB6NBgDa141slPQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tWWFXQVegfB6NBgDa141slPQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-tWWFXQVegfB6NBgDa141slPQ on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text=\"J'adore programmer.\", generation_info=None, message=AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 18, 'total_tokens': 75}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## For multiple Messages\n",
        "\n",
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "\n",
        "result = chat.generate(batch_messages) # doesn't work with `__call__()`\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNbmtsRoYg4O"
      },
      "source": [
        "we can see a lot of information that we can access about the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSuqOwQgXvUk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwkr03JkY3j1"
      },
      "source": [
        "### Prompt Templates\n",
        "\n",
        "We can make use of templating by using `MessagePromptTemplate`. We can make a **ChatPromptTemplate** from one or more **MessagePromptTemplate**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjUb3VMiZP_A"
      },
      "outputs": [],
      "source": [
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7JaBERbZqQz"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4pgYraKZq2K",
        "outputId": "415e59b6-7006-4991-cc72-bdb93b209762"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation !\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(chat_prompt.format_prompt(\n",
        "    input_language = \"English\",\n",
        "    output_language = \"French\",\n",
        "    text = \"I love programming!\").to_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxnO_kY8dGlM"
      },
      "source": [
        "### Chains\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcht-8godJjI"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate(\n",
        "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "    input_variables=[\"input_language\", \"output_language\"]\n",
        ")\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate(prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcf9hMEJdK4k"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=chat, prompt=chat_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6AcpRm3ZdO9L",
        "outputId": "765af3ac-9618-4e85-9591-048a96c54a07"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"J'adore la programmation!\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSsDd_hSeY6S"
      },
      "source": [
        "### Few Shot Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApGgxb4CdUQe"
      },
      "outputs": [],
      "source": [
        "## Using `AIMessage`\n",
        "\n",
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zCDG936felwp",
        "outputId": "7729e954-823b-4693-987c-59b055c5ec59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I be lovin' programmin', arrr!\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "chain.run(\"I love programming.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ9anEZGeoai"
      },
      "outputs": [],
      "source": [
        "## Using `SystemMessage`\n",
        "\n",
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "example_human = SystemMessagePromptTemplate.from_template(\"Hi\", additional_kwargs={\"name\": \"example_user\"})\n",
        "example_ai = SystemMessagePromptTemplate.from_template(\"Argh me mateys\", additional_kwargs={\"name\": \"example_assistant\"})\n",
        "\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "txQuiDn6eye5",
        "outputId": "1c571419-7012-4d82-a0ba-86821572ab3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I be lovin' the art of code, me hearty.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "chain.run(\"I love programming.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eEkeY_cez9l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfyIQPNe6W_"
      },
      "source": [
        "## Text Embedding Models\n",
        "\n",
        "The Embedding class is a class designed for `interfacing` with embeddings. There are lots of Embedding providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a `standard` interface for all of them.\n",
        "\n",
        "Embeddings create a `vector representation` of a piece of text. This is useful because it means we can think about text in the `vector space`, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
        "\n",
        "\n",
        "The base Embedding class in LangChain exposes two methods: `embed_documents` and `embed_query`. The largest difference is that these two methods have different interfaces: one works over multiple documents, while the other works over a single document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3QLKnM2e7lP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wFgHNukyxg8"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=open(\"openai_api.txt\", \"r\").read()) # default is `text-embedding-ada-002`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5-NlcRZy__d",
        "outputId": "24c4cdca-c18e-4bcf-ad35-92db4dc3ef3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 1536 [-0.0031265460420399904, 0.01113363541662693, -0.004037691745907068]\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a test document.\"\n",
        "\n",
        "query_result = embeddings.embed_query(text)\n",
        "print(type(query_result), len(query_result), query_result[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQMBnbIXzNQj",
        "outputId": "a5c42711-b43d-4fb5-9a96-e7f4e1499970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 1 1536 [-0.0031362669868111224, 0.011212612618712038, -0.004014022224356029]\n"
          ]
        }
      ],
      "source": [
        "doc_result = embeddings.embed_documents([text])\n",
        "\n",
        "print(type(doc_result), len(doc_result), len(doc_result[0]), doc_result[0][:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdHg7en6L8Fd"
      },
      "source": [
        "## Memory Cache\n",
        "\n",
        "It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "etz5Ob--L9OI"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", n=2, best_of=2, openai_api_key=open(\"openai_api.txt\", 'r').read())\n",
        "\n",
        "langchain.llm_cache = InMemoryCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n6d9vlYJMb7J",
        "outputId": "d510064f-4c1b-4ded-f8f5-9ee09119107f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.predict(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S7hdI7GfMcN-",
        "outputId": "461871c6-3cd4-43be-ab73-61e3db356115"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The second time it is, so it goes faster\n",
        "llm.predict(\"Tell me a joke\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
