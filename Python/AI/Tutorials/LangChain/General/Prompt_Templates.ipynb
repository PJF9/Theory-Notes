{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIjDclboj26Z",
        "outputId": "099f8bde-ccc6-4a96-db8a-46c073fe03c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install langchain -q\n",
        "# !pip install openai -q\n",
        "# !pip install tiktoken -q\n",
        "# !pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uiKveT4wj7cI"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.prompts import load_prompt\n",
        "from langchain import FewShotPromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector, MaxMarginalRelevanceExampleSelector, SemanticSimilarityExampleSelector\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatMessagePromptTemplate\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqhJiz8Dj__3",
        "outputId": "9fd78055-b342-4528-a07e-13f28a4ea711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Setting API keys\n",
        "\n",
        "with open(\"openai_api.txt\", \"r\") as f:\n",
        "    OPENAI_API = f.read()\n",
        "\n",
        "type(OPENAI_API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZF-dBrrhsp-"
      },
      "source": [
        "## Power of Prompt Engineering\n",
        "\n",
        "Traditionaly we would fine-tune a Language Model to solve a downstream taks. This implies that our models can only solve the task they have been trained on. LangChain and Large Language Models (LLM) give us a new perspective.\n",
        "\n",
        "We can, now, `prompt` a LLM to solve any dawnstream task. It's possible due to the way those model have beed trained. They have been trained to receive text as input and output once again text.\n",
        "\n",
        "So programming models is as simple as prompting the LLM to solve the task we are aiming for. A `prompt` refers to the input to the model. This input is often constructed from multiple components, like a `PromptTemplate`, which is responsible for the construction of this input. LangChain provides several classes and functions to make constructing and working with prompts easy.\n",
        "\n",
        "In this new age of LLMs, `prompts` are kings. Bad prompts produce bad outputs, and good prompts are unreasonably powerful. Constructing good prompts is a crucial skill for those building with LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVC1QfFveu5J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp1GpafJ-WjF"
      },
      "source": [
        "## Prompt Structure\n",
        "\n",
        "A prompt is typically composed of multiple parts:\n",
        "\n",
        "* `Instructions` tell the model what to do, how to use external information if provided, what to do with the query, and how to construct the output.\n",
        "\n",
        "* `External information` (context) act as an additional source of knowledge for the model. These can be manually inserted into the prompt, retrieved via a vector database (retrieval augmentation), or pulled in via other means (APIs, calculations, etc.).\n",
        "\n",
        "* `User input` (query) is typically (but not always) a query input into the system by a human user (the prompter).\n",
        "\n",
        "* `Output indicator` marks the beginning of the to-be-generated text. If generating Python code, we may use import to indicate to the model that it must begin writing Python code.\n",
        "\n",
        "Each component is usually placed in the prompt in this order. Starting with instructions, external information (where applicable), prompter input, and finally, the output indicator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADJHUpKG-W1C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTSxBGlG0FN3"
      },
      "source": [
        "## Prompt Template\n",
        "\n",
        "A `prompt template` refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
        "\n",
        "The prompt template may contain:\n",
        "* `instructions` to the LLM\n",
        "* a set of few shot `examples` to help the LLM generate better responses\n",
        "* a `question` to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aqtvnoZ0dsM",
        "outputId": "d73e896e-a8e3-489f-fc20-ea4ea75ec8b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('input_variables', ['product'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('template', '\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes {product}?\\n')\n",
            "('template_format', 'f-string')\n",
            "('validate_template', True)\n",
            "\n",
            "I want you to act as a naming consultant for new companies.\n",
            "What is a good name for a company that makes AI Software?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Setting the Template\n",
        "\n",
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n",
        "\n",
        "## Creating the Prompt Object\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = template,\n",
        "    input_variables = [\"product\"]\n",
        ")\n",
        "\n",
        "## Seeing the Prompt Components\n",
        "\n",
        "for k in prompt:\n",
        "    print(k)\n",
        "\n",
        "## Filling `product`\n",
        "\n",
        "print(prompt.format(product = \"AI Software\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBJ57FAL056h",
        "outputId": "69e61abe-8b86-4aa4-afae-be2c5b4c4a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('input_variables', ['product'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('template', '\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes {product}?\\n')\n",
            "('template_format', 'f-string')\n",
            "('validate_template', True)\n",
            "\n",
            "I want you to act as a naming consultant for new companies.\n",
            "What is a good name for a company that makes AI Software?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## We can also create a Prompt Template only using a Template\n",
        "\n",
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "for k in prompt:\n",
        "    print(k)\n",
        "\n",
        "## Filling `product`\n",
        "\n",
        "print(prompt.format(product = \"AI Software\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WxD4bsZeF50r",
        "outputId": "0f9a86fe-9eb3-4739-9608-e1893f1c977f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tell me a funny joke about chickens.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IqwkB9O2BYj"
      },
      "source": [
        "By default `PromptTemplate` will validate the template string by checking whether the `input_variables` match the variables defined in `template`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx6ihvIK1pJj",
        "outputId": "af0fb10d-801d-4616-ccd2-1dbfe74fe114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('input_variables', ['product', 'foo'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('template', '\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes {product}?\\n')\n",
            "('template_format', 'f-string')\n",
            "('validate_template', False)\n",
            "\n",
            "I want you to act as a naming consultant for new companies.\n",
            "What is a good name for a company that makes AI Software?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## We can disable that using:\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"product\", \"foo\"],\n",
        "    validate_template=False\n",
        ")\n",
        "\n",
        "for k in prompt:\n",
        "    print(k)\n",
        "\n",
        "print(prompt.format(product = \"AI Software\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvmcFY5E2TeS"
      },
      "outputs": [],
      "source": [
        "## Saving and Loading a Prompt Template\n",
        "\n",
        "prompt.save(\"simple_prompt.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfxN_LmD2rEr"
      },
      "outputs": [],
      "source": [
        "loadded_prompt = load_prompt(\"simple_prompt.json\")\n",
        "\n",
        "assert loadded_prompt == prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u56_FLDS3H8Q"
      },
      "source": [
        "## Few Shot Examples\n",
        "\n",
        "The success of LLMs comes from their large size and ability to store `knowledge` within the model parameter. However, there are more ways to pass knowledge to an LLM. The two primary methods are:\n",
        "* `Parametric knowledge` — anything that has been learned by the model during training time and is stored within the model parameters.\n",
        "\n",
        "* `Source knowledge` — any knowledge provided to the model at inference time via the input prompt.\n",
        "\n",
        "Langchain's `FewShotPromptTemplate` caters to source knowledge input. The idea is to \"train\" the model with some examples to guide how it should think to generate better responses.\n",
        "\n",
        "`FewShotPromptTemplate` takes in a `PromptTemplate` and a list of few shot examples. It then formats the prompt template with the few shot examples.\n",
        "\n",
        "Few-shot learning is perfect when our model needs help understanding what we're asking it to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzII1OKd3Jdn",
        "outputId": "96f47176-da4e-45c4-ba88-9074d33f7668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('input_variables', ['input'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('examples', [{'product': 'colourful socks', 'name': 'Cute Socks'}, {'product': 'AI Software', 'name': 'BrainAI'}])\n",
            "('example_selector', None)\n",
            "('example_prompt', PromptTemplate(input_variables=['product', 'name'], output_parser=None, partial_variables={}, template='\\nProduct: {product}\\nName: {name}\\n', template_format='f-string', validate_template=True))\n",
            "('suffix', 'Product: {input}\\nName: ')\n",
            "('example_separator', '\\n')\n",
            "('prefix', 'Provide a name to a company according to the product it is developing')\n",
            "('template_format', 'f-string')\n",
            "('validate_template', True)\n",
            "\n",
            "Provide a name to a company according to the product it is developing\n",
            "\n",
            "Product: colourful socks\n",
            "Name: Cute Socks\n",
            "\n",
            "\n",
            "Product: AI Software\n",
            "Name: BrainAI\n",
            "\n",
            "Product: Suits\n",
            "Name: \n"
          ]
        }
      ],
      "source": [
        "## Setting Examples\n",
        "\n",
        "examples = [\n",
        "    {\"product\": \"colourful socks\", \"name\": \"Cute Socks\"},\n",
        "    {\"product\": \"AI Software\", \"name\": \"BrainAI\"},\n",
        "]\n",
        "\n",
        "## Creating the Prompt for each Example\n",
        "\n",
        "example_template = \"\"\"\n",
        "Product: {product}\n",
        "Name: {name}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    template = example_template,\n",
        "    input_variables = [\"product\", \"name\"]\n",
        ")\n",
        "\n",
        "## Creating the Few Shot Example Object\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = \"Provide a name to a company according to the product it is developing\",\n",
        "    suffix = \"Product: {input}\\nName: \",\n",
        "    input_variables = [\"input\"],\n",
        "    example_separator = \"\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "## Printing the Prompt\n",
        "\n",
        "for k in few_shot_prompt:\n",
        "    print(k)\n",
        "\n",
        "print()\n",
        "print(few_shot_prompt.format(input=\"Suits\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybgxBHC7VnQ"
      },
      "source": [
        "## Select Examples\n",
        "\n",
        "If you have a large number of examples, you can use the `ExampleSelector` to select a subset of examples that will be most informative for the Language Model.\n",
        "\n",
        "This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMAjr1Bk72Lv",
        "outputId": "754a3ba3-97da-4f29-94c6-658353ade9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('examples', [{'product': 'cars', 'name': 'carX'}, {'product': 'airplanes', 'name': 'BlueSky'}, {'product': 'batteries', 'name': 'Energetic'}, {'product': 'AI software', 'name': 'SonicAI'}, {'product': 'computers', 'name': 'FututeCMP'}])\n",
            "('example_prompt', PromptTemplate(input_variables=['product', 'name'], output_parser=None, partial_variables={}, template='\\nProduct: {product}\\nName: {name}\\n', template_format='f-string', validate_template=True))\n",
            "('get_text_length', <function _get_length_based at 0x7f97724dfac0>)\n",
            "('max_length', 25)\n",
            "('example_text_lengths', [6, 6, 6, 7, 6])\n",
            "\n",
            "('input_variables', ['input'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('examples', None)\n",
            "('example_selector', LengthBasedExampleSelector(examples=[{'product': 'cars', 'name': 'carX'}, {'product': 'airplanes', 'name': 'BlueSky'}, {'product': 'batteries', 'name': 'Energetic'}, {'product': 'AI software', 'name': 'SonicAI'}, {'product': 'computers', 'name': 'FututeCMP'}], example_prompt=PromptTemplate(input_variables=['product', 'name'], output_parser=None, partial_variables={}, template='\\nProduct: {product}\\nName: {name}\\n', template_format='f-string', validate_template=True), get_text_length=<function _get_length_based at 0x7f97724dfac0>, max_length=25, example_text_lengths=[6, 6, 6, 7, 6]))\n",
            "('example_prompt', PromptTemplate(input_variables=['product', 'name'], output_parser=None, partial_variables={}, template='\\nProduct: {product}\\nName: {name}\\n', template_format='f-string', validate_template=True))\n",
            "('suffix', 'Product: {input}\\nName: ')\n",
            "('example_separator', '\\n')\n",
            "('prefix', 'Provide a name to a company according to the product it is developing')\n",
            "('template_format', 'f-string')\n",
            "('validate_template', True)\n",
            "\n",
            "Provide a name to a company according to the product it is developing\n",
            "\n",
            "Product: cars\n",
            "Name: carX\n",
            "\n",
            "\n",
            "Product: airplanes\n",
            "Name: BlueSky\n",
            "\n",
            "Product: apples, oranges, lemons, computers, cars, airplanes, web-sites, all kind of software and books\n",
            "Name: \n"
          ]
        }
      ],
      "source": [
        "## Setting Examples\n",
        "\n",
        "examples = [\n",
        "    {\"product\": \"cars\", \"name\": \"carX\"},\n",
        "    {\"product\": \"airplanes\", \"name\": \"BlueSky\"},\n",
        "    {\"product\": \"batteries\", \"name\": \"Energetic\"},\n",
        "    {\"product\": \"AI software\", \"name\": \"SonicAI\"},\n",
        "    {\"product\": \"computers\", \"name\": \"FututeCMP\"},\n",
        "]\n",
        "\n",
        "## Creating Example Prompt\n",
        "example_template = \"\"\"\n",
        "Product: {product}\n",
        "Name: {name}\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    template = example_template,\n",
        "    input_variables = [\"product\", \"name\"]\n",
        ")\n",
        "\n",
        "## Creating an Example Selector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=25 # `get_text_length` function, default is `Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))`\n",
        "    # we can set ourselfs this function (e.g. according to the tokens)\n",
        ")\n",
        "\n",
        "for k in example_selector:\n",
        "    print(k)\n",
        "print()\n",
        "\n",
        "## Creating the final Template\n",
        "\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix = \"Provide a name to a company according to the product it is developing\",\n",
        "    suffix = \"Product: {input}\\nName: \",\n",
        "    input_variables = [\"input\"],\n",
        "    example_separator = \"\\n\"\n",
        ")\n",
        "\n",
        "for k in dynamic_prompt:\n",
        "    print(k)\n",
        "print()\n",
        "\n",
        "## Seeing in Action\n",
        "\n",
        "long_string = \"apples, oranges, lemons, computers, cars, airplanes, web-sites, all kind of software and books\"\n",
        "\n",
        "print(dynamic_prompt.format(input = long_string))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKMEOn2xIzyK"
      },
      "source": [
        "there are a lot more Example Selectors that we can use, some are `SemanticSimilarityExampleSelector`, `MaxMarginalRelevanceExampleSelector` and `NGramOverlapExampleSelector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZylJUKLoJDih",
        "outputId": "aefc85ac-b17d-451f-9b7e-a83ca7c8c6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "## We will see the `MaxMarginalRelevanceExampleSelector`\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "    examples = examples,\n",
        "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API),\n",
        "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    vectorstore_cls = FAISS,\n",
        "    # The number of examples to produce.\n",
        "    k = 2,\n",
        ")\n",
        "mmr_prompt = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"]\n",
        ")\n",
        "\n",
        "# Input is a feeling, so should select the happy/sad example as the first one\n",
        "print(mmr_prompt.format(adjective=\"worried\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21UR3tZbKlf0",
        "outputId": "ddd589f4-4808-4046-bea7-ea963c7347da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "# Let's compare this to what we would just get if we went solely off of similarity\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # The list of examples available to select from.\n",
        "    examples = examples,\n",
        "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API),\n",
        "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    vectorstore_cls = FAISS,\n",
        "    # The number of examples to produce.\n",
        "    k=2,\n",
        ")\n",
        "\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Input: {adjective}\\nOutput:\",\n",
        "    input_variables=[\"adjective\"],\n",
        ")\n",
        "print(similar_prompt.format(adjective=\"worried\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vg22V6I-4o8"
      },
      "source": [
        "## Generating Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lAfRz4F-TKs"
      },
      "outputs": [],
      "source": [
        "## Initializing a LLM\n",
        "\n",
        "davinci = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    openai_api_key=OPENAI_API\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z29XKB6i-TIF",
        "outputId": "c73ab0b2-0eff-4e69-8204-4557af94e890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intellimind Technologies.\n",
            "Speed Admirals.\n"
          ]
        }
      ],
      "source": [
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template = template,\n",
        "    input_variables = [\"product\"]\n",
        ")\n",
        "\n",
        "print(davinci(prompt.format(product = \"AI Software\")).strip())\n",
        "print(davinci(prompt.format(product = \"Super Cars\")).strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye5Rgw91-TDC",
        "outputId": "a8ec05a0-b6f2-4942-f414-fd051917ab1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Oh, so you're the snappy dresser type?\n"
          ]
        }
      ],
      "source": [
        "print(davinci(few_shot_prompt.format(query = \"I like to wear suits\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOswYVGZBNO0",
        "outputId": "ee37cb9e-c23d-40ad-c6f8-35ac5b8c8e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " AllThingsTech\n"
          ]
        }
      ],
      "source": [
        "long_string = \"apples, oranges, lemons, computers, cars, airplanes, web-sites, all kind of software and books\"\n",
        "\n",
        "print(davinci(dynamic_prompt.format(input = long_string)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyX1zIEaCcSJ"
      },
      "source": [
        "## Chat Prompt Template\n",
        "\n",
        "`Chat Models` takes a list of chat messages as input - this list commonly referred to as `prompt`. These chat messages differ from raw string (which you would pass into a LLM model) in that every message is associated with a `role`.\n",
        "\n",
        "A role can be AI, Human or System. he model is supposed to follow instruction from system chat message more closely.\n",
        "\n",
        "To create a message template associated with a role, you use `MessagePromptTemplate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zE2wpAXTBNMH"
      },
      "outputs": [],
      "source": [
        "## Creating System Prompt\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "## Creating a Human Prompt\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IJqyA-LKDyzA"
      },
      "outputs": [],
      "source": [
        "## More directly we can pass a Prompt Tempate to those Prompts\n",
        "\n",
        "prompt=PromptTemplate(\n",
        "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "    input_variables=[\"input_language\", \"output_language\"],\n",
        ")\n",
        "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
        "\n",
        "assert system_message_prompt == system_message_prompt_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie6bWaF4D_eX",
        "outputId": "84c0337d-09a6-4728-8e96-6554ca10459f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('input_variables', ['text', 'input_language', 'output_language'])\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('messages', [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], output_parser=None, partial_variables={}, template='You are a helpful assistant that translates {input_language} to {output_language}.', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='{text}', template_format='f-string', validate_template=True), additional_kwargs={})])\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}), HumanMessage(content='I love programming.', additional_kwargs={}, example=False)])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Then we can create a Chat Prompt Template from one or more Message Prompt Templates\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "## Printing the Prompt\n",
        "\n",
        "for k in chat_prompt:\n",
        "    print(k)\n",
        "print()\n",
        "\n",
        "## Completing the Prompt\n",
        "chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKIQ655uEpfG",
        "outputId": "b4ac3f91-43db-4793-f140-ec8715cccbd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi')"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Specify Role\n",
        "\n",
        "template = \"May the {subject} be with you\"\n",
        "\n",
        "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=template)\n",
        "\n",
        "chat_message_prompt.format(subject=\"force\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRIjjhbgG-g7",
        "outputId": "58ef5eea-69b9-4c78-9603-255d2800e30b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
              " HumanMessage(content='Hello, how are you doing?'),\n",
              " AIMessage(content=\"I'm doing well, thanks!\"),\n",
              " HumanMessage(content='What is your name?')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## A Complete Example\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "messages = template.format_messages(\n",
        "    name=\"Bob\",\n",
        "    user_input=\"What is your name?\"\n",
        ")\n",
        "\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FI5HXWTHA1c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
