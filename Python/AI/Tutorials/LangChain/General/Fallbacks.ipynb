{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qD2NPirSroUP"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "\n",
        "from openai.error import RateLimitError\n",
        "from unittest.mock import patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi_iV7n5rZJ9"
      },
      "source": [
        "## What is a Fallback\n",
        "\n",
        "When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why we've introduced the concept of fallbacks.\n",
        "\n",
        "A `fallback` is an alternative plan that may be used in an emergency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWyldbrqw_D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5dPz32erjJr"
      },
      "source": [
        "## Fallback for LLM API Errors\n",
        "\n",
        "This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf4m26OKrnU6",
        "outputId": "593cf4b4-06c0-4244-f3b3-f4074567d425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit error\n"
          ]
        }
      ],
      "source": [
        "# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc\n",
        "\n",
        "openai_llm = ChatOpenAI(max_retries=0, openai_api_key=open(\"openai_api.txt\").read())\n",
        "\n",
        "with patch('openai.ChatCompletion.create', side_effect=RateLimitError()):\n",
        "    try:\n",
        "         print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n",
        "    except:\n",
        "        print(\"Hit error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGIKyxDZsiX_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8k_qgctsiBZ"
      },
      "source": [
        "## Fallback for Long Inputs\n",
        "\n",
        "One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynSGcVUGsJvj",
        "outputId": "c1dc0910-671a-4a53-c028-78f66c87e109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This model's maximum context length is 4097 tokens. However, your messages resulted in 16012 tokens. Please reduce the length of the messages.\n"
          ]
        }
      ],
      "source": [
        "openai_llm = ChatOpenAI(openai_api_key=open(\"openai_api.txt\").read())\n",
        "\n",
        "inputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 4000)\n",
        "\n",
        "try:\n",
        "    print(openai_llm.invoke(inputs))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4he1N0Csvvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i2m6r0usz03"
      },
      "source": [
        "## Fallback to Better Model\n",
        "\n",
        "Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lN1VRnt-tGqx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
