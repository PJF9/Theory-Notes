{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mKNLyAMx8xG1"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage, AgentAction\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import (\n",
        "    AgentType,\n",
        "    initialize_agent,\n",
        "    load_tools\n",
        ")\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
        "from langchain.callbacks.base import AsyncCallbackHandler\n",
        "\n",
        "from typing import Dict, List, Union, Any\n",
        "import asyncio\n",
        "from langchain.schema import AgentAction, AgentFinish, LLMResult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "aWGFVsCDFcAz"
      },
      "outputs": [],
      "source": [
        "with open(\"openai_api.txt\", 'r') as f1, open(\"serpapi_api.txt\", 'r') as f2:\n",
        "    OPENAI_API = f1.read()\n",
        "    SERPAPI_API = f2.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5oUK6fP809E"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "LangChain porvides a `callback system` that allows us to hook into the various stages of the LLM application. This is usefull for **logging**, **monitoring**, **streaming**, and other stuff.\n",
        "\n",
        "We can subscribe to these events using the `callback` argument. There are two main callbacks mechanisms:\n",
        "* _Constructor Callbacks_ - defined in the constructor (LLMChain(callbacks=[handler])), which will be used for calls made on that object, and are scoped to that object only, (if we pass a handler to the LLMChain constructor, it will not be used by the model attached to that chain).\n",
        "\n",
        "* _Request callbacks_ - defined in the call()/run()/apply(), which will be used for that specific request only, and all sub-requests that it contains (a call to an LLMChain triggers a call to a Model, which uses the same handler passed through)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OjLmYhGfEavw"
      },
      "outputs": [],
      "source": [
        "## The base class of all Callback Handlers is the following\n",
        "\n",
        "class BaseCallbackHandler:\n",
        "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when LLM starts running.\"\"\"\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run when LLM ends running.\"\"\"\n",
        "\n",
        "    def on_llm_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when LLM errors.\"\"\"\n",
        "\n",
        "    def on_chain_start(\n",
        "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when chain starts running.\"\"\"\n",
        "\n",
        "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
        "        \"\"\"Run when chain ends running.\"\"\"\n",
        "\n",
        "    def on_chain_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when chain errors.\"\"\"\n",
        "\n",
        "    def on_tool_start(\n",
        "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when tool starts running.\"\"\"\n",
        "\n",
        "    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run when tool ends running.\"\"\"\n",
        "\n",
        "    def on_tool_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when tool errors.\"\"\"\n",
        "\n",
        "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run on arbitrary text.\"\"\"\n",
        "\n",
        "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run on agent action.\"\"\"\n",
        "\n",
        "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
        "        \"\"\"Run on agent end.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QWAj-IEEbA0"
      },
      "source": [
        "## When Using each Mechanism\n",
        "\n",
        "* _Constructor Callbacks_ are most usefull for use cases such as logging, monitoring, which are not specific to a single request, but rather to the entire chain.\n",
        "\n",
        "* _Request Callbacks_ are most usefull for use cases such as streaming, where we want to stream the output of a single request to a specific websocket connection, or other similar use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkBMO8SoFALC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JUAWJEUFBsM"
      },
      "source": [
        "## Using an Existing Handler\n",
        "\n",
        "LangChain provides a few built-in handlers that we can use to get started. Thess are avaailable in the `langchain/callbacks` module. The most basic handler is the `StdOutCallbackHandler`, which simply logs all events to `stdout`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "YjKRh_IBFjxQ",
        "outputId": "9eb50794-d018-423c-fc41-c3288f41f626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n3'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setting the Handler (Constructor Callback)\n",
        "handler = StdOutCallbackHandler()\n",
        "\n",
        "# Setting the Chain\n",
        "chain = LLMChain(\n",
        "    llm = OpenAI(openai_api_key=OPENAI_API),\n",
        "    prompt = PromptTemplate.from_template(\"1 + {number} = \"),\n",
        "    callbacks = [handler]\n",
        ")\n",
        "\n",
        "chain.run(number = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "vNPsjRuiGQLI",
        "outputId": "c46f067f-2781-4e28-b126-b72cd99666e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n3'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Setting the Verbose Chain\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm = OpenAI(openai_api_key=OPENAI_API),\n",
        "    prompt = PromptTemplate.from_template(\"1 + {number} = \"),\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "chain.run(number = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "FInOz1hXGUym",
        "outputId": "80a0da07-a300-4998-f96b-fb8f501ba928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n3'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Using the Handler as a Request Callback\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm = OpenAI(openai_api_key=OPENAI_API),\n",
        "    prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
        ")\n",
        "\n",
        "chain.run(number=2, callbacks=[handler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fnduZpU2Gnd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owlxo6ztHGZx"
      },
      "source": [
        "## Creating Custom Handler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ldwdRK9mHJMH"
      },
      "outputs": [],
      "source": [
        "class MyCustomHandler(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        print(f\"My custom handler, token: {token}\")\n",
        "\n",
        "# Enabling `streaming` mode\n",
        "chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()], openai_api_key=OPENAI_API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOI_t5L1HVfa",
        "outputId": "1118625b-95f3-470a-b3a8-10a58c6a628c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My custom handler, token: \n",
            "My custom handler, token: Why\n",
            "My custom handler, token:  did\n",
            "My custom handler, token:  the\n",
            "My custom handler, token:  tomato\n",
            "My custom handler, token:  turn\n",
            "My custom handler, token:  red\n",
            "My custom handler, token: ?\n",
            "\n",
            "\n",
            "My custom handler, token: Because\n",
            "My custom handler, token:  it\n",
            "My custom handler, token:  saw\n",
            "My custom handler, token:  the\n",
            "My custom handler, token:  salad\n",
            "My custom handler, token:  dressing\n",
            "My custom handler, token: !\n",
            "My custom handler, token: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the tomato turn red?\\n\\nBecause it saw the salad dressing!', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat([HumanMessage(content=\"Tell me a joke\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X9fi68aIa7L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg4Jm0OBIgx6"
      },
      "source": [
        "## Async Callbacks\n",
        "\n",
        "When using the asynchronous API it's recommended to use the `AsyncCallbackHandler` to avoind blocking the runloop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnYC9I3kIwnv",
        "outputId": "031ec533-dfd7-4ed9-d7a8-9a54008d9c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zzzz....\n",
            "Hi! I just woke up. Your llm is starting\n",
            "Sync handler being called in a `thread_pool_executor`: token: \n",
            "Sync handler being called in a `thread_pool_executor`: token: Why\n",
            "Sync handler being called in a `thread_pool_executor`: token:  did\n",
            "Sync handler being called in a `thread_pool_executor`: token:  the\n",
            "Sync handler being called in a `thread_pool_executor`: token:  tomato\n",
            "Sync handler being called in a `thread_pool_executor`: token:  turn\n",
            "Sync handler being called in a `thread_pool_executor`: token:  red\n",
            "Sync handler being called in a `thread_pool_executor`: token: ?\n",
            "Sync handler being called in a `thread_pool_executor`: token:  Because\n",
            "Sync handler being called in a `thread_pool_executor`: token:  it\n",
            "Sync handler being called in a `thread_pool_executor`: token:  saw\n",
            "Sync handler being called in a `thread_pool_executor`: token:  the\n",
            "Sync handler being called in a `thread_pool_executor`: token:  salad\n",
            "Sync handler being called in a `thread_pool_executor`: token:  dressing\n",
            "Sync handler being called in a `thread_pool_executor`: token: !\n",
            "Sync handler being called in a `thread_pool_executor`: token: \n",
            "zzzz....\n",
            "Hi! I just woke up. Your llm is ending\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text='Why did the tomato turn red? Because it saw the salad dressing!', generation_info=None, message=AIMessage(content='Why did the tomato turn red? Because it saw the salad dressing!', additional_kwargs={}, example=False))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MyCustomSyncHandler(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
        "\n",
        "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
        "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
        "\n",
        "    async def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> None:\n",
        "        \"\"\"Run when chain starts running.\"\"\"\n",
        "        print(\"zzzz....\")\n",
        "        await asyncio.sleep(2)\n",
        "        class_name = serialized[\"name\"]\n",
        "        print(\"Hi! I just woke up. Your llm is starting\")\n",
        "\n",
        "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "        \"\"\"Run when chain ends running.\"\"\"\n",
        "        print(\"zzzz....\")\n",
        "        await asyncio.sleep(2)\n",
        "        print(\"Hi! I just woke up. Your llm is ending\")\n",
        "\n",
        "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
        "# Additionally, we pass in a list with our custom handler\n",
        "chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()], openai_api_key=OPENAI_API)\n",
        "\n",
        "await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYf6ufd5I3WM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uva1uxXhJMPF"
      },
      "source": [
        "## Using Multiple Handlers\n",
        "\n",
        "In previous examples, we passed in callback handlers upon creating of an object suing `callback=[...]`. In this case, the callbacks will be scoped to that perticular object\n",
        "\n",
        "However in many cases, it's advantageous to pass in handlers instead when running the object. When we pass through `CallbackHandlers` using the `callbacks` keyword when executing a run, those callbacks will be issued by all nested objects involved in the execution. \n",
        "\n",
        "For example when a handler is passed through to an `Agent`, it will be used for all callbacks related to the agent and all the objects involved in the agent's execution (Tools, Chains and LLm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YmZxe-WoKRxW",
        "outputId": "0185d866-70f3-4e03-b8e9-8c87b1ce9e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "on_chain_start AgentExecutor\n",
            "on_chain_start LLMChain\n",
            "on_llm_start OpenAI\n",
            "on_llm_start (I'm the second handler!!) OpenAI\n",
            "on_new_token  I\n",
            "on_new_token  need\n",
            "on_new_token  to\n",
            "on_new_token  use\n",
            "on_new_token  a\n",
            "on_new_token  calculator\n",
            "on_new_token  to\n",
            "on_new_token  solve\n",
            "on_new_token  this\n",
            "on_new_token .\n",
            "on_new_token \n",
            "Action\n",
            "on_new_token :\n",
            "on_new_token  Calculator\n",
            "on_new_token \n",
            "Action\n",
            "on_new_token  Input\n",
            "on_new_token :\n",
            "on_new_token  2\n",
            "on_new_token ^\n",
            "on_new_token 0\n",
            "on_new_token .\n",
            "on_new_token 235\n",
            "on_new_token \n",
            "on_agent_action AgentAction(tool='Calculator', tool_input='2^0.235', log=' I need to use a calculator to solve this.\\nAction: Calculator\\nAction Input: 2^0.235')\n",
            "on_tool_start Calculator\n",
            "on_chain_start LLMMathChain\n",
            "on_chain_start LLMChain\n",
            "on_llm_start OpenAI\n",
            "on_llm_start (I'm the second handler!!) OpenAI\n",
            "on_new_token \n",
            "on_new_token ```text\n",
            "on_new_token \n",
            "\n",
            "on_new_token 2\n",
            "on_new_token **\n",
            "on_new_token 0\n",
            "on_new_token .\n",
            "on_new_token 235\n",
            "on_new_token \n",
            "\n",
            "on_new_token ```\n",
            "\n",
            "on_new_token ...\n",
            "on_new_token num\n",
            "on_new_token expr\n",
            "on_new_token .\n",
            "on_new_token evaluate\n",
            "on_new_token (\"\n",
            "on_new_token 2\n",
            "on_new_token **\n",
            "on_new_token 0\n",
            "on_new_token .\n",
            "on_new_token 235\n",
            "on_new_token \")\n",
            "on_new_token ...\n",
            "on_new_token \n",
            "\n",
            "on_new_token \n",
            "on_chain_start LLMChain\n",
            "on_llm_start OpenAI\n",
            "on_llm_start (I'm the second handler!!) OpenAI\n",
            "on_new_token  I\n",
            "on_new_token  now\n",
            "on_new_token  know\n",
            "on_new_token  the\n",
            "on_new_token  final\n",
            "on_new_token  answer\n",
            "on_new_token .\n",
            "on_new_token \n",
            "Final\n",
            "on_new_token  Answer\n",
            "on_new_token :\n",
            "on_new_token  1\n",
            "on_new_token .\n",
            "on_new_token 17\n",
            "on_new_token 690\n",
            "on_new_token 67\n",
            "on_new_token 372\n",
            "on_new_token 187\n",
            "on_new_token 674\n",
            "on_new_token \n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.1769067372187674'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MyCustomHandlerOne(BaseCallbackHandler):\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        print(f\"on_llm_start {serialized['name']}\")\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
        "        print(f\"on_new_token {token}\")\n",
        "\n",
        "    def on_llm_error(\n",
        "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        \"\"\"Run when LLM errors.\"\"\"\n",
        "\n",
        "    def on_chain_start(\n",
        "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        print(f\"on_chain_start {serialized['name']}\")\n",
        "\n",
        "    def on_tool_start(\n",
        "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
        "    ) -> Any:\n",
        "        print(f\"on_tool_start {serialized['name']}\")\n",
        "\n",
        "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
        "        print(f\"on_agent_action {action}\")\n",
        "\n",
        "\n",
        "class MyCustomHandlerTwo(BaseCallbackHandler):\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
        "    ) -> Any:\n",
        "        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n",
        "\n",
        "\n",
        "# Instantiating the handlers\n",
        "handler1 = MyCustomHandlerOne()\n",
        "handler2 = MyCustomHandlerTwo()\n",
        "\n",
        "# Seting up the agent (only the `llm` will issue callbacks for handler2)\n",
        "llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2], openai_api_key=OPENAI_API)\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")\n",
        "\n",
        "# Callbacks for handler1 will be issued by every object involved in the Agent execution (llm, llmchain, tool, agent executor)\n",
        "agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G96EJNe0Mu_P"
      },
      "source": [
        "## Streaming Only the Output\n",
        "\n",
        "By default LangChains assume that the token sequence `Final, ` `Answer`, `:` indecates that the agent has reached an answer. We can manually set the an answer sequence in the `answer_prefix_tokens` argument.\n",
        "\n",
        "For convenience, the callback automatically strips whitespaces and new line characters when comparing to `answer_prefix_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m1gN_nZK3EF",
        "outputId": "9583d50a-737b-4d20-efd5-8631d11bedd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Democracy is a form of government in which the people have the authority to deliberate and decide legislation, or to choose governing officials to do so. Examples of democracy include voting in elections and contacting elected officials."
          ]
        }
      ],
      "source": [
        "llm = OpenAI(temperature=0, streaming=True, openai_api_key=OPENAI_API, callbacks=[FinalStreamingStdOutCallbackHandler()])\n",
        "tools = load_tools([\"llm-math\", \"serpapi\"], llm=llm, serpapi_api_key=SERPAPI_API)\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")\n",
        "\n",
        "res = agent.run(\"Expain me what democracy is\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qZknkSm1QaFg",
        "outputId": "e4138cc8-6b00-4e2e-b286-01ea77aa3978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# I#\n",
            "# need#\n",
            "# to#\n",
            "# find#\n",
            "# out#\n",
            "# when#\n",
            "# he#\n",
            "# became#\n",
            "# Chancellor#\n",
            "#.#\n",
            "#\n",
            "Action#\n",
            "#:#\n",
            "# Wikipedia#\n",
            "#\n",
            "Action#\n",
            "# Input#\n",
            "#:#\n",
            "# Kon#\n",
            "#rad#\n",
            "# Aden#\n",
            "#auer#\n",
            "##\n",
            "# I#\n",
            "# now#\n",
            "# know#\n",
            "# that#\n",
            "# Kon#\n",
            "#rad#\n",
            "# Aden#\n",
            "#auer#\n",
            "# became#\n",
            "# Chancellor#\n",
            "# of#\n",
            "# Germany#\n",
            "# in#\n",
            "# 1949#\n",
            "#.#\n",
            "#\n",
            "Final#\n",
            "# Answer#\n",
            "#:#\n",
            "# Kon#\n",
            "#rad#\n",
            "# Aden#\n",
            "#auer#\n",
            "# became#\n",
            "# Chancellor#\n",
            "# of#\n",
            "# Germany#\n",
            "# in#\n",
            "# 1949#\n",
            "#,#\n",
            "# 74#\n",
            "# years#\n",
            "# ago#\n",
            "#.#\n",
            "##\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago.'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Seeing what the ideal `answer_prefix_tokens` is:\n",
        "\n",
        "class MyCallbackHandler(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token, **kwargs) -> None:\n",
        "        # print every token on a new line\n",
        "        print(f\"#{token}#\")\n",
        "\n",
        "llm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()], openai_api_key=OPENAI_API)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\n",
        "agent.run(\"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xOV1IEuQ28X"
      },
      "source": [
        "here is [`Final` `Answer`, `:`]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JEAHWrfIRFtw",
        "outputId": "041c38d8-e06e-489b-e2a4-73b5e5092655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Konrad Adenauer became Chancellor of Germany in 1949, which is 74 years ago from 2023."
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Konrad Adenauer became Chancellor of Germany in 1949, which is 74 years ago from 2023.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = OpenAI(streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=[\"Final\", \"Answer\", \":\"])], openai_api_key=OPENAI_API)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\n",
        "agent.run(\"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_rfQt1uRNbT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
