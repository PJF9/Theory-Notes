{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_api.txt\", \"r\") as f:\n",
    "    OPENAI_API = f.read()\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name = \"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")\n",
    "\n",
    "embedding_llm = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Over Documents with Chat History\n",
    "\n",
    "The difference between this Chain and `RetrivalQAChain` is that this allows for passing in of a history which can be used to allow for follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Docsearch Object\n",
    "\n",
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "text = text_splitter.split_documents(docs)\n",
    "\n",
    "docsearch = Chroma.from_documents(\n",
    "    documents = text,\n",
    "    embedding = embedding_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting Memory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting Chain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a different model for condensing the question\n",
    "\n",
    "This chain has two steps:\n",
    "1. It condenses the current question and the chat history into a standalone question. This is neccessary to create a standanlone vector to use for retrieval.\n",
    "2. It does retrieval and then answers the question using retrieval augmented generation with a separate model.\n",
    "\n",
    "We can use seperate models for those tasks, i.e., a cheaper model for task 1 and a more powerfull model for part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Condenses with Different Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting a `get history` function\n",
    "\n",
    "def get_chat_history(inputs: tuple) -> str:\n",
    "    res = []\n",
    "    for human, ai in inputs:\n",
    "        res.append(f\"Human:{human}\\nAI:{ai}\")\n",
    "    return \"\\n\".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = chat_llm,\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    condense_question_llm = llm,\n",
    "    get_chat_history = get_chat_history\n",
    ")\n",
    "\n",
    "## We have to set the `chat history` ourselfs\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What Advengers do?\"\n",
    "results = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updating `chat history`\n",
    "chat_history.append((query, results[\"answer\"]))\n",
    "\n",
    "query = \"Why can I trust them?\"\n",
    "results = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, results[\"answer\"]))\n",
    "\n",
    "query = \"What are they awards?\"\n",
    "results = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, results[\"answer\"]))\n",
    "\n",
    "query = \"To who I am reffering as 'they'?\"\n",
    "results = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RAG with Different Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the RAG Chain\n",
    "\n",
    "custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. At the end of standalone question add this 'Answer the question in German language.' If you do not know the answer reply with 'I am sorry'.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = chat_llm,\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    condense_question_prompt = CUSTOM_QUESTION_PROMPT,\n",
    "    memory = memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Did he mention who she succeeded\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Combine Documents Types\n",
    "\n",
    "We can also use different types of combine document chains with the ConversationalRetrievalChain chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain with `map_reduce`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    question_generator = question_generator,\n",
    "    combine_docs_chain = doc_chain\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationalRetrievalChain with Question Answering with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm=llm, chain_type=\"map_reduce\")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever = docsearch.as_retriever(),\n",
    "    question_generator = question_generator,\n",
    "    combine_docs_chain = doc_chain\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
