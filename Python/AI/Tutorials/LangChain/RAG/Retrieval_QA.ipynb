{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_api.txt\", \"r\") as f:\n",
    "    OPENAI_API = f.read()\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name = \"gpt-3.5-turbo-instruct\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")\n",
    "\n",
    "embedding_llm = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\",\n",
    "    openai_api_key = OPENAI_API\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Question/Answering\n",
    "\n",
    "This chain help us question answering over a Knowledge Base.\n",
    "\n",
    "Vectorstore Retriever Options:\n",
    "* There are two main ways to retrieve documents relevant to a query - **Similarity Search** and **Max Marginal Relevance Search** (MMR Search). Similarity Search is the default, but you can use MMR by adding the `search_type` parameter: _docsearch.as_retriever(search_type=\"mmr\")_\n",
    "\n",
    "* You can also modify the search by passing specific search arguments through the retriever to the search function, using the `search_kwargs` keyword argument.\n",
    "    * `k` defines how many documents are returned; defaults to 4.\n",
    "    * `score_threshold` allows you to set a minimum relevance for documents returned by the retriever, if you are using the \"similarity_score_threshold\" search type.\n",
    "    * `fetch_k` determines the amount of documents to pass to the MMR algorithm; defaults to 20.\n",
    "    * `lambda_mult` controls the diversity of results returned by the MMR algorithm, with 1 being minimum diversity and 0 being maximum. Defaults to 0.5.\n",
    "    * `filter` allows you to define a filter on what documents should be retrieved, based on the documents' metadata. This has no effect if the Vectorstore doesn't store any metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the Docsearch Object\n",
    "\n",
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "text = text_splitter.split_documents(docs)\n",
    "\n",
    "docsearch = Chroma.from_documents(\n",
    "    documents = text,\n",
    "    embedding = embedding_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the QA Chain\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What Advengers do?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
