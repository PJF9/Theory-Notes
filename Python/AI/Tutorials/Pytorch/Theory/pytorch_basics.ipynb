{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ywsauAAGwwFW"
      },
      "source": [
        "## What are Tensors?\n",
        "Tensors can be anything, every representation of numbers, like scalars, vectors, matrixes and also a multidimensional arrays. Pytorch has a lot of functions and methods that applies on tensors and gives us more functionality to our programms."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p_tlccitydQd"
      },
      "source": [
        "## Importing Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhamKEDRym3z"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iQFw7cqJz_Hs"
      },
      "source": [
        "## Pytorch Scalars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xInbpn4zh1a",
        "outputId": "f5606e92-681e-4ffd-a080-5642fa007da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0\n",
            "torch.Size([]) torch.Size([])\n",
            "torch.int64 torch.float32\n",
            "9 <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "# We can create scalar tensors:\n",
        "t1 = torch.tensor(9)\n",
        "t2 = torch.tensor(9.99)\n",
        "\n",
        "# We can see that those are tensors is a scalar using the attribute 'ndim'\n",
        "print(t1.ndim, t2.ndim)\n",
        "\n",
        "# Other than 'ndim' we can use the attribute 'shape' (equivelant with 'size()')\n",
        "print(t1.shape, t2.shape)\n",
        "\n",
        "# Can also get the data type of a tensor's items\n",
        "print(t1.dtype, t2.dtype)\n",
        "\n",
        "# To get the data of a tensor and convert it in a Python class we use 'item()'\n",
        "print(t1.item(), type(t1.item()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3GLmVwk80ZRl"
      },
      "source": [
        "## Pytorch Higher Dimensions Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_afUxX-w0eWE",
        "outputId": "7dd7c884-9104-4da8-a579-86e3720c6a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([3])\n",
            "torch.float32\n",
            "torch.float32\n",
            "[3, 6, 9]\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# We can create vectors using:\n",
        "t1 = torch.tensor([3, 6, 9])\n",
        "\n",
        "# Printing the shape and the number of dimensions of this tensor:\n",
        "print(t1.ndim, t1.shape)\n",
        "\n",
        "# We can create a tensor with specific data type (we can see all data types available in the link above)\n",
        "t2 = torch.tensor([3, 6, 9], dtype=torch.float32)\n",
        "\n",
        "print(t2.dtype)\n",
        "\n",
        "# In the special case that some element of a tensor is a higher data type (like double in a tensor with ints), then all the other\n",
        "#   elements will became automaticly the higher data type\n",
        "t3 = torch.tensor([3, 6, 9.])\n",
        "\n",
        "print(t3.dtype)\n",
        "\n",
        "# To convert a Tensor to a Python List we use (can also be used for higher dimensions Tensors):\n",
        "l1 = t1.tolist()\n",
        "\n",
        "print(l1)\n",
        "\n",
        "# To get the number of elements in a Tensor we use:\n",
        "print(t1.numel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3A3w2PW3wou",
        "outputId": "48486cf4-e782-4474-dd79-87e973e50092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3, 6, 9], [13, 16, 19]]\n",
            "2 torch.Size([2, 3])\n",
            "torch.Size([2, 3, 1])\n",
            "Invalid Dimensions...\n"
          ]
        }
      ],
      "source": [
        "# We can create higher dimesnion tensors:\n",
        "t1 = torch.tensor([[3, 6, 9], [13, 16, 19]])\n",
        "\n",
        "# Printing the number of dimension and the shape of the tensor\n",
        "print(t1.ndim, t1.shape)\n",
        "\n",
        "# How shape works?\n",
        "#   First we need to see how many dimensions the tensor has. That will be the length of 'shape' tensor\n",
        "#   As first element this tensor have the elements (or the subtensors) of the higher dimension, as\n",
        "#   second element the tensors of the second higher dimension and so on\n",
        "# We should add that this 'shape' tensor is an iterable\n",
        "\n",
        "# Another example is:\n",
        "t2 = torch.tensor([[[3], [6], [9]], [[9], [12], [15]]])\n",
        "\n",
        "# This tensor has shape: 2x3x1\n",
        "print(t2.shape)\n",
        "\n",
        "# If a tensor that we create has invalid (not matching) dimensions then Pytorch raise ValueError\n",
        "try:\n",
        "    t3 = torch.tensor([[3], [6, 9]])\n",
        "except ValueError:\n",
        "    print(\"Invalid Dimensions...\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBlmTfk7UT5"
      },
      "source": [
        "## Tensor Indexing and Slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7CiZwa7Xe5",
        "outputId": "1ec307ae-1f45-4cff-a195-0bd4620d81f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 3])\n",
            "tensor([[ 3,  6,  9],\n",
            "        [ 6,  9, 12]])\n",
            "tensor([3, 6, 9])\n",
            "tensor(9)\n",
            "tensor([3, 6])\n",
            "tensor([3, 6])\n"
          ]
        }
      ],
      "source": [
        "# Creating a 1x2x3 tensor\n",
        "t = torch.tensor([[[3, 6, 9], [6, 9, 12]]])\n",
        "\n",
        "print(t.shape)\n",
        "\n",
        "# Indexing works like `numpy` arrays\n",
        "print(t[0])       # accessing the 2x3 tensor\n",
        "print(t[0, 0])    # accessing the first vector-tensor\n",
        "print(t[0, 0, 2]) # accessing the last element of that vector-tensor\n",
        "\n",
        "# Slicing works also like `numpy` arrays\n",
        "print(t[0][:, 0])   # accessing the first elements of the 2 vector-tensors\n",
        "print(t[0][0, 0:2]) # accessing the first 2 elements of the first vector-tensor\n",
        "\n",
        "# This correlation is due to the fact that Pytorch Tensors are in fact Numpy Arrays that can be run in GPU"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TZsobVbF-xfD"
      },
      "source": [
        "## Convention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JLnTZrr-y30"
      },
      "outputs": [],
      "source": [
        "# Typically we write the identifiers of scalar and vector Tensors with lowercase names and the higher\n",
        "#   dimension Tensors with upeprcase\n",
        "\n",
        "t1 = torch.tensor(9.)\n",
        "t2 = torch.tensor([3, 6, 9])\n",
        "T1 = torch.tensor([[3, 6], [6, 9]])\n",
        "T2 = torch.tensor([[[3], [6]], [[9], [12]], [[15], [18]]]) # 3x2x1 Tensor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hznsri_zMOKF"
      },
      "source": [
        "## Changing Data Type of a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxajql2RMRdd",
        "outputId": "2c84ac21-732c-4a22-e657-c0141b1a73eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3, 6, 9], dtype=torch.int16) torch.int16\n",
            "tensor([9., 6., 9.]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.tensor([3, 6, 9], dtype=torch.int16)\n",
        "\n",
        "print(t1, t1.dtype)\n",
        "\n",
        "# We can change that data type of a Tensor using (don't share memory locations):\n",
        "t2 = t1.type(torch.float32)\n",
        "\n",
        "print(t2, t2.dtype)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dNoq6d4D_rCx"
      },
      "source": [
        "## Random Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWx3jGKm_sff",
        "outputId": "f8806165-623c-49d6-bec1-7e918a26ed6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 4]) tensor([[[0.2677, 0.8256, 0.2678, 0.5786],\n",
            "         [0.9565, 0.2914, 0.1297, 0.1660],\n",
            "         [0.6565, 0.2139, 0.5560, 0.3178]]])\n",
            "torch.Size([1, 3, 4]) tensor([[[0.5156, 0.0890, 0.2774, 0.6858],\n",
            "         [0.4877, 0.4849, 0.2816, 0.2211],\n",
            "         [0.5426, 0.3132, 0.7145, 0.1043]]])\n",
            "torch.Size([3, 244, 235])\n",
            "tensor([[ 2.3907, -2.0185,  0.2982],\n",
            "        [ 2.5956, -1.6334, -0.9039],\n",
            "        [-0.9368,  0.3874,  0.8656]])\n",
            "tensor([[4, 8],\n",
            "        [8, 5]])\n",
            "tensor([5, 0, 7, 1, 6, 2, 8, 3, 4])\n",
            "tensor([0.6036, 0.0122, 0.5503, 0.7819, 0.5431, 0.2678, 0.8170, 0.7145, 0.5579])\n"
          ]
        }
      ],
      "source": [
        "# Learning to create Tensors with `random` numbers is essential in ML algorithms because at the beginning of learning\n",
        "#   we should initialize the weights (the rule finders) with random numbers and through the features and labels we will\n",
        "#   correct those random numbers in the appropriate rules.\n",
        "\n",
        "# Create a random Tensor, according to the uniform distribution [0, 1)\n",
        "t1 = torch.rand(1, 3, 4)  # As argument we pass the shape\n",
        "\n",
        "print(t1.shape, t1)\n",
        "\n",
        "# The 'shape' of the Tensor can be passed using 'size=(...)'\n",
        "t2 = torch.rand(size=(1, 3, 4))\n",
        "\n",
        "print(t2.shape, t2)\n",
        "\n",
        "# So for example if we have an RGB image of 244x235 bits we can create the weight Tensor as follows:\n",
        "t3 = torch.rand(size=(3, 244, 235)) # Where 3: number of channels, 244: height, 235: width\n",
        "\n",
        "print(t3.shape)\n",
        "\n",
        "\n",
        "# To create a random Tensor based on the standard Distribution (mean=0, variance=1) we use:\n",
        "t4 = torch.randn(size=(3, 3))\n",
        "\n",
        "print(t4)\n",
        "\n",
        "\n",
        "# To create a random Tensor that contains random intagers from n1 (default 0) to n2 we use [n1, n2):\n",
        "t4 = torch.randint(3, 9, size=(2, 2))\n",
        "\n",
        "print(t4)\n",
        "\n",
        "\n",
        "# To create a random vector-tensor with all intagers for 0 to n-1 we use:\n",
        "t5 = torch.randperm(9)\n",
        "\n",
        "print(t5)\n",
        "\n",
        "\n",
        "# We can also set manual the seed:\n",
        "gen = torch.Generator()\n",
        "gen.manual_seed(36912151821)\n",
        "\n",
        "t5 = torch.rand(size=(9,), generator=gen)\n",
        "\n",
        "print(t5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nHT0ujsmEyMg"
      },
      "source": [
        "## Empty-Ones-Zeros Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlUv-G7BE3um",
        "outputId": "d7813aab-da41-47e0-e33f-99ac2df8aace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2]) tensor([[-1.2252e-34,  4.5860e-41],\n",
            "        [-1.2252e-34,  4.5860e-41]])\n",
            "tensor([[1., 1., 1.]])\n",
            "tensor([[0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# The default data type of those Tensors is 'float32'\n",
        "\n",
        "# Creating an empty-tensor\n",
        "t1 = torch.empty(9)\n",
        "t2 = torch.empty(size=(2, 2))\n",
        "\n",
        "print(t2.shape, t2)\n",
        "\n",
        "\n",
        "# Creating ones-tensor\n",
        "t3 = torch.ones(3)\n",
        "t4 = torch.ones(size=(1, 3))\n",
        "\n",
        "print(t4)\n",
        "\n",
        "\n",
        "# Creating zeros-tensor\n",
        "t5 = torch.zeros(3)\n",
        "t6 = torch.zeros(size=(1, 3))\n",
        "\n",
        "print(t6)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aSS8uOebIWQy"
      },
      "source": [
        "## Eye-Full Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udpYR5bAIXno",
        "outputId": "fe6c7261-fcc3-4716-a46b-c80e0352ac36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9, 9]) tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
            "tensor([[3, 3],\n",
            "        [3, 3]])\n"
          ]
        }
      ],
      "source": [
        "# To create the identical Matrix nxn we use:\n",
        "t1 = torch.eye(9)\n",
        "\n",
        "print(t1.shape, t1)\n",
        "\n",
        "# To create a Tensor of custom dimensions with the same element:\n",
        "t2 = torch.full(size=(2, 2), fill_value=3)\n",
        "print(t2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_sYnXhOSG95d"
      },
      "source": [
        "## Range Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoaZ9DIJHADJ",
        "outputId": "761f95f9-9353-4125-80dd-dd147cadc331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3, 4, 5, 6, 7, 8, 9]) tensor([1, 3, 5, 7, 9])\n",
            "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
            "         0.7778,  1.0000])\n"
          ]
        }
      ],
      "source": [
        "# To create the range [n1, n2) we use (default n1=0):\n",
        "t1 = torch.arange(3, 10)\n",
        "t2 = torch.arange(start=1, end=10, step=2) # With step\n",
        "\n",
        "print(t1, t2)\n",
        "\n",
        "# To create a range Tensor [n1, n2) with n3 elements we use:\n",
        "t3 = torch.linspace(start=-1, end=1, steps=10)\n",
        "\n",
        "print(t3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kIJNLK0EJF2a"
      },
      "source": [
        "## Like Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zIP9nYUJHO_",
        "outputId": "e6ec6200-067d-4250-96e9-22d798f02f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "torch.Size([3, 3]) tensor([[ 0.6418,  0.8502,  0.8367],\n",
            "        [-0.7127, -1.4369, -0.5261],\n",
            "        [-1.0469, -1.0671,  0.9048]])\n"
          ]
        }
      ],
      "source": [
        "# We can create a Tensor base on the shape of another Tensor\n",
        "t1 = torch.rand(size=(3, 3))\n",
        "t2 = torch.ones(size=(9,))\n",
        "\n",
        "t3 = torch.zeros_like(input=t2)\n",
        "t4 = torch.randn_like(input=t1)\n",
        "\n",
        "print(t3.shape, t3)\n",
        "print(t4.shape, t4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGoVyXUJxEo"
      },
      "source": [
        "## Copying Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TopM-aJcJy2m",
        "outputId": "515146c5-9f87-4980-b37c-630edc323629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 3,  6,  9],\n",
            "        [12, 15, 18]]) tensor([[ 3,  6,  9],\n",
            "        [12, 15, 18]])\n",
            "tensor([[ 3,  6,  9],\n",
            "        [12, 15, 18]]) tensor([[ 3,  6,  9],\n",
            "        [ 9, 15, 18]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.tensor([[3, 6, 9], [12, 15, 18]])\n",
        "\n",
        "# We can deep-copy a Tensor using:\n",
        "t2 = t1.detach().clone()\n",
        "\n",
        "print(t1, t2)\n",
        "\n",
        "t2[1][0] = torch.tensor(9)\n",
        "\n",
        "print(t1, t2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o6aYHFdiwwFZ"
      },
      "source": [
        "\n",
        "## Reshaping Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPHtQUBENwGp",
        "outputId": "8b5865c1-71c5-454a-bf70-901b7a645088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 3, 12],\n",
            "        [ 6, 15],\n",
            "        [ 9, 18]])\n",
            "tensor([[[ 3,  6],\n",
            "         [ 9, 12],\n",
            "         [15, 18]]])\n",
            "tensor([ 3,  6,  9, 12, 15, 18])\n",
            "tensor([[[ 3,  6],\n",
            "         [ 9, 12],\n",
            "         [15, 18]]])\n",
            "tensor([ 3,  6,  9, 12, 15, 18])\n",
            "tensor([[ 9,  6,  9],\n",
            "        [12, 15, 18]])\n",
            "tensor([[ 9,  6,  9],\n",
            "        [12, 15, 18]])\n",
            "tensor([ 9,  6,  9, 12, 15, 18])\n",
            "tensor([[ 9,  6],\n",
            "        [ 9, 12],\n",
            "        [15, 18]])\n",
            "tensor([ 9,  6,  9, 12, 15, 18])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([[3, 6, 9], [12, 15, 18]])\n",
        "\n",
        "# We can get the Transpose Tensor using:\n",
        "print(t.T)\n",
        "\n",
        "# We can reshape the tensor, but the new dimension must be compatable with the original (the reshaped Tensor is a shallow copy of original)\n",
        "print(t.reshape(1, 3, 2))\n",
        "print(t.reshape(6))\n",
        "# Similar we can do:\n",
        "print(t.view(1, 3, 2))\n",
        "print(t.view(6))\n",
        "\n",
        "# The issue with shallow copy\n",
        "a = t.reshape(6)\n",
        "a[0] = 9\n",
        "print(t)\n",
        "a = torch.tensor([3])\n",
        "print(t)\n",
        "\n",
        "# We can convert a Tensor into a vector-tensor (returns a view):\n",
        "print(t.flatten())\n",
        "\n",
        "# If we want Pytorch to handle the exact dimensions that need to be passed so the new Tensor to be compatable with the original:\n",
        "print(t.reshape(3, -1))\n",
        "print(t.reshape(-1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XWQ2JebP2ltl"
      },
      "source": [
        "## Stacking Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTTtRJlo2n7S",
        "outputId": "c43098f8-188d-46b2-8970-85ce6a3f9a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 3,  6],\n",
            "         [ 9, 12]],\n",
            "\n",
            "        [[ 3,  6],\n",
            "         [ 9, 12]],\n",
            "\n",
            "        [[ 3,  6],\n",
            "         [ 9, 12]]])\n",
            "tensor([[ 3,  6],\n",
            "        [ 9, 12],\n",
            "        [ 3,  6],\n",
            "        [ 9, 12],\n",
            "        [ 3,  6],\n",
            "        [ 9, 12]])\n",
            "tensor([[[ 3,  6],\n",
            "         [ 3,  6],\n",
            "         [ 3,  6]],\n",
            "\n",
            "        [[ 9, 12],\n",
            "         [ 9, 12],\n",
            "         [ 9, 12]]])\n",
            "tensor([[ 3,  6,  3,  6,  3,  6],\n",
            "        [ 9, 12,  9, 12,  9, 12]])\n",
            "tensor([[[ 3,  3,  3],\n",
            "         [ 6,  6,  6]],\n",
            "\n",
            "        [[ 9,  9,  9],\n",
            "         [12, 12, 12]]])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([[3, 6], [9, 12]])\n",
        "\n",
        "# To vertically stack 2 or more Tensors we do:\n",
        "print(torch.stack((t, t, t), dim=0)) # works like t = [t, t, t]: ndim = 3\n",
        "print(torch.vstack((t, t, t)))       # The same but reducing the ndim to 2\n",
        "\n",
        "# To horizontally stacj 2 or more Tensors we do:\n",
        "print(torch.stack((t, t, t), dim=1)) # works like t = [[t[0], t[0], t[0]], [t[1], t[1], t[1]]]\n",
        "print(torch.hstack((t, t, t)))       # The same but reducing the ndim to 2\n",
        "\n",
        "# Another stacking method is to stack element-wise\n",
        "print(torch.dstack((t, t, t)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5oNMW7oC5hA9"
      },
      "source": [
        "## Squeeze and Unsqueeze Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjotufd55k01",
        "outputId": "2d9122c3-a4e6-469f-8975-4fdb9afe9934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3, 6, 9]]) torch.Size([1, 3])\n",
            "tensor([3, 6, 9])\n",
            "tensor([[3],\n",
            "        [6],\n",
            "        [9]])\n",
            "tensor([[3, 6, 9]])\n"
          ]
        }
      ],
      "source": [
        "# We can remove and add 1 dimensions to our Tensors\n",
        "t = torch.tensor([[3, 6, 9]])\n",
        "\n",
        "print(t, t.shape)\n",
        "\n",
        "t = t.reshape((3))\n",
        "\n",
        "print(t.squeeze())        # Remove all the single dimensions (1d)\n",
        "print(t.unsqueeze(dim=1)) # Add one dimension alongside axis 1\n",
        "print(t.unsqueeze(dim=0)) # Add one dimension alongside axis 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kBRuHDLB7KM7"
      },
      "source": [
        "## Permute the Dimensions of a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05eMO-Dg7Mb0",
        "outputId": "d84efb08-9cb2-4db6-9a88-cd7544303294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 3],\n",
            "         [ 6],\n",
            "         [ 9]],\n",
            "\n",
            "        [[ 6],\n",
            "         [ 9],\n",
            "         [12]]])\n",
            "tensor([[[ 3],\n",
            "         [ 6]],\n",
            "\n",
            "        [[ 6],\n",
            "         [ 9]],\n",
            "\n",
            "        [[ 9],\n",
            "         [12]]])\n",
            "tensor([[[ 3,  6,  9],\n",
            "         [ 6,  9, 12]]])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([[[3, 6], [6, 9], [9, 12]]]) # shape 1x3x2\n",
        "\n",
        "# We can rearange the dimensions of a Tensor using (returns a view):\n",
        "print(t.permute(2, 1, 0)) # shape: 2x3x1\n",
        "print(t.permute(1, 2, 0)) # shape: 3x2x1\n",
        "print(t.permute(0, 2, 1)) # shape: 1x2x3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yn_0GT-Z8bwY"
      },
      "source": [
        "## Mathematical Operations with Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jBJmf9ZX36g",
        "outputId": "ffc4a163-82e6-44bf-e0c0-ad6a1a3c611b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 6,  9, 12])\n",
            "tensor([ 9, 18, 27])\n",
            "tensor([12, 12, 12])\n",
            "tensor([-6,  0,  6])\n",
            "tensor([27, 36, 27])\n",
            "tensor([0.3333, 1.0000, 3.0000])\n",
            "tensor([3, 0, 0])\n",
            "tensor(90)\n",
            "tensor(90)\n",
            "tensor([1.7321, 2.4495, 3.0000])\n",
            "tensor([3, 6, 9])\n",
            "tensor([-3, -6, -9])\n",
            "tensor([3, 6, 9])\n",
            "tensor([3, 6, 9])\n",
            "tensor([ 9, 36, 81])\n",
            "tensor(18)\n",
            "tensor(18)\n",
            "tensor(162)\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.tensor([3, 6, 9])\n",
        "t2 = torch.tensor([9, 6, 3])\n",
        "\n",
        "# The operations between Tensors and scalars are element-wise operation (work with all operators):\n",
        "print(t1 + 3)\n",
        "print(t1 * 3)\n",
        "\n",
        "# All those methods can be called from a Tensor, by adding the suffix '_' and then passing the appropriate arguments\n",
        "\n",
        "# For performing operations between 2 Tensors we use:\n",
        "print(torch.add(t1, t2))         # + element-wise\n",
        "print(torch.sub(t1, t2))         # - element-wise\n",
        "print(torch.mul(t1, t2))         # * element-wise\n",
        "print(torch.div(t1, t2))         # / element-wise\n",
        "print(torch.fmod(t1, t2))        # % element-wise\n",
        "print(torch.dot(t1, t2))         # dot-product of 2 vector-tensors\n",
        "print(torch.matmul(t1, t2))      # @, mm the dot-product of 2 tensors\n",
        "\n",
        "print(torch.sqrt(t1))            # square root\n",
        "print(torch.abs(t1))             # |t|\n",
        "print(torch.neg(t1))             # -t\n",
        "print(torch.ceil(t1))            # ceil\n",
        "print(torch.floor(t1))           # floor\n",
        "\n",
        "print(torch.pow(t1, exponent=2)) # ** exponent\n",
        "print(torch.sum(t1))             # Σ\n",
        "print(torch.nansum(t1))          # Σ with nan = 0\n",
        "print(torch.prod(t1))            # Π"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cgwRIi0DnZt-"
      },
      "source": [
        "## Mathematical Functions to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uZmEWTgnd0L",
        "outputId": "d4e8494e-02ab-4636-a710-cd93dc265cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.9900,  0.9602, -0.9111])\n",
            "tensor([ 0.1411, -0.2794,  0.4121])\n",
            "tensor([-0.1425, -0.2910, -0.4523])\n",
            "tensor([  10.0677,  201.7156, 4051.5420])\n",
            "tensor([  10.0179,  201.7132, 4051.5420])\n",
            "tensor([0.9951, 1.0000, 1.0000])\n",
            "tensor([0.0000, 1.5708, 3.1416, 4.7124, 6.2832])\n",
            "tensor([  0.,  90., 180., 270., 360.])\n",
            "tensor([  20.0855,  403.4288, 8103.0840])\n",
            "tensor([  8.,  64., 512.])\n",
            "tensor([1.0986, 1.7918, 2.1972])\n",
            "tensor([1.5850, 2.5850, 3.1699])\n",
            "tensor([0.4771, 0.7782, 0.9542])\n",
            "tensor([0.9526, 0.9975, 0.9999])\n",
            "tensor(6.)\n",
            "tensor(6.)\n",
            "tensor(3.)\n",
            "tensor(9.)\n",
            "(tensor(3.), tensor(6.))\n",
            "(tensor(9.), tensor(6.))\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([3, 6, 9])\n",
        "\n",
        "# Trigonometric Functions\n",
        "print(torch.cos(t))\n",
        "print(torch.sin(t))\n",
        "print(torch.tan(t))\n",
        "print(torch.cosh(t))\n",
        "print(torch.sinh(t))\n",
        "print(torch.tanh(t))\n",
        "# Converting degrees to rads\n",
        "print(torch.deg2rad(torch.tensor([0, 90, 180, 270, 360], dtype=torch.float32)))\n",
        "print(torch.rad2deg(torch.tensor([0, torch.pi/2, torch.pi, 3*torch.pi/2, 2*torch.pi], dtype=torch.float32)))\n",
        "\n",
        "# Exponential and Logarithic Functions\n",
        "print(torch.exp(t))   # e ^ t\n",
        "print(torch.exp2(t))  # 2 ^ t\n",
        "print(torch.log(t))   # ln(t)\n",
        "print(torch.log2(t))\n",
        "print(torch.log10(t))\n",
        "\n",
        "# The sigmoid Function\n",
        "print(torch.sigmoid(t))\n",
        "\n",
        "# Probability Functions: need floating point numbers\n",
        "t = t.type(torch.float32)\n",
        "print(torch.mean(t))\n",
        "print(torch.median(t))   # The miidle value in the sorted Tensor t\n",
        "print(torch.std(t))      # Standard Deviation (σ) of elements from mean\n",
        "print(torch.var(t))      # Variance (σ^2) of elements from mean\n",
        "print(torch.std_mean(t)) # The tuple that contains std and mean\n",
        "print(torch.var_mean(t)) # The tuple that contains var and mean\n",
        "\n",
        "# Pytorch also support Fourier Transformations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXNy8NjrXY4"
      },
      "source": [
        "## Other Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQh6FEhFrY47",
        "outputId": "e01a50dc-f6b9-4159-bcd2-2441d76215c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(18)\n",
            "tensor(4)\n",
            "tensor(3)\n",
            "tensor(2)\n",
            "tensor(6)\n",
            "tensor([ True, False, False])\n",
            "tensor([False, False,  True])\n",
            "tensor([[2, 1, 0],\n",
            "        [2, 0, 1]])\n",
            "tensor([[ 9,  6,  3],\n",
            "        [18, 15, 12]]) tensor([[0, 1, 2],\n",
            "        [1, 0, 2]])\n"
          ]
        }
      ],
      "source": [
        "t = torch.tensor([[9, 6, 3], [15, 18, 12]])\n",
        "t1 = torch.tensor([float(\"inf\"), 9, 3])\n",
        "t2 = torch.tensor([3, 9, float(\"nan\")])\n",
        "\n",
        "# Minimum and Maximum values of a Tensor\n",
        "print(torch.max(t))    # converting the Tensor into a vector-tensor\n",
        "print(torch.argmax(t)) # the index of the maximum in the vector-tensor\n",
        "print(torch.min(t))\n",
        "print(torch.argmin(t))\n",
        "\n",
        "# Returning how many non zero element exists in the Tensor\n",
        "print(torch.count_nonzero(t))\n",
        "\n",
        "# Return boolean-tensor\n",
        "print(torch.isinf(t1))\n",
        "print(torch.isnan(t2))\n",
        "\n",
        "# Return the correct indexes of each element on the sorted version of Tensor\n",
        "print(torch.argsort(t))\n",
        "\n",
        "# Sorting a Tensor\n",
        "sorted_t, indexes = torch.sort(input=t, dim=1, descending=True) # dim=1 for sorting each row and dim=0 for sorting each column\n",
        "print(sorted_t, indexes)                                        # descending=True for descending sorting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WnOuACphuIHL"
      },
      "source": [
        "## Conditional Operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xitMcGsTuQ85",
        "outputId": "6e1c7fcc-6424-4bea-ddf9-9496e4d145f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ True, False,  True])\n",
            "tensor([False,  True, False])\n",
            "tensor([False, False, False])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.tensor([3, 6, 9])\n",
        "t2 = torch.tensor([3, 9, 9])\n",
        "\n",
        "print(t1 == t2) # torch.eq(t1, t2)\n",
        "print(t1 != t2) # torch.ne(t1, t2)\n",
        "print(t1 > t2)  # torch.lt(t1, t2)\n",
        "\n",
        "# We have available by Pytorch all the other operators"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAi0a5lwwFc"
      },
      "source": [
        "## Interoperability with Numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLtzz0SIwwFc",
        "outputId": "956eb90f-3d7d-4afa-e48c-0779ad3f5708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3., 6., 9.]) <class 'torch.Tensor'>\n",
            "tensor([3, 6, 9]) <class 'torch.Tensor'>\n",
            "tensor([3, 6, 9], dtype=torch.int16) <class 'torch.Tensor'>\n",
            "[3 6 9] <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# The difference of Pytorch's Tesnors and Numpy's Arrays is that the last only works on cpu.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "a = np.array([3, 6, 9])\n",
        "t = torch.tensor([3, 6, 9])\n",
        "\n",
        "t1 = torch.as_tensor(a, dtype=torch.float32) # Converting an object to Tensor (don't share memory)\n",
        "t2 = torch.from_numpy(a)                     # Converting a numpy array into a Tensor (takes not any other arg) (share memory)\n",
        "t3 = torch.from_numpy(a).type(torch.int16)   # Converting a numpy array into a Tensot and changing the data type (don't share memory)\n",
        "a1 = t.numpy()                               # Converting a Tensor into a numpy array (share memory)\n",
        "\n",
        "print(t1, type(t1))\n",
        "print(t2, type(t2))\n",
        "print(t3, type(t3))\n",
        "print(a1, type(a1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U_23a6sKwwFd"
      },
      "source": [
        "## Tensor Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgrCnkeKwwFd",
        "outputId": "19bd8d56-2772-4f6a-c0fd-00a7b6041771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "cpu\n",
            "cuda:0\n",
            "cpu\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# We can make numerical computations between Tensors either on CPU or GPU\n",
        "# The attribute that shows us where a Tensor is allocated is 'device' or 'get_device()'\n",
        "\n",
        "t1 = torch.tensor([3, 6, 9]) # default device 'cpu'\n",
        "\n",
        "print(t1.device)\n",
        "\n",
        "# We can set the device when initializing the Tensor object\n",
        "t2 = torch.tensor([3, 6, 9], device=\"cpu\") # the same as 'device=None'\n",
        "\n",
        "print(t2.device)\n",
        "\n",
        "# To enable GPU on Google Colab: Runtime->Change runtime type\n",
        "\n",
        "t3 = torch.tensor([3, 6, 9], device=\"cuda\") # cuda for GPU\n",
        "\n",
        "print(t3.device)\n",
        "\n",
        "# Can also change the device after initialization\n",
        "d = torch.device(\"cuda\") # device object\n",
        "\n",
        "# Checking if GPU is available ('to()' returns a copy of the Tensor into that device, doesn't move the Tesnor into the device, unlike 'nn.Module')\n",
        "if torch.cuda.is_available():\n",
        "    t1 = t1.to(d)\n",
        "\n",
        "print(t1.device)\n",
        "\n",
        "# If we had more than 1 GPU we can count them:\n",
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Rdx6VHIWw-"
      },
      "source": [
        "## Device Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtKNT3kNIa7O",
        "outputId": "e22d7bf9-e49d-473f-cfb5-a6640cd5e00a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# In all out Pytorch files we are going to use this line:\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Simply set the device into cuda if GPU is available"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z-FWmcffwwFd"
      },
      "source": [
        "## Working with Gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9GVyfhTwwFe",
        "outputId": "896a6ffb-77a6-471e-c56d-841cbc42685f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(20.) tensor(9.) tensor(1.)\n",
            "tensor(0.) tensor(0.) tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "# Another powerfull feature of Pytorch is that it calculate gradients very eazy.\n",
        "\n",
        "# To calculate the gradients we need to add the requires_grad=True parameter on each tensor that is inlcuded in the function.\n",
        "\n",
        "x = torch.tensor(9., requires_grad=True)\n",
        "y = torch.tensor(2., requires_grad=True)\n",
        "z = torch.tensor(4., requires_grad=True)\n",
        "\n",
        "f = x**2 + y*x + z  # The function that we want to calculate the gradient.\n",
        "\n",
        "f.backward()        # This method compute all the gradients automatically with respect of x and y and z\n",
        "                    #   Those gradients are stored in the 'grad' attribute of each Tensor.\n",
        "\n",
        "g1 = x.grad         # 20 because the derivative of f with respect of x is 'f = 2x + y'.\n",
        "g2 = y.grad         # 9 because the derivative of f with respect of y is 'f = x'.\n",
        "g3 = z.grad         # 1 because the derivative of f with respect of z is 'f = 1'.\n",
        "\n",
        "print(g1, g2, g3)\n",
        "\n",
        "# Reseating the gradients to zero (Nessesary at the end of each epoch)\n",
        "g1.zero_()\n",
        "g2.zero_()\n",
        "g3.zero_()\n",
        "\n",
        "print(g1, g2, g3)\n",
        "\n",
        "# If we dont work with gradients (makes thinks faster).\n",
        "with torch.no_grad():\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
